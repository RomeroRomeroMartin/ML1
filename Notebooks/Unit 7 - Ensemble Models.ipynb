{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5dd4668",
   "metadata": {},
   "source": [
    "# Ensemble Models\n",
    "\n",
    "One of the latest trends in artificial intelligence modelling can be summarised as \"knowledge of the whole or the crowd\". What this somewhat familiar phrase defines is the use of a multitude of so-called \"weak\" models in a meta-classifier. The aim is to generate a \"strong\" model based on the knowledge extracted by the \"weak\" models. For example, although it will be detailed later, multiple, much simpler Decision Trees are developed in a Random Forest. The combination of these ones in the Random Forest exceeds the performance of any of the individual models. The models that emerge in this way, as meta-classifiers or meta-regressors, are generically called **Ensemble models**.\n",
    "\n",
    "Is is worth mentioned that these models may not be limited only to decision trees, but may instead be composed of any type of machine learning model that has been seen previously. They can even be mixed models where not all models have been obtained in the same way, but can be created through the combined use of several techniques such as K-NN, SVM, etc. Thus, the first criteria to classifify the ensemble models would be if they are homogeneous or heterogeneous models. However this is not the only criteria to classifity the ensemble models, in this unit, we will explore various ways of generating the models and how to combine them later on. We will also take a closer look at two of the most common techniques within ensemble models such as Random Forest and _XGBoost_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f73db",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Unlike the first tutorials, where the iris flower problem has been used as a benchmark, in this tutorial we will use a different one. The problem is also included in the UCI repository, although it is also small, the number of variables increases significantly and therefore it will give us some more room to explore. Specifically, it is a classic machine learning problem, which is informally called Rock or Mine? It is a small database consisting of 111 patterns corresponding to rocks and 97 to water mines (simulated as metal cylinders). Each of the patterns consists of 60 numerical measurements corresponding to a section of the sonar sequences. These values are already between 0.0 and 1.0, although it is worth normalising them to be on the safe side. These measurements represent the energy value of different wavelength ranges for a certain period of time.\n",
    "\n",
    "We are going to use a couple of new packages in the process, more specificly, [DataFrames.jl](https://juliaai.github.io/DataScienceTutorials.jl/data/dataframe/) and [UrlDownload.jl](https://github.com/Arkoniak/UrlDownload.jl). Therefore, first thing first, ensure that the packages are correcly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0eb88ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg;\n",
    "Pkg.add(\"CSV\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"UrlDownload\")\n",
    "Pkg.add(\"MLDataUtils\")\n",
    "Pkg.add(\"ScikitLearn\")\n",
    "Pkg.add(\"HTTP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad05475",
   "metadata": {},
   "source": [
    "After that, the data will be downloaded if they are not already available, for which the following code can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "007c0232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>61×7 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">36 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">variable</th><th style = \"text-align: left;\">mean</th><th style = \"text-align: left;\">min</th><th style = \"text-align: left;\">median</th><th style = \"text-align: left;\">max</th><th style = \"text-align: left;\">nmissing</th><th style = \"text-align: left;\">eltype</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Symbol\" style = \"text-align: left;\">Symbol</th><th title = \"Union{Nothing, Float64}\" style = \"text-align: left;\">Union…</th><th title = \"Any\" style = \"text-align: left;\">Any</th><th title = \"Union{Nothing, Float64}\" style = \"text-align: left;\">Union…</th><th title = \"Any\" style = \"text-align: left;\">Any</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"DataType\" style = \"text-align: left;\">DataType</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">Column1</td><td style = \"text-align: left;\">0.0291639</td><td style = \"text-align: left;\">0.0015</td><td style = \"text-align: left;\">0.0228</td><td style = \"text-align: left;\">0.1371</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">Column2</td><td style = \"text-align: left;\">0.0384365</td><td style = \"text-align: left;\">0.0006</td><td style = \"text-align: left;\">0.0308</td><td style = \"text-align: left;\">0.2339</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">Column3</td><td style = \"text-align: left;\">0.0438322</td><td style = \"text-align: left;\">0.0015</td><td style = \"text-align: left;\">0.0343</td><td style = \"text-align: left;\">0.3059</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">Column4</td><td style = \"text-align: left;\">0.0538923</td><td style = \"text-align: left;\">0.0058</td><td style = \"text-align: left;\">0.04405</td><td style = \"text-align: left;\">0.4264</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">Column5</td><td style = \"text-align: left;\">0.0752024</td><td style = \"text-align: left;\">0.0067</td><td style = \"text-align: left;\">0.0625</td><td style = \"text-align: left;\">0.401</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">Column6</td><td style = \"text-align: left;\">0.10457</td><td style = \"text-align: left;\">0.0102</td><td style = \"text-align: left;\">0.09215</td><td style = \"text-align: left;\">0.3823</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: left;\">Column7</td><td style = \"text-align: left;\">0.121747</td><td style = \"text-align: left;\">0.0033</td><td style = \"text-align: left;\">0.10695</td><td style = \"text-align: left;\">0.3729</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: left;\">Column8</td><td style = \"text-align: left;\">0.134799</td><td style = \"text-align: left;\">0.0055</td><td style = \"text-align: left;\">0.1121</td><td style = \"text-align: left;\">0.459</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: left;\">Column9</td><td style = \"text-align: left;\">0.178003</td><td style = \"text-align: left;\">0.0075</td><td style = \"text-align: left;\">0.15225</td><td style = \"text-align: left;\">0.6828</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: left;\">Column10</td><td style = \"text-align: left;\">0.208259</td><td style = \"text-align: left;\">0.0113</td><td style = \"text-align: left;\">0.1824</td><td style = \"text-align: left;\">0.7106</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: left;\">Column11</td><td style = \"text-align: left;\">0.236013</td><td style = \"text-align: left;\">0.0289</td><td style = \"text-align: left;\">0.2248</td><td style = \"text-align: left;\">0.7342</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: left;\">Column12</td><td style = \"text-align: left;\">0.250221</td><td style = \"text-align: left;\">0.0236</td><td style = \"text-align: left;\">0.24905</td><td style = \"text-align: left;\">0.706</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: left;\">Column13</td><td style = \"text-align: left;\">0.273305</td><td style = \"text-align: left;\">0.0184</td><td style = \"text-align: left;\">0.26395</td><td style = \"text-align: left;\">0.7131</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">50</td><td style = \"text-align: left;\">Column50</td><td style = \"text-align: left;\">0.020424</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">0.0179</td><td style = \"text-align: left;\">0.0825</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">51</td><td style = \"text-align: left;\">Column51</td><td style = \"text-align: left;\">0.0160687</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">0.0139</td><td style = \"text-align: left;\">0.1004</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">52</td><td style = \"text-align: left;\">Column52</td><td style = \"text-align: left;\">0.0134202</td><td style = \"text-align: left;\">0.0008</td><td style = \"text-align: left;\">0.0114</td><td style = \"text-align: left;\">0.0709</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">53</td><td style = \"text-align: left;\">Column53</td><td style = \"text-align: left;\">0.0107091</td><td style = \"text-align: left;\">0.0005</td><td style = \"text-align: left;\">0.00955</td><td style = \"text-align: left;\">0.039</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">54</td><td style = \"text-align: left;\">Column54</td><td style = \"text-align: left;\">0.0109409</td><td style = \"text-align: left;\">0.001</td><td style = \"text-align: left;\">0.0093</td><td style = \"text-align: left;\">0.0352</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">55</td><td style = \"text-align: left;\">Column55</td><td style = \"text-align: left;\">0.00929038</td><td style = \"text-align: left;\">0.0006</td><td style = \"text-align: left;\">0.0075</td><td style = \"text-align: left;\">0.0447</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">56</td><td style = \"text-align: left;\">Column56</td><td style = \"text-align: left;\">0.00822163</td><td style = \"text-align: left;\">0.0004</td><td style = \"text-align: left;\">0.00685</td><td style = \"text-align: left;\">0.0394</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">57</td><td style = \"text-align: left;\">Column57</td><td style = \"text-align: left;\">0.00782019</td><td style = \"text-align: left;\">0.0003</td><td style = \"text-align: left;\">0.00595</td><td style = \"text-align: left;\">0.0355</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">58</td><td style = \"text-align: left;\">Column58</td><td style = \"text-align: left;\">0.00794904</td><td style = \"text-align: left;\">0.0003</td><td style = \"text-align: left;\">0.0058</td><td style = \"text-align: left;\">0.044</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">59</td><td style = \"text-align: left;\">Column59</td><td style = \"text-align: left;\">0.00794135</td><td style = \"text-align: left;\">0.0001</td><td style = \"text-align: left;\">0.0064</td><td style = \"text-align: left;\">0.0364</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">60</td><td style = \"text-align: left;\">Column60</td><td style = \"text-align: left;\">0.00650721</td><td style = \"text-align: left;\">0.0006</td><td style = \"text-align: left;\">0.0053</td><td style = \"text-align: left;\">0.0439</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">61</td><td style = \"text-align: left;\">Column61</td><td style = \"font-style: italic; text-align: left;\"></td><td style = \"text-align: left;\">M</td><td style = \"font-style: italic; text-align: left;\"></td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">String1</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& variable & mean & min & median & max & nmissing & eltype\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Union… & Any & Union… & Any & Int64 & DataType\\\\\n",
       "\t\\hline\n",
       "\t1 & Column1 & 0.0291639 & 0.0015 & 0.0228 & 0.1371 & 0 & Float64 \\\\\n",
       "\t2 & Column2 & 0.0384365 & 0.0006 & 0.0308 & 0.2339 & 0 & Float64 \\\\\n",
       "\t3 & Column3 & 0.0438322 & 0.0015 & 0.0343 & 0.3059 & 0 & Float64 \\\\\n",
       "\t4 & Column4 & 0.0538923 & 0.0058 & 0.04405 & 0.4264 & 0 & Float64 \\\\\n",
       "\t5 & Column5 & 0.0752024 & 0.0067 & 0.0625 & 0.401 & 0 & Float64 \\\\\n",
       "\t6 & Column6 & 0.10457 & 0.0102 & 0.09215 & 0.3823 & 0 & Float64 \\\\\n",
       "\t7 & Column7 & 0.121747 & 0.0033 & 0.10695 & 0.3729 & 0 & Float64 \\\\\n",
       "\t8 & Column8 & 0.134799 & 0.0055 & 0.1121 & 0.459 & 0 & Float64 \\\\\n",
       "\t9 & Column9 & 0.178003 & 0.0075 & 0.15225 & 0.6828 & 0 & Float64 \\\\\n",
       "\t10 & Column10 & 0.208259 & 0.0113 & 0.1824 & 0.7106 & 0 & Float64 \\\\\n",
       "\t11 & Column11 & 0.236013 & 0.0289 & 0.2248 & 0.7342 & 0 & Float64 \\\\\n",
       "\t12 & Column12 & 0.250221 & 0.0236 & 0.24905 & 0.706 & 0 & Float64 \\\\\n",
       "\t13 & Column13 & 0.273305 & 0.0184 & 0.26395 & 0.7131 & 0 & Float64 \\\\\n",
       "\t14 & Column14 & 0.296568 & 0.0273 & 0.2811 & 0.997 & 0 & Float64 \\\\\n",
       "\t15 & Column15 & 0.320201 & 0.0031 & 0.2817 & 1.0 & 0 & Float64 \\\\\n",
       "\t16 & Column16 & 0.378487 & 0.0162 & 0.3047 & 0.9988 & 0 & Float64 \\\\\n",
       "\t17 & Column17 & 0.415983 & 0.0349 & 0.3084 & 1.0 & 0 & Float64 \\\\\n",
       "\t18 & Column18 & 0.452318 & 0.0375 & 0.3683 & 1.0 & 0 & Float64 \\\\\n",
       "\t19 & Column19 & 0.504812 & 0.0494 & 0.43495 & 1.0 & 0 & Float64 \\\\\n",
       "\t20 & Column20 & 0.563047 & 0.0656 & 0.5425 & 1.0 & 0 & Float64 \\\\\n",
       "\t21 & Column21 & 0.60906 & 0.0512 & 0.6177 & 1.0 & 0 & Float64 \\\\\n",
       "\t22 & Column22 & 0.624275 & 0.0219 & 0.6649 & 1.0 & 0 & Float64 \\\\\n",
       "\t23 & Column23 & 0.646975 & 0.0563 & 0.6997 & 1.0 & 0 & Float64 \\\\\n",
       "\t24 & Column24 & 0.672654 & 0.0239 & 0.6985 & 1.0 & 0 & Float64 \\\\\n",
       "\t25 & Column25 & 0.675424 & 0.024 & 0.7211 & 1.0 & 0 & Float64 \\\\\n",
       "\t26 & Column26 & 0.699866 & 0.0921 & 0.7545 & 1.0 & 0 & Float64 \\\\\n",
       "\t27 & Column27 & 0.702155 & 0.0481 & 0.7456 & 1.0 & 0 & Float64 \\\\\n",
       "\t28 & Column28 & 0.694024 & 0.0284 & 0.7319 & 1.0 & 0 & Float64 \\\\\n",
       "\t29 & Column29 & 0.642074 & 0.0144 & 0.6808 & 1.0 & 0 & Float64 \\\\\n",
       "\t30 & Column30 & 0.580928 & 0.0613 & 0.60715 & 1.0 & 0 & Float64 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m61×7 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m variable \u001b[0m\u001b[1m mean       \u001b[0m\u001b[1m min    \u001b[0m\u001b[1m median  \u001b[0m\u001b[1m max    \u001b[0m\u001b[1m nmissing \u001b[0m\u001b[1m eltype   \u001b[0m\n",
       "     │\u001b[90m Symbol   \u001b[0m\u001b[90m Union…     \u001b[0m\u001b[90m Any    \u001b[0m\u001b[90m Union…  \u001b[0m\u001b[90m Any    \u001b[0m\u001b[90m Int64    \u001b[0m\u001b[90m DataType \u001b[0m\n",
       "─────┼───────────────────────────────────────────────────────────────────\n",
       "   1 │ Column1   0.0291639   0.0015  0.0228   0.1371         0  Float64\n",
       "   2 │ Column2   0.0384365   0.0006  0.0308   0.2339         0  Float64\n",
       "   3 │ Column3   0.0438322   0.0015  0.0343   0.3059         0  Float64\n",
       "   4 │ Column4   0.0538923   0.0058  0.04405  0.4264         0  Float64\n",
       "   5 │ Column5   0.0752024   0.0067  0.0625   0.401          0  Float64\n",
       "   6 │ Column6   0.10457     0.0102  0.09215  0.3823         0  Float64\n",
       "   7 │ Column7   0.121747    0.0033  0.10695  0.3729         0  Float64\n",
       "   8 │ Column8   0.134799    0.0055  0.1121   0.459          0  Float64\n",
       "   9 │ Column9   0.178003    0.0075  0.15225  0.6828         0  Float64\n",
       "  10 │ Column10  0.208259    0.0113  0.1824   0.7106         0  Float64\n",
       "  11 │ Column11  0.236013    0.0289  0.2248   0.7342         0  Float64\n",
       "  ⋮  │    ⋮          ⋮         ⋮        ⋮       ⋮        ⋮         ⋮\n",
       "  52 │ Column52  0.0134202   0.0008  0.0114   0.0709         0  Float64\n",
       "  53 │ Column53  0.0107091   0.0005  0.00955  0.039          0  Float64\n",
       "  54 │ Column54  0.0109409   0.001   0.0093   0.0352         0  Float64\n",
       "  55 │ Column55  0.00929038  0.0006  0.0075   0.0447         0  Float64\n",
       "  56 │ Column56  0.00822163  0.0004  0.00685  0.0394         0  Float64\n",
       "  57 │ Column57  0.00782019  0.0003  0.00595  0.0355         0  Float64\n",
       "  58 │ Column58  0.00794904  0.0003  0.0058   0.044          0  Float64\n",
       "  59 │ Column59  0.00794135  0.0001  0.0064   0.0364         0  Float64\n",
       "  60 │ Column60  0.00650721  0.0006  0.0053   0.0439         0  Float64\n",
       "  61 │ Column61 \u001b[90m            \u001b[0m M      \u001b[90m         \u001b[0m R              0  String1\n",
       "\u001b[36m                                                          40 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using UrlDownload\n",
    "using DataFrames\n",
    "using CSV\n",
    "\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\"\n",
    "data = urldownload(url, true, format=:CSV, header=false) |> DataFrame\n",
    "describe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>61×7 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">36 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">variable</th><th style = \"text-align: left;\">mean</th><th style = \"text-align: left;\">min</th><th style = \"text-align: left;\">median</th><th style = \"text-align: left;\">max</th><th style = \"text-align: left;\">nmissing</th><th style = \"text-align: left;\">eltype</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Symbol\" style = \"text-align: left;\">Symbol</th><th title = \"Union{Nothing, Float64}\" style = \"text-align: left;\">Union…</th><th title = \"Any\" style = \"text-align: left;\">Any</th><th title = \"Union{Nothing, Float64}\" style = \"text-align: left;\">Union…</th><th title = \"Any\" style = \"text-align: left;\">Any</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"DataType\" style = \"text-align: left;\">DataType</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">Column1</td><td style = \"text-align: left;\">0.0291639</td><td style = \"text-align: left;\">0.0015</td><td style = \"text-align: left;\">0.0228</td><td style = \"text-align: left;\">0.1371</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">Column2</td><td style = \"text-align: left;\">0.0384365</td><td style = \"text-align: left;\">0.0006</td><td style = \"text-align: left;\">0.0308</td><td style = \"text-align: left;\">0.2339</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">Column3</td><td style = \"text-align: left;\">0.0438322</td><td style = \"text-align: left;\">0.0015</td><td style = \"text-align: left;\">0.0343</td><td style = \"text-align: left;\">0.3059</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">Column4</td><td style = \"text-align: left;\">0.0538923</td><td style = \"text-align: left;\">0.0058</td><td style = \"text-align: left;\">0.04405</td><td style = \"text-align: left;\">0.4264</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">Column5</td><td style = \"text-align: left;\">0.0752024</td><td style = \"text-align: left;\">0.0067</td><td style = \"text-align: left;\">0.0625</td><td style = \"text-align: left;\">0.401</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">Column6</td><td style = \"text-align: left;\">0.10457</td><td style = \"text-align: left;\">0.0102</td><td style = \"text-align: left;\">0.09215</td><td style = \"text-align: left;\">0.3823</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: left;\">Column7</td><td style = \"text-align: left;\">0.121747</td><td style = \"text-align: left;\">0.0033</td><td style = \"text-align: left;\">0.10695</td><td style = \"text-align: left;\">0.3729</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: left;\">Column8</td><td style = \"text-align: left;\">0.134799</td><td style = \"text-align: left;\">0.0055</td><td style = \"text-align: left;\">0.1121</td><td style = \"text-align: left;\">0.459</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: left;\">Column9</td><td style = \"text-align: left;\">0.178003</td><td style = \"text-align: left;\">0.0075</td><td style = \"text-align: left;\">0.15225</td><td style = \"text-align: left;\">0.6828</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: left;\">Column10</td><td style = \"text-align: left;\">0.208259</td><td style = \"text-align: left;\">0.0113</td><td style = \"text-align: left;\">0.1824</td><td style = \"text-align: left;\">0.7106</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: left;\">Column11</td><td style = \"text-align: left;\">0.236013</td><td style = \"text-align: left;\">0.0289</td><td style = \"text-align: left;\">0.2248</td><td style = \"text-align: left;\">0.7342</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: left;\">Column12</td><td style = \"text-align: left;\">0.250221</td><td style = \"text-align: left;\">0.0236</td><td style = \"text-align: left;\">0.24905</td><td style = \"text-align: left;\">0.706</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: left;\">Column13</td><td style = \"text-align: left;\">0.273305</td><td style = \"text-align: left;\">0.0184</td><td style = \"text-align: left;\">0.26395</td><td style = \"text-align: left;\">0.7131</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">50</td><td style = \"text-align: left;\">Column50</td><td style = \"text-align: left;\">0.020424</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">0.0179</td><td style = \"text-align: left;\">0.0825</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">51</td><td style = \"text-align: left;\">Column51</td><td style = \"text-align: left;\">0.0160687</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">0.0139</td><td style = \"text-align: left;\">0.1004</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">52</td><td style = \"text-align: left;\">Column52</td><td style = \"text-align: left;\">0.0134202</td><td style = \"text-align: left;\">0.0008</td><td style = \"text-align: left;\">0.0114</td><td style = \"text-align: left;\">0.0709</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">53</td><td style = \"text-align: left;\">Column53</td><td style = \"text-align: left;\">0.0107091</td><td style = \"text-align: left;\">0.0005</td><td style = \"text-align: left;\">0.00955</td><td style = \"text-align: left;\">0.039</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">54</td><td style = \"text-align: left;\">Column54</td><td style = \"text-align: left;\">0.0109409</td><td style = \"text-align: left;\">0.001</td><td style = \"text-align: left;\">0.0093</td><td style = \"text-align: left;\">0.0352</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">55</td><td style = \"text-align: left;\">Column55</td><td style = \"text-align: left;\">0.00929038</td><td style = \"text-align: left;\">0.0006</td><td style = \"text-align: left;\">0.0075</td><td style = \"text-align: left;\">0.0447</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">56</td><td style = \"text-align: left;\">Column56</td><td style = \"text-align: left;\">0.00822163</td><td style = \"text-align: left;\">0.0004</td><td style = \"text-align: left;\">0.00685</td><td style = \"text-align: left;\">0.0394</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">57</td><td style = \"text-align: left;\">Column57</td><td style = \"text-align: left;\">0.00782019</td><td style = \"text-align: left;\">0.0003</td><td style = \"text-align: left;\">0.00595</td><td style = \"text-align: left;\">0.0355</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">58</td><td style = \"text-align: left;\">Column58</td><td style = \"text-align: left;\">0.00794904</td><td style = \"text-align: left;\">0.0003</td><td style = \"text-align: left;\">0.0058</td><td style = \"text-align: left;\">0.044</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">59</td><td style = \"text-align: left;\">Column59</td><td style = \"text-align: left;\">0.00794135</td><td style = \"text-align: left;\">0.0001</td><td style = \"text-align: left;\">0.0064</td><td style = \"text-align: left;\">0.0364</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">60</td><td style = \"text-align: left;\">Column60</td><td style = \"text-align: left;\">0.00650721</td><td style = \"text-align: left;\">0.0006</td><td style = \"text-align: left;\">0.0053</td><td style = \"text-align: left;\">0.0439</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">61</td><td style = \"text-align: left;\">Column61</td><td style = \"font-style: italic; text-align: left;\"></td><td style = \"text-align: left;\">M</td><td style = \"font-style: italic; text-align: left;\"></td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">String1</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& variable & mean & min & median & max & nmissing & eltype\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Union… & Any & Union… & Any & Int64 & DataType\\\\\n",
       "\t\\hline\n",
       "\t1 & Column1 & 0.0291639 & 0.0015 & 0.0228 & 0.1371 & 0 & Float64 \\\\\n",
       "\t2 & Column2 & 0.0384365 & 0.0006 & 0.0308 & 0.2339 & 0 & Float64 \\\\\n",
       "\t3 & Column3 & 0.0438322 & 0.0015 & 0.0343 & 0.3059 & 0 & Float64 \\\\\n",
       "\t4 & Column4 & 0.0538923 & 0.0058 & 0.04405 & 0.4264 & 0 & Float64 \\\\\n",
       "\t5 & Column5 & 0.0752024 & 0.0067 & 0.0625 & 0.401 & 0 & Float64 \\\\\n",
       "\t6 & Column6 & 0.10457 & 0.0102 & 0.09215 & 0.3823 & 0 & Float64 \\\\\n",
       "\t7 & Column7 & 0.121747 & 0.0033 & 0.10695 & 0.3729 & 0 & Float64 \\\\\n",
       "\t8 & Column8 & 0.134799 & 0.0055 & 0.1121 & 0.459 & 0 & Float64 \\\\\n",
       "\t9 & Column9 & 0.178003 & 0.0075 & 0.15225 & 0.6828 & 0 & Float64 \\\\\n",
       "\t10 & Column10 & 0.208259 & 0.0113 & 0.1824 & 0.7106 & 0 & Float64 \\\\\n",
       "\t11 & Column11 & 0.236013 & 0.0289 & 0.2248 & 0.7342 & 0 & Float64 \\\\\n",
       "\t12 & Column12 & 0.250221 & 0.0236 & 0.24905 & 0.706 & 0 & Float64 \\\\\n",
       "\t13 & Column13 & 0.273305 & 0.0184 & 0.26395 & 0.7131 & 0 & Float64 \\\\\n",
       "\t14 & Column14 & 0.296568 & 0.0273 & 0.2811 & 0.997 & 0 & Float64 \\\\\n",
       "\t15 & Column15 & 0.320201 & 0.0031 & 0.2817 & 1.0 & 0 & Float64 \\\\\n",
       "\t16 & Column16 & 0.378487 & 0.0162 & 0.3047 & 0.9988 & 0 & Float64 \\\\\n",
       "\t17 & Column17 & 0.415983 & 0.0349 & 0.3084 & 1.0 & 0 & Float64 \\\\\n",
       "\t18 & Column18 & 0.452318 & 0.0375 & 0.3683 & 1.0 & 0 & Float64 \\\\\n",
       "\t19 & Column19 & 0.504812 & 0.0494 & 0.43495 & 1.0 & 0 & Float64 \\\\\n",
       "\t20 & Column20 & 0.563047 & 0.0656 & 0.5425 & 1.0 & 0 & Float64 \\\\\n",
       "\t21 & Column21 & 0.60906 & 0.0512 & 0.6177 & 1.0 & 0 & Float64 \\\\\n",
       "\t22 & Column22 & 0.624275 & 0.0219 & 0.6649 & 1.0 & 0 & Float64 \\\\\n",
       "\t23 & Column23 & 0.646975 & 0.0563 & 0.6997 & 1.0 & 0 & Float64 \\\\\n",
       "\t24 & Column24 & 0.672654 & 0.0239 & 0.6985 & 1.0 & 0 & Float64 \\\\\n",
       "\t25 & Column25 & 0.675424 & 0.024 & 0.7211 & 1.0 & 0 & Float64 \\\\\n",
       "\t26 & Column26 & 0.699866 & 0.0921 & 0.7545 & 1.0 & 0 & Float64 \\\\\n",
       "\t27 & Column27 & 0.702155 & 0.0481 & 0.7456 & 1.0 & 0 & Float64 \\\\\n",
       "\t28 & Column28 & 0.694024 & 0.0284 & 0.7319 & 1.0 & 0 & Float64 \\\\\n",
       "\t29 & Column29 & 0.642074 & 0.0144 & 0.6808 & 1.0 & 0 & Float64 \\\\\n",
       "\t30 & Column30 & 0.580928 & 0.0613 & 0.60715 & 1.0 & 0 & Float64 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m61×7 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m variable \u001b[0m\u001b[1m mean       \u001b[0m\u001b[1m min    \u001b[0m\u001b[1m median  \u001b[0m\u001b[1m max    \u001b[0m\u001b[1m nmissing \u001b[0m\u001b[1m eltype   \u001b[0m\n",
       "     │\u001b[90m Symbol   \u001b[0m\u001b[90m Union…     \u001b[0m\u001b[90m Any    \u001b[0m\u001b[90m Union…  \u001b[0m\u001b[90m Any    \u001b[0m\u001b[90m Int64    \u001b[0m\u001b[90m DataType \u001b[0m\n",
       "─────┼───────────────────────────────────────────────────────────────────\n",
       "   1 │ Column1   0.0291639   0.0015  0.0228   0.1371         0  Float64\n",
       "   2 │ Column2   0.0384365   0.0006  0.0308   0.2339         0  Float64\n",
       "   3 │ Column3   0.0438322   0.0015  0.0343   0.3059         0  Float64\n",
       "   4 │ Column4   0.0538923   0.0058  0.04405  0.4264         0  Float64\n",
       "   5 │ Column5   0.0752024   0.0067  0.0625   0.401          0  Float64\n",
       "   6 │ Column6   0.10457     0.0102  0.09215  0.3823         0  Float64\n",
       "   7 │ Column7   0.121747    0.0033  0.10695  0.3729         0  Float64\n",
       "   8 │ Column8   0.134799    0.0055  0.1121   0.459          0  Float64\n",
       "   9 │ Column9   0.178003    0.0075  0.15225  0.6828         0  Float64\n",
       "  10 │ Column10  0.208259    0.0113  0.1824   0.7106         0  Float64\n",
       "  11 │ Column11  0.236013    0.0289  0.2248   0.7342         0  Float64\n",
       "  ⋮  │    ⋮          ⋮         ⋮        ⋮       ⋮        ⋮         ⋮\n",
       "  52 │ Column52  0.0134202   0.0008  0.0114   0.0709         0  Float64\n",
       "  53 │ Column53  0.0107091   0.0005  0.00955  0.039          0  Float64\n",
       "  54 │ Column54  0.0109409   0.001   0.0093   0.0352         0  Float64\n",
       "  55 │ Column55  0.00929038  0.0006  0.0075   0.0447         0  Float64\n",
       "  56 │ Column56  0.00822163  0.0004  0.00685  0.0394         0  Float64\n",
       "  57 │ Column57  0.00782019  0.0003  0.00595  0.0355         0  Float64\n",
       "  58 │ Column58  0.00794904  0.0003  0.0058   0.044          0  Float64\n",
       "  59 │ Column59  0.00794135  0.0001  0.0064   0.0364         0  Float64\n",
       "  60 │ Column60  0.00650721  0.0006  0.0053   0.0439         0  Float64\n",
       "  61 │ Column61 \u001b[90m            \u001b[0m M      \u001b[90m         \u001b[0m R              0  String1\n",
       "\u001b[36m                                                          40 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "\n",
    "# Reemplaza \"ruta/al/archivo/sonar.all-data\" con la ruta real donde guardaste el archivo descargado\n",
    "ruta_del_archivo = \"sonar.all-data\"\n",
    "\n",
    "data = CSV.File(ruta_del_archivo, header=false) |> DataFrame\n",
    "describe(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ebf504",
   "metadata": {},
   "source": [
    "As it can be seen in the previos line, we have downloaded de data and pipe it, with the operator `|>`, into the function DataFrame. This is going to create an structure simular to a database table which is particular convinient to check for missing values or the ranges of the different variables. In fact, the library makes it particularly easy to deal with missing values with functions to fullfill or remove the samples with non-valid measures. However it is too long to see every single variable on the output report, if some queries are made we can identify  that here is no missing values. Additionally no variable is over 1.0 but some of them are not normalized. A similar structure can be found in other languages, like R or Python.\n",
    "\n",
    "As an example, of this process lets make the an additional column in order to convert to categorical the las column 60 which has a **M** for each Mine and an **R** for each sample of rock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a07ecb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>208×62 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">183 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">Column1</th><th style = \"text-align: left;\">Column2</th><th style = \"text-align: left;\">Column3</th><th style = \"text-align: left;\">Column4</th><th style = \"text-align: left;\">Column5</th><th style = \"text-align: left;\">Column6</th><th style = \"text-align: left;\">Column7</th><th style = \"text-align: left;\">Column8</th><th style = \"text-align: left;\">Column9</th><th style = \"text-align: left;\">Column10</th><th style = \"text-align: left;\">Column11</th><th style = \"text-align: left;\">Column12</th><th style = \"text-align: left;\">Column13</th><th style = \"text-align: left;\">Column14</th><th style = \"text-align: left;\">Column15</th><th style = \"text-align: left;\">Column16</th><th style = \"text-align: left;\">Column17</th><th style = \"text-align: left;\">Column18</th><th style = \"text-align: left;\">Column19</th><th style = \"text-align: left;\">Column20</th><th style = \"text-align: left;\">Column21</th><th style = \"text-align: left;\">Column22</th><th style = \"text-align: left;\">Column23</th><th style = \"text-align: left;\">Column24</th><th style = \"text-align: left;\">Column25</th><th style = \"text-align: left;\">Column26</th><th style = \"text-align: left;\">Column27</th><th style = \"text-align: left;\">Column28</th><th style = \"text-align: left;\">Column29</th><th style = \"text-align: left;\">Column30</th><th style = \"text-align: left;\">Column31</th><th style = \"text-align: left;\">Column32</th><th style = \"text-align: left;\">Column33</th><th style = \"text-align: left;\">Column34</th><th style = \"text-align: left;\">Column35</th><th style = \"text-align: left;\">Column36</th><th style = \"text-align: left;\">Column37</th><th style = \"text-align: left;\">Column38</th><th style = \"text-align: left;\">Column39</th><th style = \"text-align: left;\">Column40</th><th style = \"text-align: left;\">Column41</th><th style = \"text-align: left;\">Column42</th><th style = \"text-align: left;\">Column43</th><th style = \"text-align: left;\">Column44</th><th style = \"text-align: left;\">Column45</th><th style = \"text-align: left;\">Column46</th><th style = \"text-align: left;\">Column47</th><th style = \"text-align: left;\">Column48</th><th style = \"text-align: left;\">Column49</th><th style = \"text-align: left;\">Column50</th><th style = \"text-align: left;\">Column51</th><th style = \"text-align: left;\">Column52</th><th style = \"text-align: left;\">Column53</th><th style = \"text-align: left;\">Column54</th><th style = \"text-align: left;\">Column55</th><th style = \"text-align: left;\">Column56</th><th style = \"text-align: left;\">Column57</th><th style = \"text-align: left;\">Column58</th><th style = \"text-align: left;\">Column59</th><th style = \"text-align: left;\">Column60</th><th style = \"text-align: left;\">Column61</th><th style = \"text-align: left;\">Mine</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"String1\" style = \"text-align: left;\">String1</th><th title = \"Bool\" style = \"text-align: left;\">Bool</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">0.02</td><td style = \"text-align: right;\">0.0371</td><td style = \"text-align: right;\">0.0428</td><td style = \"text-align: right;\">0.0207</td><td style = \"text-align: right;\">0.0954</td><td style = \"text-align: right;\">0.0986</td><td style = \"text-align: right;\">0.1539</td><td style = \"text-align: right;\">0.1601</td><td style = \"text-align: right;\">0.3109</td><td style = \"text-align: right;\">0.2111</td><td style = \"text-align: right;\">0.1609</td><td style = \"text-align: right;\">0.1582</td><td style = \"text-align: right;\">0.2238</td><td style = \"text-align: right;\">0.0645</td><td style = \"text-align: right;\">0.066</td><td style = \"text-align: right;\">0.2273</td><td style = \"text-align: right;\">0.31</td><td style = \"text-align: right;\">0.2999</td><td style = \"text-align: right;\">0.5078</td><td style = \"text-align: right;\">0.4797</td><td style = \"text-align: right;\">0.5783</td><td style = \"text-align: right;\">0.5071</td><td style = \"text-align: right;\">0.4328</td><td style = \"text-align: right;\">0.555</td><td style = \"text-align: right;\">0.6711</td><td style = \"text-align: right;\">0.6415</td><td style = \"text-align: right;\">0.7104</td><td style = \"text-align: right;\">0.808</td><td style = \"text-align: right;\">0.6791</td><td style = \"text-align: right;\">0.3857</td><td style = \"text-align: right;\">0.1307</td><td style = \"text-align: right;\">0.2604</td><td style = \"text-align: right;\">0.5121</td><td style = \"text-align: right;\">0.7547</td><td style = \"text-align: right;\">0.8537</td><td style = \"text-align: right;\">0.8507</td><td style = \"text-align: right;\">0.6692</td><td style = \"text-align: right;\">0.6097</td><td style = \"text-align: right;\">0.4943</td><td style = \"text-align: right;\">0.2744</td><td style = \"text-align: right;\">0.051</td><td style = \"text-align: right;\">0.2834</td><td style = \"text-align: right;\">0.2825</td><td style = \"text-align: right;\">0.4256</td><td style = \"text-align: right;\">0.2641</td><td style = \"text-align: right;\">0.1386</td><td style = \"text-align: right;\">0.1051</td><td style = \"text-align: right;\">0.1343</td><td style = \"text-align: right;\">0.0383</td><td style = \"text-align: right;\">0.0324</td><td style = \"text-align: right;\">0.0232</td><td style = \"text-align: right;\">0.0027</td><td style = \"text-align: right;\">0.0065</td><td style = \"text-align: right;\">0.0159</td><td style = \"text-align: right;\">0.0072</td><td style = \"text-align: right;\">0.0167</td><td style = \"text-align: right;\">0.018</td><td style = \"text-align: right;\">0.0084</td><td style = \"text-align: right;\">0.009</td><td style = \"text-align: right;\">0.0032</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">0.0453</td><td style = \"text-align: right;\">0.0523</td><td style = \"text-align: right;\">0.0843</td><td style = \"text-align: right;\">0.0689</td><td style = \"text-align: right;\">0.1183</td><td style = \"text-align: right;\">0.2583</td><td style = \"text-align: right;\">0.2156</td><td style = \"text-align: right;\">0.3481</td><td style = \"text-align: right;\">0.3337</td><td style = \"text-align: right;\">0.2872</td><td style = \"text-align: right;\">0.4918</td><td style = \"text-align: right;\">0.6552</td><td style = \"text-align: right;\">0.6919</td><td style = \"text-align: right;\">0.7797</td><td style = \"text-align: right;\">0.7464</td><td style = \"text-align: right;\">0.9444</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.8874</td><td style = \"text-align: right;\">0.8024</td><td style = \"text-align: right;\">0.7818</td><td style = \"text-align: right;\">0.5212</td><td style = \"text-align: right;\">0.4052</td><td style = \"text-align: right;\">0.3957</td><td style = \"text-align: right;\">0.3914</td><td style = \"text-align: right;\">0.325</td><td style = \"text-align: right;\">0.32</td><td style = \"text-align: right;\">0.3271</td><td style = \"text-align: right;\">0.2767</td><td style = \"text-align: right;\">0.4423</td><td style = \"text-align: right;\">0.2028</td><td style = \"text-align: right;\">0.3788</td><td style = \"text-align: right;\">0.2947</td><td style = \"text-align: right;\">0.1984</td><td style = \"text-align: right;\">0.2341</td><td style = \"text-align: right;\">0.1306</td><td style = \"text-align: right;\">0.4182</td><td style = \"text-align: right;\">0.3835</td><td style = \"text-align: right;\">0.1057</td><td style = \"text-align: right;\">0.184</td><td style = \"text-align: right;\">0.197</td><td style = \"text-align: right;\">0.1674</td><td style = \"text-align: right;\">0.0583</td><td style = \"text-align: right;\">0.1401</td><td style = \"text-align: right;\">0.1628</td><td style = \"text-align: right;\">0.0621</td><td style = \"text-align: right;\">0.0203</td><td style = \"text-align: right;\">0.053</td><td style = \"text-align: right;\">0.0742</td><td style = \"text-align: right;\">0.0409</td><td style = \"text-align: right;\">0.0061</td><td style = \"text-align: right;\">0.0125</td><td style = \"text-align: right;\">0.0084</td><td style = \"text-align: right;\">0.0089</td><td style = \"text-align: right;\">0.0048</td><td style = \"text-align: right;\">0.0094</td><td style = \"text-align: right;\">0.0191</td><td style = \"text-align: right;\">0.014</td><td style = \"text-align: right;\">0.0049</td><td style = \"text-align: right;\">0.0052</td><td style = \"text-align: right;\">0.0044</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">0.0262</td><td style = \"text-align: right;\">0.0582</td><td style = \"text-align: right;\">0.1099</td><td style = \"text-align: right;\">0.1083</td><td style = \"text-align: right;\">0.0974</td><td style = \"text-align: right;\">0.228</td><td style = \"text-align: right;\">0.2431</td><td style = \"text-align: right;\">0.3771</td><td style = \"text-align: right;\">0.5598</td><td style = \"text-align: right;\">0.6194</td><td style = \"text-align: right;\">0.6333</td><td style = \"text-align: right;\">0.706</td><td style = \"text-align: right;\">0.5544</td><td style = \"text-align: right;\">0.532</td><td style = \"text-align: right;\">0.6479</td><td style = \"text-align: right;\">0.6931</td><td style = \"text-align: right;\">0.6759</td><td style = \"text-align: right;\">0.7551</td><td style = \"text-align: right;\">0.8929</td><td style = \"text-align: right;\">0.8619</td><td style = \"text-align: right;\">0.7974</td><td style = \"text-align: right;\">0.6737</td><td style = \"text-align: right;\">0.4293</td><td style = \"text-align: right;\">0.3648</td><td style = \"text-align: right;\">0.5331</td><td style = \"text-align: right;\">0.2413</td><td style = \"text-align: right;\">0.507</td><td style = \"text-align: right;\">0.8533</td><td style = \"text-align: right;\">0.6036</td><td style = \"text-align: right;\">0.8514</td><td style = \"text-align: right;\">0.8512</td><td style = \"text-align: right;\">0.5045</td><td style = \"text-align: right;\">0.1862</td><td style = \"text-align: right;\">0.2709</td><td style = \"text-align: right;\">0.4232</td><td style = \"text-align: right;\">0.3043</td><td style = \"text-align: right;\">0.6116</td><td style = \"text-align: right;\">0.6756</td><td style = \"text-align: right;\">0.5375</td><td style = \"text-align: right;\">0.4719</td><td style = \"text-align: right;\">0.4647</td><td style = \"text-align: right;\">0.2587</td><td style = \"text-align: right;\">0.2129</td><td style = \"text-align: right;\">0.2222</td><td style = \"text-align: right;\">0.2111</td><td style = \"text-align: right;\">0.0176</td><td style = \"text-align: right;\">0.1348</td><td style = \"text-align: right;\">0.0744</td><td style = \"text-align: right;\">0.013</td><td style = \"text-align: right;\">0.0106</td><td style = \"text-align: right;\">0.0033</td><td style = \"text-align: right;\">0.0232</td><td style = \"text-align: right;\">0.0166</td><td style = \"text-align: right;\">0.0095</td><td style = \"text-align: right;\">0.018</td><td style = \"text-align: right;\">0.0244</td><td style = \"text-align: right;\">0.0316</td><td style = \"text-align: right;\">0.0164</td><td style = \"text-align: right;\">0.0095</td><td style = \"text-align: right;\">0.0078</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">0.01</td><td style = \"text-align: right;\">0.0171</td><td style = \"text-align: right;\">0.0623</td><td style = \"text-align: right;\">0.0205</td><td style = \"text-align: right;\">0.0205</td><td style = \"text-align: right;\">0.0368</td><td style = \"text-align: right;\">0.1098</td><td style = \"text-align: right;\">0.1276</td><td style = \"text-align: right;\">0.0598</td><td style = \"text-align: right;\">0.1264</td><td style = \"text-align: right;\">0.0881</td><td style = \"text-align: right;\">0.1992</td><td style = \"text-align: right;\">0.0184</td><td style = \"text-align: right;\">0.2261</td><td style = \"text-align: right;\">0.1729</td><td style = \"text-align: right;\">0.2131</td><td style = \"text-align: right;\">0.0693</td><td style = \"text-align: right;\">0.2281</td><td style = \"text-align: right;\">0.406</td><td style = \"text-align: right;\">0.3973</td><td style = \"text-align: right;\">0.2741</td><td style = \"text-align: right;\">0.369</td><td style = \"text-align: right;\">0.5556</td><td style = \"text-align: right;\">0.4846</td><td style = \"text-align: right;\">0.314</td><td style = \"text-align: right;\">0.5334</td><td style = \"text-align: right;\">0.5256</td><td style = \"text-align: right;\">0.252</td><td style = \"text-align: right;\">0.209</td><td style = \"text-align: right;\">0.3559</td><td style = \"text-align: right;\">0.626</td><td style = \"text-align: right;\">0.734</td><td style = \"text-align: right;\">0.612</td><td style = \"text-align: right;\">0.3497</td><td style = \"text-align: right;\">0.3953</td><td style = \"text-align: right;\">0.3012</td><td style = \"text-align: right;\">0.5408</td><td style = \"text-align: right;\">0.8814</td><td style = \"text-align: right;\">0.9857</td><td style = \"text-align: right;\">0.9167</td><td style = \"text-align: right;\">0.6121</td><td style = \"text-align: right;\">0.5006</td><td style = \"text-align: right;\">0.321</td><td style = \"text-align: right;\">0.3202</td><td style = \"text-align: right;\">0.4295</td><td style = \"text-align: right;\">0.3654</td><td style = \"text-align: right;\">0.2655</td><td style = \"text-align: right;\">0.1576</td><td style = \"text-align: right;\">0.0681</td><td style = \"text-align: right;\">0.0294</td><td style = \"text-align: right;\">0.0241</td><td style = \"text-align: right;\">0.0121</td><td style = \"text-align: right;\">0.0036</td><td style = \"text-align: right;\">0.015</td><td style = \"text-align: right;\">0.0085</td><td style = \"text-align: right;\">0.0073</td><td style = \"text-align: right;\">0.005</td><td style = \"text-align: right;\">0.0044</td><td style = \"text-align: right;\">0.004</td><td style = \"text-align: right;\">0.0117</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">0.0762</td><td style = \"text-align: right;\">0.0666</td><td style = \"text-align: right;\">0.0481</td><td style = \"text-align: right;\">0.0394</td><td style = \"text-align: right;\">0.059</td><td style = \"text-align: right;\">0.0649</td><td style = \"text-align: right;\">0.1209</td><td style = \"text-align: right;\">0.2467</td><td style = \"text-align: right;\">0.3564</td><td style = \"text-align: right;\">0.4459</td><td style = \"text-align: right;\">0.4152</td><td style = \"text-align: right;\">0.3952</td><td style = \"text-align: right;\">0.4256</td><td style = \"text-align: right;\">0.4135</td><td style = \"text-align: right;\">0.4528</td><td style = \"text-align: right;\">0.5326</td><td style = \"text-align: right;\">0.7306</td><td style = \"text-align: right;\">0.6193</td><td style = \"text-align: right;\">0.2032</td><td style = \"text-align: right;\">0.4636</td><td style = \"text-align: right;\">0.4148</td><td style = \"text-align: right;\">0.4292</td><td style = \"text-align: right;\">0.573</td><td style = \"text-align: right;\">0.5399</td><td style = \"text-align: right;\">0.3161</td><td style = \"text-align: right;\">0.2285</td><td style = \"text-align: right;\">0.6995</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.7262</td><td style = \"text-align: right;\">0.4724</td><td style = \"text-align: right;\">0.5103</td><td style = \"text-align: right;\">0.5459</td><td style = \"text-align: right;\">0.2881</td><td style = \"text-align: right;\">0.0981</td><td style = \"text-align: right;\">0.1951</td><td style = \"text-align: right;\">0.4181</td><td style = \"text-align: right;\">0.4604</td><td style = \"text-align: right;\">0.3217</td><td style = \"text-align: right;\">0.2828</td><td style = \"text-align: right;\">0.243</td><td style = \"text-align: right;\">0.1979</td><td style = \"text-align: right;\">0.2444</td><td style = \"text-align: right;\">0.1847</td><td style = \"text-align: right;\">0.0841</td><td style = \"text-align: right;\">0.0692</td><td style = \"text-align: right;\">0.0528</td><td style = \"text-align: right;\">0.0357</td><td style = \"text-align: right;\">0.0085</td><td style = \"text-align: right;\">0.023</td><td style = \"text-align: right;\">0.0046</td><td style = \"text-align: right;\">0.0156</td><td style = \"text-align: right;\">0.0031</td><td style = \"text-align: right;\">0.0054</td><td style = \"text-align: right;\">0.0105</td><td style = \"text-align: right;\">0.011</td><td style = \"text-align: right;\">0.0015</td><td style = \"text-align: right;\">0.0072</td><td style = \"text-align: right;\">0.0048</td><td style = \"text-align: right;\">0.0107</td><td style = \"text-align: right;\">0.0094</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">0.0286</td><td style = \"text-align: right;\">0.0453</td><td style = \"text-align: right;\">0.0277</td><td style = \"text-align: right;\">0.0174</td><td style = \"text-align: right;\">0.0384</td><td style = \"text-align: right;\">0.099</td><td style = \"text-align: right;\">0.1201</td><td style = \"text-align: right;\">0.1833</td><td style = \"text-align: right;\">0.2105</td><td style = \"text-align: right;\">0.3039</td><td style = \"text-align: right;\">0.2988</td><td style = \"text-align: right;\">0.425</td><td style = \"text-align: right;\">0.6343</td><td style = \"text-align: right;\">0.8198</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.9988</td><td style = \"text-align: right;\">0.9508</td><td style = \"text-align: right;\">0.9025</td><td style = \"text-align: right;\">0.7234</td><td style = \"text-align: right;\">0.5122</td><td style = \"text-align: right;\">0.2074</td><td style = \"text-align: right;\">0.3985</td><td style = \"text-align: right;\">0.589</td><td style = \"text-align: right;\">0.2872</td><td style = \"text-align: right;\">0.2043</td><td style = \"text-align: right;\">0.5782</td><td style = \"text-align: right;\">0.5389</td><td style = \"text-align: right;\">0.375</td><td style = \"text-align: right;\">0.3411</td><td style = \"text-align: right;\">0.5067</td><td style = \"text-align: right;\">0.558</td><td style = \"text-align: right;\">0.4778</td><td style = \"text-align: right;\">0.3299</td><td style = \"text-align: right;\">0.2198</td><td style = \"text-align: right;\">0.1407</td><td style = \"text-align: right;\">0.2856</td><td style = \"text-align: right;\">0.3807</td><td style = \"text-align: right;\">0.4158</td><td style = \"text-align: right;\">0.4054</td><td style = \"text-align: right;\">0.3296</td><td style = \"text-align: right;\">0.2707</td><td style = \"text-align: right;\">0.265</td><td style = \"text-align: right;\">0.0723</td><td style = \"text-align: right;\">0.1238</td><td style = \"text-align: right;\">0.1192</td><td style = \"text-align: right;\">0.1089</td><td style = \"text-align: right;\">0.0623</td><td style = \"text-align: right;\">0.0494</td><td style = \"text-align: right;\">0.0264</td><td style = \"text-align: right;\">0.0081</td><td style = \"text-align: right;\">0.0104</td><td style = \"text-align: right;\">0.0045</td><td style = \"text-align: right;\">0.0014</td><td style = \"text-align: right;\">0.0038</td><td style = \"text-align: right;\">0.0013</td><td style = \"text-align: right;\">0.0089</td><td style = \"text-align: right;\">0.0057</td><td style = \"text-align: right;\">0.0027</td><td style = \"text-align: right;\">0.0051</td><td style = \"text-align: right;\">0.0062</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">0.0317</td><td style = \"text-align: right;\">0.0956</td><td style = \"text-align: right;\">0.1321</td><td style = \"text-align: right;\">0.1408</td><td style = \"text-align: right;\">0.1674</td><td style = \"text-align: right;\">0.171</td><td style = \"text-align: right;\">0.0731</td><td style = \"text-align: right;\">0.1401</td><td style = \"text-align: right;\">0.2083</td><td style = \"text-align: right;\">0.3513</td><td style = \"text-align: right;\">0.1786</td><td style = \"text-align: right;\">0.0658</td><td style = \"text-align: right;\">0.0513</td><td style = \"text-align: right;\">0.3752</td><td style = \"text-align: right;\">0.5419</td><td style = \"text-align: right;\">0.544</td><td style = \"text-align: right;\">0.515</td><td style = \"text-align: right;\">0.4262</td><td style = \"text-align: right;\">0.2024</td><td style = \"text-align: right;\">0.4233</td><td style = \"text-align: right;\">0.7723</td><td style = \"text-align: right;\">0.9735</td><td style = \"text-align: right;\">0.939</td><td style = \"text-align: right;\">0.5559</td><td style = \"text-align: right;\">0.5268</td><td style = \"text-align: right;\">0.6826</td><td style = \"text-align: right;\">0.5713</td><td style = \"text-align: right;\">0.5429</td><td style = \"text-align: right;\">0.2177</td><td style = \"text-align: right;\">0.2149</td><td style = \"text-align: right;\">0.5811</td><td style = \"text-align: right;\">0.6323</td><td style = \"text-align: right;\">0.2965</td><td style = \"text-align: right;\">0.1873</td><td style = \"text-align: right;\">0.2969</td><td style = \"text-align: right;\">0.5163</td><td style = \"text-align: right;\">0.6153</td><td style = \"text-align: right;\">0.4283</td><td style = \"text-align: right;\">0.5479</td><td style = \"text-align: right;\">0.6133</td><td style = \"text-align: right;\">0.5017</td><td style = \"text-align: right;\">0.2377</td><td style = \"text-align: right;\">0.1957</td><td style = \"text-align: right;\">0.1749</td><td style = \"text-align: right;\">0.1304</td><td style = \"text-align: right;\">0.0597</td><td style = \"text-align: right;\">0.1124</td><td style = \"text-align: right;\">0.1047</td><td style = \"text-align: right;\">0.0507</td><td style = \"text-align: right;\">0.0159</td><td style = \"text-align: right;\">0.0195</td><td style = \"text-align: right;\">0.0201</td><td style = \"text-align: right;\">0.0248</td><td style = \"text-align: right;\">0.0131</td><td style = \"text-align: right;\">0.007</td><td style = \"text-align: right;\">0.0138</td><td style = \"text-align: right;\">0.0092</td><td style = \"text-align: right;\">0.0143</td><td style = \"text-align: right;\">0.0036</td><td style = \"text-align: right;\">0.0103</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">0.0519</td><td style = \"text-align: right;\">0.0548</td><td style = \"text-align: right;\">0.0842</td><td style = \"text-align: right;\">0.0319</td><td style = \"text-align: right;\">0.1158</td><td style = \"text-align: right;\">0.0922</td><td style = \"text-align: right;\">0.1027</td><td style = \"text-align: right;\">0.0613</td><td style = \"text-align: right;\">0.1465</td><td style = \"text-align: right;\">0.2838</td><td style = \"text-align: right;\">0.2802</td><td style = \"text-align: right;\">0.3086</td><td style = \"text-align: right;\">0.2657</td><td style = \"text-align: right;\">0.3801</td><td style = \"text-align: right;\">0.5626</td><td style = \"text-align: right;\">0.4376</td><td style = \"text-align: right;\">0.2617</td><td style = \"text-align: right;\">0.1199</td><td style = \"text-align: right;\">0.6676</td><td style = \"text-align: right;\">0.9402</td><td style = \"text-align: right;\">0.7832</td><td style = \"text-align: right;\">0.5352</td><td style = \"text-align: right;\">0.6809</td><td style = \"text-align: right;\">0.9174</td><td style = \"text-align: right;\">0.7613</td><td style = \"text-align: right;\">0.822</td><td style = \"text-align: right;\">0.8872</td><td style = \"text-align: right;\">0.6091</td><td style = \"text-align: right;\">0.2967</td><td style = \"text-align: right;\">0.1103</td><td style = \"text-align: right;\">0.1318</td><td style = \"text-align: right;\">0.0624</td><td style = \"text-align: right;\">0.099</td><td style = \"text-align: right;\">0.4006</td><td style = \"text-align: right;\">0.3666</td><td style = \"text-align: right;\">0.105</td><td style = \"text-align: right;\">0.1915</td><td style = \"text-align: right;\">0.393</td><td style = \"text-align: right;\">0.4288</td><td style = \"text-align: right;\">0.2546</td><td style = \"text-align: right;\">0.1151</td><td style = \"text-align: right;\">0.2196</td><td style = \"text-align: right;\">0.1879</td><td style = \"text-align: right;\">0.1437</td><td style = \"text-align: right;\">0.2146</td><td style = \"text-align: right;\">0.236</td><td style = \"text-align: right;\">0.1125</td><td style = \"text-align: right;\">0.0254</td><td style = \"text-align: right;\">0.0285</td><td style = \"text-align: right;\">0.0178</td><td style = \"text-align: right;\">0.0052</td><td style = \"text-align: right;\">0.0081</td><td style = \"text-align: right;\">0.012</td><td style = \"text-align: right;\">0.0045</td><td style = \"text-align: right;\">0.0121</td><td style = \"text-align: right;\">0.0097</td><td style = \"text-align: right;\">0.0085</td><td style = \"text-align: right;\">0.0047</td><td style = \"text-align: right;\">0.0048</td><td style = \"text-align: right;\">0.0053</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: right;\">0.0223</td><td style = \"text-align: right;\">0.0375</td><td style = \"text-align: right;\">0.0484</td><td style = \"text-align: right;\">0.0475</td><td style = \"text-align: right;\">0.0647</td><td style = \"text-align: right;\">0.0591</td><td style = \"text-align: right;\">0.0753</td><td style = \"text-align: right;\">0.0098</td><td style = \"text-align: right;\">0.0684</td><td style = \"text-align: right;\">0.1487</td><td style = \"text-align: right;\">0.1156</td><td style = \"text-align: right;\">0.1654</td><td style = \"text-align: right;\">0.3833</td><td style = \"text-align: right;\">0.3598</td><td style = \"text-align: right;\">0.1713</td><td style = \"text-align: right;\">0.1136</td><td style = \"text-align: right;\">0.0349</td><td style = \"text-align: right;\">0.3796</td><td style = \"text-align: right;\">0.7401</td><td style = \"text-align: right;\">0.9925</td><td style = \"text-align: right;\">0.9802</td><td style = \"text-align: right;\">0.889</td><td style = \"text-align: right;\">0.6712</td><td style = \"text-align: right;\">0.4286</td><td style = \"text-align: right;\">0.3374</td><td style = \"text-align: right;\">0.7366</td><td style = \"text-align: right;\">0.9611</td><td style = \"text-align: right;\">0.7353</td><td style = \"text-align: right;\">0.4856</td><td style = \"text-align: right;\">0.1594</td><td style = \"text-align: right;\">0.3007</td><td style = \"text-align: right;\">0.4096</td><td style = \"text-align: right;\">0.317</td><td style = \"text-align: right;\">0.3305</td><td style = \"text-align: right;\">0.3408</td><td style = \"text-align: right;\">0.2186</td><td style = \"text-align: right;\">0.2463</td><td style = \"text-align: right;\">0.2726</td><td style = \"text-align: right;\">0.168</td><td style = \"text-align: right;\">0.2792</td><td style = \"text-align: right;\">0.2558</td><td style = \"text-align: right;\">0.174</td><td style = \"text-align: right;\">0.2121</td><td style = \"text-align: right;\">0.1099</td><td style = \"text-align: right;\">0.0985</td><td style = \"text-align: right;\">0.1271</td><td style = \"text-align: right;\">0.1459</td><td style = \"text-align: right;\">0.1164</td><td style = \"text-align: right;\">0.0777</td><td style = \"text-align: right;\">0.0439</td><td style = \"text-align: right;\">0.0061</td><td style = \"text-align: right;\">0.0145</td><td style = \"text-align: right;\">0.0128</td><td style = \"text-align: right;\">0.0145</td><td style = \"text-align: right;\">0.0058</td><td style = \"text-align: right;\">0.0049</td><td style = \"text-align: right;\">0.0065</td><td style = \"text-align: right;\">0.0093</td><td style = \"text-align: right;\">0.0059</td><td style = \"text-align: right;\">0.0022</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: right;\">0.0164</td><td style = \"text-align: right;\">0.0173</td><td style = \"text-align: right;\">0.0347</td><td style = \"text-align: right;\">0.007</td><td style = \"text-align: right;\">0.0187</td><td style = \"text-align: right;\">0.0671</td><td style = \"text-align: right;\">0.1056</td><td style = \"text-align: right;\">0.0697</td><td style = \"text-align: right;\">0.0962</td><td style = \"text-align: right;\">0.0251</td><td style = \"text-align: right;\">0.0801</td><td style = \"text-align: right;\">0.1056</td><td style = \"text-align: right;\">0.1266</td><td style = \"text-align: right;\">0.089</td><td style = \"text-align: right;\">0.0198</td><td style = \"text-align: right;\">0.1133</td><td style = \"text-align: right;\">0.2826</td><td style = \"text-align: right;\">0.3234</td><td style = \"text-align: right;\">0.3238</td><td style = \"text-align: right;\">0.4333</td><td style = \"text-align: right;\">0.6068</td><td style = \"text-align: right;\">0.7652</td><td style = \"text-align: right;\">0.9203</td><td style = \"text-align: right;\">0.9719</td><td style = \"text-align: right;\">0.9207</td><td style = \"text-align: right;\">0.7545</td><td style = \"text-align: right;\">0.8289</td><td style = \"text-align: right;\">0.8907</td><td style = \"text-align: right;\">0.7309</td><td style = \"text-align: right;\">0.6896</td><td style = \"text-align: right;\">0.5829</td><td style = \"text-align: right;\">0.4935</td><td style = \"text-align: right;\">0.3101</td><td style = \"text-align: right;\">0.0306</td><td style = \"text-align: right;\">0.0244</td><td style = \"text-align: right;\">0.1108</td><td style = \"text-align: right;\">0.1594</td><td style = \"text-align: right;\">0.1371</td><td style = \"text-align: right;\">0.0696</td><td style = \"text-align: right;\">0.0452</td><td style = \"text-align: right;\">0.062</td><td style = \"text-align: right;\">0.1421</td><td style = \"text-align: right;\">0.1597</td><td style = \"text-align: right;\">0.1384</td><td style = \"text-align: right;\">0.0372</td><td style = \"text-align: right;\">0.0688</td><td style = \"text-align: right;\">0.0867</td><td style = \"text-align: right;\">0.0513</td><td style = \"text-align: right;\">0.0092</td><td style = \"text-align: right;\">0.0198</td><td style = \"text-align: right;\">0.0118</td><td style = \"text-align: right;\">0.009</td><td style = \"text-align: right;\">0.0223</td><td style = \"text-align: right;\">0.0179</td><td style = \"text-align: right;\">0.0084</td><td style = \"text-align: right;\">0.0068</td><td style = \"text-align: right;\">0.0032</td><td style = \"text-align: right;\">0.0035</td><td style = \"text-align: right;\">0.0056</td><td style = \"text-align: right;\">0.004</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: right;\">0.0039</td><td style = \"text-align: right;\">0.0063</td><td style = \"text-align: right;\">0.0152</td><td style = \"text-align: right;\">0.0336</td><td style = \"text-align: right;\">0.031</td><td style = \"text-align: right;\">0.0284</td><td style = \"text-align: right;\">0.0396</td><td style = \"text-align: right;\">0.0272</td><td style = \"text-align: right;\">0.0323</td><td style = \"text-align: right;\">0.0452</td><td style = \"text-align: right;\">0.0492</td><td style = \"text-align: right;\">0.0996</td><td style = \"text-align: right;\">0.1424</td><td style = \"text-align: right;\">0.1194</td><td style = \"text-align: right;\">0.0628</td><td style = \"text-align: right;\">0.0907</td><td style = \"text-align: right;\">0.1177</td><td style = \"text-align: right;\">0.1429</td><td style = \"text-align: right;\">0.1223</td><td style = \"text-align: right;\">0.1104</td><td style = \"text-align: right;\">0.1847</td><td style = \"text-align: right;\">0.3715</td><td style = \"text-align: right;\">0.4382</td><td style = \"text-align: right;\">0.5707</td><td style = \"text-align: right;\">0.6654</td><td style = \"text-align: right;\">0.7476</td><td style = \"text-align: right;\">0.7654</td><td style = \"text-align: right;\">0.8555</td><td style = \"text-align: right;\">0.972</td><td style = \"text-align: right;\">0.9221</td><td style = \"text-align: right;\">0.7502</td><td style = \"text-align: right;\">0.7209</td><td style = \"text-align: right;\">0.7757</td><td style = \"text-align: right;\">0.6055</td><td style = \"text-align: right;\">0.5021</td><td style = \"text-align: right;\">0.4499</td><td style = \"text-align: right;\">0.3947</td><td style = \"text-align: right;\">0.4281</td><td style = \"text-align: right;\">0.4427</td><td style = \"text-align: right;\">0.3749</td><td style = \"text-align: right;\">0.1972</td><td style = \"text-align: right;\">0.0511</td><td style = \"text-align: right;\">0.0793</td><td style = \"text-align: right;\">0.1269</td><td style = \"text-align: right;\">0.1533</td><td style = \"text-align: right;\">0.069</td><td style = \"text-align: right;\">0.0402</td><td style = \"text-align: right;\">0.0534</td><td style = \"text-align: right;\">0.0228</td><td style = \"text-align: right;\">0.0073</td><td style = \"text-align: right;\">0.0062</td><td style = \"text-align: right;\">0.0062</td><td style = \"text-align: right;\">0.012</td><td style = \"text-align: right;\">0.0052</td><td style = \"text-align: right;\">0.0056</td><td style = \"text-align: right;\">0.0093</td><td style = \"text-align: right;\">0.0042</td><td style = \"text-align: right;\">0.0003</td><td style = \"text-align: right;\">0.0053</td><td style = \"text-align: right;\">0.0036</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: right;\">0.0123</td><td style = \"text-align: right;\">0.0309</td><td style = \"text-align: right;\">0.0169</td><td style = \"text-align: right;\">0.0313</td><td style = \"text-align: right;\">0.0358</td><td style = \"text-align: right;\">0.0102</td><td style = \"text-align: right;\">0.0182</td><td style = \"text-align: right;\">0.0579</td><td style = \"text-align: right;\">0.1122</td><td style = \"text-align: right;\">0.0835</td><td style = \"text-align: right;\">0.0548</td><td style = \"text-align: right;\">0.0847</td><td style = \"text-align: right;\">0.2026</td><td style = \"text-align: right;\">0.2557</td><td style = \"text-align: right;\">0.187</td><td style = \"text-align: right;\">0.2032</td><td style = \"text-align: right;\">0.1463</td><td style = \"text-align: right;\">0.2849</td><td style = \"text-align: right;\">0.5824</td><td style = \"text-align: right;\">0.7728</td><td style = \"text-align: right;\">0.7852</td><td style = \"text-align: right;\">0.8515</td><td style = \"text-align: right;\">0.5312</td><td style = \"text-align: right;\">0.3653</td><td style = \"text-align: right;\">0.5973</td><td style = \"text-align: right;\">0.8275</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.8673</td><td style = \"text-align: right;\">0.6301</td><td style = \"text-align: right;\">0.4591</td><td style = \"text-align: right;\">0.394</td><td style = \"text-align: right;\">0.2576</td><td style = \"text-align: right;\">0.2817</td><td style = \"text-align: right;\">0.2641</td><td style = \"text-align: right;\">0.2757</td><td style = \"text-align: right;\">0.2698</td><td style = \"text-align: right;\">0.3994</td><td style = \"text-align: right;\">0.4576</td><td style = \"text-align: right;\">0.394</td><td style = \"text-align: right;\">0.2522</td><td style = \"text-align: right;\">0.1782</td><td style = \"text-align: right;\">0.1354</td><td style = \"text-align: right;\">0.0516</td><td style = \"text-align: right;\">0.0337</td><td style = \"text-align: right;\">0.0894</td><td style = \"text-align: right;\">0.0861</td><td style = \"text-align: right;\">0.0872</td><td style = \"text-align: right;\">0.0445</td><td style = \"text-align: right;\">0.0134</td><td style = \"text-align: right;\">0.0217</td><td style = \"text-align: right;\">0.0188</td><td style = \"text-align: right;\">0.0133</td><td style = \"text-align: right;\">0.0265</td><td style = \"text-align: right;\">0.0224</td><td style = \"text-align: right;\">0.0074</td><td style = \"text-align: right;\">0.0118</td><td style = \"text-align: right;\">0.0026</td><td style = \"text-align: right;\">0.0092</td><td style = \"text-align: right;\">0.0009</td><td style = \"text-align: right;\">0.0044</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: right;\">0.0079</td><td style = \"text-align: right;\">0.0086</td><td style = \"text-align: right;\">0.0055</td><td style = \"text-align: right;\">0.025</td><td style = \"text-align: right;\">0.0344</td><td style = \"text-align: right;\">0.0546</td><td style = \"text-align: right;\">0.0528</td><td style = \"text-align: right;\">0.0958</td><td style = \"text-align: right;\">0.1009</td><td style = \"text-align: right;\">0.124</td><td style = \"text-align: right;\">0.1097</td><td style = \"text-align: right;\">0.1215</td><td style = \"text-align: right;\">0.1874</td><td style = \"text-align: right;\">0.3383</td><td style = \"text-align: right;\">0.3227</td><td style = \"text-align: right;\">0.2723</td><td style = \"text-align: right;\">0.3943</td><td style = \"text-align: right;\">0.6432</td><td style = \"text-align: right;\">0.7271</td><td style = \"text-align: right;\">0.8673</td><td style = \"text-align: right;\">0.9674</td><td style = \"text-align: right;\">0.9847</td><td style = \"text-align: right;\">0.948</td><td style = \"text-align: right;\">0.8036</td><td style = \"text-align: right;\">0.6833</td><td style = \"text-align: right;\">0.5136</td><td style = \"text-align: right;\">0.309</td><td style = \"text-align: right;\">0.0832</td><td style = \"text-align: right;\">0.4019</td><td style = \"text-align: right;\">0.2344</td><td style = \"text-align: right;\">0.1905</td><td style = \"text-align: right;\">0.1235</td><td style = \"text-align: right;\">0.1717</td><td style = \"text-align: right;\">0.2351</td><td style = \"text-align: right;\">0.2489</td><td style = \"text-align: right;\">0.3649</td><td style = \"text-align: right;\">0.3382</td><td style = \"text-align: right;\">0.1589</td><td style = \"text-align: right;\">0.0989</td><td style = \"text-align: right;\">0.1089</td><td style = \"text-align: right;\">0.1043</td><td style = \"text-align: right;\">0.0839</td><td style = \"text-align: right;\">0.1391</td><td style = \"text-align: right;\">0.0819</td><td style = \"text-align: right;\">0.0678</td><td style = \"text-align: right;\">0.0663</td><td style = \"text-align: right;\">0.1202</td><td style = \"text-align: right;\">0.0692</td><td style = \"text-align: right;\">0.0152</td><td style = \"text-align: right;\">0.0266</td><td style = \"text-align: right;\">0.0174</td><td style = \"text-align: right;\">0.0176</td><td style = \"text-align: right;\">0.0127</td><td style = \"text-align: right;\">0.0088</td><td style = \"text-align: right;\">0.0098</td><td style = \"text-align: right;\">0.0019</td><td style = \"text-align: right;\">0.0059</td><td style = \"text-align: right;\">0.0058</td><td style = \"text-align: right;\">0.0059</td><td style = \"text-align: right;\">0.0032</td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">false</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">197</td><td style = \"text-align: right;\">0.005</td><td style = \"text-align: right;\">0.0017</td><td style = \"text-align: right;\">0.027</td><td style = \"text-align: right;\">0.045</td><td style = \"text-align: right;\">0.0958</td><td style = \"text-align: right;\">0.083</td><td style = \"text-align: right;\">0.0879</td><td style = \"text-align: right;\">0.122</td><td style = \"text-align: right;\">0.1977</td><td style = \"text-align: right;\">0.2282</td><td style = \"text-align: right;\">0.2521</td><td style = \"text-align: right;\">0.3484</td><td style = \"text-align: right;\">0.3309</td><td style = \"text-align: right;\">0.2614</td><td style = \"text-align: right;\">0.1782</td><td style = \"text-align: right;\">0.2055</td><td style = \"text-align: right;\">0.2298</td><td style = \"text-align: right;\">0.3545</td><td style = \"text-align: right;\">0.6218</td><td style = \"text-align: right;\">0.7265</td><td style = \"text-align: right;\">0.8346</td><td style = \"text-align: right;\">0.8268</td><td style = \"text-align: right;\">0.8366</td><td style = \"text-align: right;\">0.9408</td><td style = \"text-align: right;\">0.951</td><td style = \"text-align: right;\">0.9801</td><td style = \"text-align: right;\">0.9974</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.9036</td><td style = \"text-align: right;\">0.6409</td><td style = \"text-align: right;\">0.3857</td><td style = \"text-align: right;\">0.2908</td><td style = \"text-align: right;\">0.204</td><td style = \"text-align: right;\">0.1653</td><td style = \"text-align: right;\">0.1769</td><td style = \"text-align: right;\">0.114</td><td style = \"text-align: right;\">0.074</td><td style = \"text-align: right;\">0.0941</td><td style = \"text-align: right;\">0.0621</td><td style = \"text-align: right;\">0.0426</td><td style = \"text-align: right;\">0.0572</td><td style = \"text-align: right;\">0.1068</td><td style = \"text-align: right;\">0.1909</td><td style = \"text-align: right;\">0.2229</td><td style = \"text-align: right;\">0.2203</td><td style = \"text-align: right;\">0.2265</td><td style = \"text-align: right;\">0.1766</td><td style = \"text-align: right;\">0.1097</td><td style = \"text-align: right;\">0.0558</td><td style = \"text-align: right;\">0.0142</td><td style = \"text-align: right;\">0.0281</td><td style = \"text-align: right;\">0.0165</td><td style = \"text-align: right;\">0.0056</td><td style = \"text-align: right;\">0.001</td><td style = \"text-align: right;\">0.0027</td><td style = \"text-align: right;\">0.0062</td><td style = \"text-align: right;\">0.0024</td><td style = \"text-align: right;\">0.0063</td><td style = \"text-align: right;\">0.0017</td><td style = \"text-align: right;\">0.0028</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">198</td><td style = \"text-align: right;\">0.0366</td><td style = \"text-align: right;\">0.0421</td><td style = \"text-align: right;\">0.0504</td><td style = \"text-align: right;\">0.025</td><td style = \"text-align: right;\">0.0596</td><td style = \"text-align: right;\">0.0252</td><td style = \"text-align: right;\">0.0958</td><td style = \"text-align: right;\">0.0991</td><td style = \"text-align: right;\">0.1419</td><td style = \"text-align: right;\">0.1847</td><td style = \"text-align: right;\">0.2222</td><td style = \"text-align: right;\">0.2648</td><td style = \"text-align: right;\">0.2508</td><td style = \"text-align: right;\">0.2291</td><td style = \"text-align: right;\">0.1555</td><td style = \"text-align: right;\">0.1863</td><td style = \"text-align: right;\">0.2387</td><td style = \"text-align: right;\">0.3345</td><td style = \"text-align: right;\">0.5233</td><td style = \"text-align: right;\">0.6684</td><td style = \"text-align: right;\">0.7766</td><td style = \"text-align: right;\">0.7928</td><td style = \"text-align: right;\">0.794</td><td style = \"text-align: right;\">0.9129</td><td style = \"text-align: right;\">0.9498</td><td style = \"text-align: right;\">0.9835</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.9471</td><td style = \"text-align: right;\">0.8237</td><td style = \"text-align: right;\">0.6252</td><td style = \"text-align: right;\">0.4181</td><td style = \"text-align: right;\">0.3209</td><td style = \"text-align: right;\">0.2658</td><td style = \"text-align: right;\">0.2196</td><td style = \"text-align: right;\">0.1588</td><td style = \"text-align: right;\">0.0561</td><td style = \"text-align: right;\">0.0948</td><td style = \"text-align: right;\">0.17</td><td style = \"text-align: right;\">0.1215</td><td style = \"text-align: right;\">0.1282</td><td style = \"text-align: right;\">0.0386</td><td style = \"text-align: right;\">0.1329</td><td style = \"text-align: right;\">0.2331</td><td style = \"text-align: right;\">0.2468</td><td style = \"text-align: right;\">0.196</td><td style = \"text-align: right;\">0.1985</td><td style = \"text-align: right;\">0.157</td><td style = \"text-align: right;\">0.0921</td><td style = \"text-align: right;\">0.0549</td><td style = \"text-align: right;\">0.0194</td><td style = \"text-align: right;\">0.0166</td><td style = \"text-align: right;\">0.0132</td><td style = \"text-align: right;\">0.0027</td><td style = \"text-align: right;\">0.0022</td><td style = \"text-align: right;\">0.0059</td><td style = \"text-align: right;\">0.0016</td><td style = \"text-align: right;\">0.0025</td><td style = \"text-align: right;\">0.0017</td><td style = \"text-align: right;\">0.0027</td><td style = \"text-align: right;\">0.0027</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">199</td><td style = \"text-align: right;\">0.0238</td><td style = \"text-align: right;\">0.0318</td><td style = \"text-align: right;\">0.0422</td><td style = \"text-align: right;\">0.0399</td><td style = \"text-align: right;\">0.0788</td><td style = \"text-align: right;\">0.0766</td><td style = \"text-align: right;\">0.0881</td><td style = \"text-align: right;\">0.1143</td><td style = \"text-align: right;\">0.1594</td><td style = \"text-align: right;\">0.2048</td><td style = \"text-align: right;\">0.2652</td><td style = \"text-align: right;\">0.31</td><td style = \"text-align: right;\">0.2381</td><td style = \"text-align: right;\">0.1918</td><td style = \"text-align: right;\">0.143</td><td style = \"text-align: right;\">0.1735</td><td style = \"text-align: right;\">0.1781</td><td style = \"text-align: right;\">0.2852</td><td style = \"text-align: right;\">0.5036</td><td style = \"text-align: right;\">0.6166</td><td style = \"text-align: right;\">0.7616</td><td style = \"text-align: right;\">0.8125</td><td style = \"text-align: right;\">0.7793</td><td style = \"text-align: right;\">0.8788</td><td style = \"text-align: right;\">0.8813</td><td style = \"text-align: right;\">0.947</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.9739</td><td style = \"text-align: right;\">0.8446</td><td style = \"text-align: right;\">0.6151</td><td style = \"text-align: right;\">0.4302</td><td style = \"text-align: right;\">0.3165</td><td style = \"text-align: right;\">0.2869</td><td style = \"text-align: right;\">0.2017</td><td style = \"text-align: right;\">0.1206</td><td style = \"text-align: right;\">0.0271</td><td style = \"text-align: right;\">0.058</td><td style = \"text-align: right;\">0.1262</td><td style = \"text-align: right;\">0.1072</td><td style = \"text-align: right;\">0.1082</td><td style = \"text-align: right;\">0.036</td><td style = \"text-align: right;\">0.1197</td><td style = \"text-align: right;\">0.2061</td><td style = \"text-align: right;\">0.2054</td><td style = \"text-align: right;\">0.1878</td><td style = \"text-align: right;\">0.2047</td><td style = \"text-align: right;\">0.1716</td><td style = \"text-align: right;\">0.1069</td><td style = \"text-align: right;\">0.0477</td><td style = \"text-align: right;\">0.017</td><td style = \"text-align: right;\">0.0186</td><td style = \"text-align: right;\">0.0096</td><td style = \"text-align: right;\">0.0071</td><td style = \"text-align: right;\">0.0084</td><td style = \"text-align: right;\">0.0038</td><td style = \"text-align: right;\">0.0026</td><td style = \"text-align: right;\">0.0028</td><td style = \"text-align: right;\">0.0013</td><td style = \"text-align: right;\">0.0035</td><td style = \"text-align: right;\">0.006</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">200</td><td style = \"text-align: right;\">0.0116</td><td style = \"text-align: right;\">0.0744</td><td style = \"text-align: right;\">0.0367</td><td style = \"text-align: right;\">0.0225</td><td style = \"text-align: right;\">0.0076</td><td style = \"text-align: right;\">0.0545</td><td style = \"text-align: right;\">0.111</td><td style = \"text-align: right;\">0.1069</td><td style = \"text-align: right;\">0.1708</td><td style = \"text-align: right;\">0.2271</td><td style = \"text-align: right;\">0.3171</td><td style = \"text-align: right;\">0.2882</td><td style = \"text-align: right;\">0.2657</td><td style = \"text-align: right;\">0.2307</td><td style = \"text-align: right;\">0.1889</td><td style = \"text-align: right;\">0.1791</td><td style = \"text-align: right;\">0.2298</td><td style = \"text-align: right;\">0.3715</td><td style = \"text-align: right;\">0.6223</td><td style = \"text-align: right;\">0.726</td><td style = \"text-align: right;\">0.7934</td><td style = \"text-align: right;\">0.8045</td><td style = \"text-align: right;\">0.8067</td><td style = \"text-align: right;\">0.9173</td><td style = \"text-align: right;\">0.9327</td><td style = \"text-align: right;\">0.9562</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.9818</td><td style = \"text-align: right;\">0.8684</td><td style = \"text-align: right;\">0.6381</td><td style = \"text-align: right;\">0.3997</td><td style = \"text-align: right;\">0.3242</td><td style = \"text-align: right;\">0.2835</td><td style = \"text-align: right;\">0.2413</td><td style = \"text-align: right;\">0.2321</td><td style = \"text-align: right;\">0.126</td><td style = \"text-align: right;\">0.0693</td><td style = \"text-align: right;\">0.0701</td><td style = \"text-align: right;\">0.1439</td><td style = \"text-align: right;\">0.1475</td><td style = \"text-align: right;\">0.0438</td><td style = \"text-align: right;\">0.0469</td><td style = \"text-align: right;\">0.1476</td><td style = \"text-align: right;\">0.1742</td><td style = \"text-align: right;\">0.1555</td><td style = \"text-align: right;\">0.1651</td><td style = \"text-align: right;\">0.1181</td><td style = \"text-align: right;\">0.072</td><td style = \"text-align: right;\">0.0321</td><td style = \"text-align: right;\">0.0056</td><td style = \"text-align: right;\">0.0202</td><td style = \"text-align: right;\">0.0141</td><td style = \"text-align: right;\">0.0103</td><td style = \"text-align: right;\">0.01</td><td style = \"text-align: right;\">0.0034</td><td style = \"text-align: right;\">0.0026</td><td style = \"text-align: right;\">0.0037</td><td style = \"text-align: right;\">0.0044</td><td style = \"text-align: right;\">0.0057</td><td style = \"text-align: right;\">0.0035</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">201</td><td style = \"text-align: right;\">0.0131</td><td style = \"text-align: right;\">0.0387</td><td style = \"text-align: right;\">0.0329</td><td style = \"text-align: right;\">0.0078</td><td style = \"text-align: right;\">0.0721</td><td style = \"text-align: right;\">0.1341</td><td style = \"text-align: right;\">0.1626</td><td style = \"text-align: right;\">0.1902</td><td style = \"text-align: right;\">0.261</td><td style = \"text-align: right;\">0.3193</td><td style = \"text-align: right;\">0.3468</td><td style = \"text-align: right;\">0.3738</td><td style = \"text-align: right;\">0.3055</td><td style = \"text-align: right;\">0.1926</td><td style = \"text-align: right;\">0.1385</td><td style = \"text-align: right;\">0.2122</td><td style = \"text-align: right;\">0.2758</td><td style = \"text-align: right;\">0.4576</td><td style = \"text-align: right;\">0.6487</td><td style = \"text-align: right;\">0.7154</td><td style = \"text-align: right;\">0.801</td><td style = \"text-align: right;\">0.7924</td><td style = \"text-align: right;\">0.8793</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.9865</td><td style = \"text-align: right;\">0.9474</td><td style = \"text-align: right;\">0.9474</td><td style = \"text-align: right;\">0.9315</td><td style = \"text-align: right;\">0.8326</td><td style = \"text-align: right;\">0.6213</td><td style = \"text-align: right;\">0.3772</td><td style = \"text-align: right;\">0.2822</td><td style = \"text-align: right;\">0.2042</td><td style = \"text-align: right;\">0.219</td><td style = \"text-align: right;\">0.2223</td><td style = \"text-align: right;\">0.1327</td><td style = \"text-align: right;\">0.0521</td><td style = \"text-align: right;\">0.0618</td><td style = \"text-align: right;\">0.1416</td><td style = \"text-align: right;\">0.146</td><td style = \"text-align: right;\">0.0846</td><td style = \"text-align: right;\">0.1055</td><td style = \"text-align: right;\">0.1639</td><td style = \"text-align: right;\">0.1916</td><td style = \"text-align: right;\">0.2085</td><td style = \"text-align: right;\">0.2335</td><td style = \"text-align: right;\">0.1964</td><td style = \"text-align: right;\">0.13</td><td style = \"text-align: right;\">0.0633</td><td style = \"text-align: right;\">0.0183</td><td style = \"text-align: right;\">0.0137</td><td style = \"text-align: right;\">0.015</td><td style = \"text-align: right;\">0.0076</td><td style = \"text-align: right;\">0.0032</td><td style = \"text-align: right;\">0.0037</td><td style = \"text-align: right;\">0.0071</td><td style = \"text-align: right;\">0.004</td><td style = \"text-align: right;\">0.0009</td><td style = \"text-align: right;\">0.0015</td><td style = \"text-align: right;\">0.0085</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">202</td><td style = \"text-align: right;\">0.0335</td><td style = \"text-align: right;\">0.0258</td><td style = \"text-align: right;\">0.0398</td><td style = \"text-align: right;\">0.057</td><td style = \"text-align: right;\">0.0529</td><td style = \"text-align: right;\">0.1091</td><td style = \"text-align: right;\">0.1709</td><td style = \"text-align: right;\">0.1684</td><td style = \"text-align: right;\">0.1865</td><td style = \"text-align: right;\">0.266</td><td style = \"text-align: right;\">0.3188</td><td style = \"text-align: right;\">0.3553</td><td style = \"text-align: right;\">0.3116</td><td style = \"text-align: right;\">0.1965</td><td style = \"text-align: right;\">0.178</td><td style = \"text-align: right;\">0.2794</td><td style = \"text-align: right;\">0.287</td><td style = \"text-align: right;\">0.3969</td><td style = \"text-align: right;\">0.5599</td><td style = \"text-align: right;\">0.6936</td><td style = \"text-align: right;\">0.7969</td><td style = \"text-align: right;\">0.7452</td><td style = \"text-align: right;\">0.8203</td><td style = \"text-align: right;\">0.9261</td><td style = \"text-align: right;\">0.881</td><td style = \"text-align: right;\">0.8814</td><td style = \"text-align: right;\">0.9301</td><td style = \"text-align: right;\">0.9955</td><td style = \"text-align: right;\">0.8576</td><td style = \"text-align: right;\">0.6069</td><td style = \"text-align: right;\">0.3934</td><td style = \"text-align: right;\">0.2464</td><td style = \"text-align: right;\">0.1645</td><td style = \"text-align: right;\">0.114</td><td style = \"text-align: right;\">0.0956</td><td style = \"text-align: right;\">0.008</td><td style = \"text-align: right;\">0.0702</td><td style = \"text-align: right;\">0.0936</td><td style = \"text-align: right;\">0.0894</td><td style = \"text-align: right;\">0.1127</td><td style = \"text-align: right;\">0.0873</td><td style = \"text-align: right;\">0.102</td><td style = \"text-align: right;\">0.1964</td><td style = \"text-align: right;\">0.2256</td><td style = \"text-align: right;\">0.1814</td><td style = \"text-align: right;\">0.2012</td><td style = \"text-align: right;\">0.1688</td><td style = \"text-align: right;\">0.1037</td><td style = \"text-align: right;\">0.0501</td><td style = \"text-align: right;\">0.0136</td><td style = \"text-align: right;\">0.013</td><td style = \"text-align: right;\">0.012</td><td style = \"text-align: right;\">0.0039</td><td style = \"text-align: right;\">0.0053</td><td style = \"text-align: right;\">0.0062</td><td style = \"text-align: right;\">0.0046</td><td style = \"text-align: right;\">0.0045</td><td style = \"text-align: right;\">0.0022</td><td style = \"text-align: right;\">0.0005</td><td style = \"text-align: right;\">0.0031</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">203</td><td style = \"text-align: right;\">0.0272</td><td style = \"text-align: right;\">0.0378</td><td style = \"text-align: right;\">0.0488</td><td style = \"text-align: right;\">0.0848</td><td style = \"text-align: right;\">0.1127</td><td style = \"text-align: right;\">0.1103</td><td style = \"text-align: right;\">0.1349</td><td style = \"text-align: right;\">0.2337</td><td style = \"text-align: right;\">0.3113</td><td style = \"text-align: right;\">0.3997</td><td style = \"text-align: right;\">0.3941</td><td style = \"text-align: right;\">0.3309</td><td style = \"text-align: right;\">0.2926</td><td style = \"text-align: right;\">0.176</td><td style = \"text-align: right;\">0.1739</td><td style = \"text-align: right;\">0.2043</td><td style = \"text-align: right;\">0.2088</td><td style = \"text-align: right;\">0.2678</td><td style = \"text-align: right;\">0.2434</td><td style = \"text-align: right;\">0.1839</td><td style = \"text-align: right;\">0.2802</td><td style = \"text-align: right;\">0.6172</td><td style = \"text-align: right;\">0.8015</td><td style = \"text-align: right;\">0.8313</td><td style = \"text-align: right;\">0.844</td><td style = \"text-align: right;\">0.8494</td><td style = \"text-align: right;\">0.9168</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.7896</td><td style = \"text-align: right;\">0.5371</td><td style = \"text-align: right;\">0.6472</td><td style = \"text-align: right;\">0.6505</td><td style = \"text-align: right;\">0.4959</td><td style = \"text-align: right;\">0.2175</td><td style = \"text-align: right;\">0.099</td><td style = \"text-align: right;\">0.0434</td><td style = \"text-align: right;\">0.1708</td><td style = \"text-align: right;\">0.1979</td><td style = \"text-align: right;\">0.188</td><td style = \"text-align: right;\">0.1108</td><td style = \"text-align: right;\">0.1702</td><td style = \"text-align: right;\">0.0585</td><td style = \"text-align: right;\">0.0638</td><td style = \"text-align: right;\">0.1391</td><td style = \"text-align: right;\">0.0638</td><td style = \"text-align: right;\">0.0581</td><td style = \"text-align: right;\">0.0641</td><td style = \"text-align: right;\">0.1044</td><td style = \"text-align: right;\">0.0732</td><td style = \"text-align: right;\">0.0275</td><td style = \"text-align: right;\">0.0146</td><td style = \"text-align: right;\">0.0091</td><td style = \"text-align: right;\">0.0045</td><td style = \"text-align: right;\">0.0043</td><td style = \"text-align: right;\">0.0043</td><td style = \"text-align: right;\">0.0098</td><td style = \"text-align: right;\">0.0054</td><td style = \"text-align: right;\">0.0051</td><td style = \"text-align: right;\">0.0065</td><td style = \"text-align: right;\">0.0103</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">204</td><td style = \"text-align: right;\">0.0187</td><td style = \"text-align: right;\">0.0346</td><td style = \"text-align: right;\">0.0168</td><td style = \"text-align: right;\">0.0177</td><td style = \"text-align: right;\">0.0393</td><td style = \"text-align: right;\">0.163</td><td style = \"text-align: right;\">0.2028</td><td style = \"text-align: right;\">0.1694</td><td style = \"text-align: right;\">0.2328</td><td style = \"text-align: right;\">0.2684</td><td style = \"text-align: right;\">0.3108</td><td style = \"text-align: right;\">0.2933</td><td style = \"text-align: right;\">0.2275</td><td style = \"text-align: right;\">0.0994</td><td style = \"text-align: right;\">0.1801</td><td style = \"text-align: right;\">0.22</td><td style = \"text-align: right;\">0.2732</td><td style = \"text-align: right;\">0.2862</td><td style = \"text-align: right;\">0.2034</td><td style = \"text-align: right;\">0.174</td><td style = \"text-align: right;\">0.413</td><td style = \"text-align: right;\">0.6879</td><td style = \"text-align: right;\">0.812</td><td style = \"text-align: right;\">0.8453</td><td style = \"text-align: right;\">0.8919</td><td style = \"text-align: right;\">0.93</td><td style = \"text-align: right;\">0.9987</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.8104</td><td style = \"text-align: right;\">0.6199</td><td style = \"text-align: right;\">0.6041</td><td style = \"text-align: right;\">0.5547</td><td style = \"text-align: right;\">0.416</td><td style = \"text-align: right;\">0.1472</td><td style = \"text-align: right;\">0.0849</td><td style = \"text-align: right;\">0.0608</td><td style = \"text-align: right;\">0.0969</td><td style = \"text-align: right;\">0.1411</td><td style = \"text-align: right;\">0.1676</td><td style = \"text-align: right;\">0.12</td><td style = \"text-align: right;\">0.1201</td><td style = \"text-align: right;\">0.1036</td><td style = \"text-align: right;\">0.1977</td><td style = \"text-align: right;\">0.1339</td><td style = \"text-align: right;\">0.0902</td><td style = \"text-align: right;\">0.1085</td><td style = \"text-align: right;\">0.1521</td><td style = \"text-align: right;\">0.1363</td><td style = \"text-align: right;\">0.0858</td><td style = \"text-align: right;\">0.029</td><td style = \"text-align: right;\">0.0203</td><td style = \"text-align: right;\">0.0116</td><td style = \"text-align: right;\">0.0098</td><td style = \"text-align: right;\">0.0199</td><td style = \"text-align: right;\">0.0033</td><td style = \"text-align: right;\">0.0101</td><td style = \"text-align: right;\">0.0065</td><td style = \"text-align: right;\">0.0115</td><td style = \"text-align: right;\">0.0193</td><td style = \"text-align: right;\">0.0157</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">205</td><td style = \"text-align: right;\">0.0323</td><td style = \"text-align: right;\">0.0101</td><td style = \"text-align: right;\">0.0298</td><td style = \"text-align: right;\">0.0564</td><td style = \"text-align: right;\">0.076</td><td style = \"text-align: right;\">0.0958</td><td style = \"text-align: right;\">0.099</td><td style = \"text-align: right;\">0.1018</td><td style = \"text-align: right;\">0.103</td><td style = \"text-align: right;\">0.2154</td><td style = \"text-align: right;\">0.3085</td><td style = \"text-align: right;\">0.3425</td><td style = \"text-align: right;\">0.299</td><td style = \"text-align: right;\">0.1402</td><td style = \"text-align: right;\">0.1235</td><td style = \"text-align: right;\">0.1534</td><td style = \"text-align: right;\">0.1901</td><td style = \"text-align: right;\">0.2429</td><td style = \"text-align: right;\">0.212</td><td style = \"text-align: right;\">0.2395</td><td style = \"text-align: right;\">0.3272</td><td style = \"text-align: right;\">0.5949</td><td style = \"text-align: right;\">0.8302</td><td style = \"text-align: right;\">0.9045</td><td style = \"text-align: right;\">0.9888</td><td style = \"text-align: right;\">0.9912</td><td style = \"text-align: right;\">0.9448</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.9092</td><td style = \"text-align: right;\">0.7412</td><td style = \"text-align: right;\">0.7691</td><td style = \"text-align: right;\">0.7117</td><td style = \"text-align: right;\">0.5304</td><td style = \"text-align: right;\">0.2131</td><td style = \"text-align: right;\">0.0928</td><td style = \"text-align: right;\">0.1297</td><td style = \"text-align: right;\">0.1159</td><td style = \"text-align: right;\">0.1226</td><td style = \"text-align: right;\">0.1768</td><td style = \"text-align: right;\">0.0345</td><td style = \"text-align: right;\">0.1562</td><td style = \"text-align: right;\">0.0824</td><td style = \"text-align: right;\">0.1149</td><td style = \"text-align: right;\">0.1694</td><td style = \"text-align: right;\">0.0954</td><td style = \"text-align: right;\">0.008</td><td style = \"text-align: right;\">0.079</td><td style = \"text-align: right;\">0.1255</td><td style = \"text-align: right;\">0.0647</td><td style = \"text-align: right;\">0.0179</td><td style = \"text-align: right;\">0.0051</td><td style = \"text-align: right;\">0.0061</td><td style = \"text-align: right;\">0.0093</td><td style = \"text-align: right;\">0.0135</td><td style = \"text-align: right;\">0.0063</td><td style = \"text-align: right;\">0.0063</td><td style = \"text-align: right;\">0.0034</td><td style = \"text-align: right;\">0.0032</td><td style = \"text-align: right;\">0.0062</td><td style = \"text-align: right;\">0.0067</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">206</td><td style = \"text-align: right;\">0.0522</td><td style = \"text-align: right;\">0.0437</td><td style = \"text-align: right;\">0.018</td><td style = \"text-align: right;\">0.0292</td><td style = \"text-align: right;\">0.0351</td><td style = \"text-align: right;\">0.1171</td><td style = \"text-align: right;\">0.1257</td><td style = \"text-align: right;\">0.1178</td><td style = \"text-align: right;\">0.1258</td><td style = \"text-align: right;\">0.2529</td><td style = \"text-align: right;\">0.2716</td><td style = \"text-align: right;\">0.2374</td><td style = \"text-align: right;\">0.1878</td><td style = \"text-align: right;\">0.0983</td><td style = \"text-align: right;\">0.0683</td><td style = \"text-align: right;\">0.1503</td><td style = \"text-align: right;\">0.1723</td><td style = \"text-align: right;\">0.2339</td><td style = \"text-align: right;\">0.1962</td><td style = \"text-align: right;\">0.1395</td><td style = \"text-align: right;\">0.3164</td><td style = \"text-align: right;\">0.5888</td><td style = \"text-align: right;\">0.7631</td><td style = \"text-align: right;\">0.8473</td><td style = \"text-align: right;\">0.9424</td><td style = \"text-align: right;\">0.9986</td><td style = \"text-align: right;\">0.9699</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.863</td><td style = \"text-align: right;\">0.6979</td><td style = \"text-align: right;\">0.7717</td><td style = \"text-align: right;\">0.7305</td><td style = \"text-align: right;\">0.5197</td><td style = \"text-align: right;\">0.1786</td><td style = \"text-align: right;\">0.1098</td><td style = \"text-align: right;\">0.1446</td><td style = \"text-align: right;\">0.1066</td><td style = \"text-align: right;\">0.144</td><td style = \"text-align: right;\">0.1929</td><td style = \"text-align: right;\">0.0325</td><td style = \"text-align: right;\">0.149</td><td style = \"text-align: right;\">0.0328</td><td style = \"text-align: right;\">0.0537</td><td style = \"text-align: right;\">0.1309</td><td style = \"text-align: right;\">0.091</td><td style = \"text-align: right;\">0.0757</td><td style = \"text-align: right;\">0.1059</td><td style = \"text-align: right;\">0.1005</td><td style = \"text-align: right;\">0.0535</td><td style = \"text-align: right;\">0.0235</td><td style = \"text-align: right;\">0.0155</td><td style = \"text-align: right;\">0.016</td><td style = \"text-align: right;\">0.0029</td><td style = \"text-align: right;\">0.0051</td><td style = \"text-align: right;\">0.0062</td><td style = \"text-align: right;\">0.0089</td><td style = \"text-align: right;\">0.014</td><td style = \"text-align: right;\">0.0138</td><td style = \"text-align: right;\">0.0077</td><td style = \"text-align: right;\">0.0031</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">207</td><td style = \"text-align: right;\">0.0303</td><td style = \"text-align: right;\">0.0353</td><td style = \"text-align: right;\">0.049</td><td style = \"text-align: right;\">0.0608</td><td style = \"text-align: right;\">0.0167</td><td style = \"text-align: right;\">0.1354</td><td style = \"text-align: right;\">0.1465</td><td style = \"text-align: right;\">0.1123</td><td style = \"text-align: right;\">0.1945</td><td style = \"text-align: right;\">0.2354</td><td style = \"text-align: right;\">0.2898</td><td style = \"text-align: right;\">0.2812</td><td style = \"text-align: right;\">0.1578</td><td style = \"text-align: right;\">0.0273</td><td style = \"text-align: right;\">0.0673</td><td style = \"text-align: right;\">0.1444</td><td style = \"text-align: right;\">0.207</td><td style = \"text-align: right;\">0.2645</td><td style = \"text-align: right;\">0.2828</td><td style = \"text-align: right;\">0.4293</td><td style = \"text-align: right;\">0.5685</td><td style = \"text-align: right;\">0.699</td><td style = \"text-align: right;\">0.7246</td><td style = \"text-align: right;\">0.7622</td><td style = \"text-align: right;\">0.9242</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.9979</td><td style = \"text-align: right;\">0.8297</td><td style = \"text-align: right;\">0.7032</td><td style = \"text-align: right;\">0.7141</td><td style = \"text-align: right;\">0.6893</td><td style = \"text-align: right;\">0.4961</td><td style = \"text-align: right;\">0.2584</td><td style = \"text-align: right;\">0.0969</td><td style = \"text-align: right;\">0.0776</td><td style = \"text-align: right;\">0.0364</td><td style = \"text-align: right;\">0.1572</td><td style = \"text-align: right;\">0.1823</td><td style = \"text-align: right;\">0.1349</td><td style = \"text-align: right;\">0.0849</td><td style = \"text-align: right;\">0.0492</td><td style = \"text-align: right;\">0.1367</td><td style = \"text-align: right;\">0.1552</td><td style = \"text-align: right;\">0.1548</td><td style = \"text-align: right;\">0.1319</td><td style = \"text-align: right;\">0.0985</td><td style = \"text-align: right;\">0.1258</td><td style = \"text-align: right;\">0.0954</td><td style = \"text-align: right;\">0.0489</td><td style = \"text-align: right;\">0.0241</td><td style = \"text-align: right;\">0.0042</td><td style = \"text-align: right;\">0.0086</td><td style = \"text-align: right;\">0.0046</td><td style = \"text-align: right;\">0.0126</td><td style = \"text-align: right;\">0.0036</td><td style = \"text-align: right;\">0.0035</td><td style = \"text-align: right;\">0.0034</td><td style = \"text-align: right;\">0.0079</td><td style = \"text-align: right;\">0.0036</td><td style = \"text-align: right;\">0.0048</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">208</td><td style = \"text-align: right;\">0.026</td><td style = \"text-align: right;\">0.0363</td><td style = \"text-align: right;\">0.0136</td><td style = \"text-align: right;\">0.0272</td><td style = \"text-align: right;\">0.0214</td><td style = \"text-align: right;\">0.0338</td><td style = \"text-align: right;\">0.0655</td><td style = \"text-align: right;\">0.14</td><td style = \"text-align: right;\">0.1843</td><td style = \"text-align: right;\">0.2354</td><td style = \"text-align: right;\">0.272</td><td style = \"text-align: right;\">0.2442</td><td style = \"text-align: right;\">0.1665</td><td style = \"text-align: right;\">0.0336</td><td style = \"text-align: right;\">0.1302</td><td style = \"text-align: right;\">0.1708</td><td style = \"text-align: right;\">0.2177</td><td style = \"text-align: right;\">0.3175</td><td style = \"text-align: right;\">0.3714</td><td style = \"text-align: right;\">0.4552</td><td style = \"text-align: right;\">0.57</td><td style = \"text-align: right;\">0.7397</td><td style = \"text-align: right;\">0.8062</td><td style = \"text-align: right;\">0.8837</td><td style = \"text-align: right;\">0.9432</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">0.9375</td><td style = \"text-align: right;\">0.7603</td><td style = \"text-align: right;\">0.7123</td><td style = \"text-align: right;\">0.8358</td><td style = \"text-align: right;\">0.7622</td><td style = \"text-align: right;\">0.4567</td><td style = \"text-align: right;\">0.1715</td><td style = \"text-align: right;\">0.1549</td><td style = \"text-align: right;\">0.1641</td><td style = \"text-align: right;\">0.1869</td><td style = \"text-align: right;\">0.2655</td><td style = \"text-align: right;\">0.1713</td><td style = \"text-align: right;\">0.0959</td><td style = \"text-align: right;\">0.0768</td><td style = \"text-align: right;\">0.0847</td><td style = \"text-align: right;\">0.2076</td><td style = \"text-align: right;\">0.2505</td><td style = \"text-align: right;\">0.1862</td><td style = \"text-align: right;\">0.1439</td><td style = \"text-align: right;\">0.147</td><td style = \"text-align: right;\">0.0991</td><td style = \"text-align: right;\">0.0041</td><td style = \"text-align: right;\">0.0154</td><td style = \"text-align: right;\">0.0116</td><td style = \"text-align: right;\">0.0181</td><td style = \"text-align: right;\">0.0146</td><td style = \"text-align: right;\">0.0129</td><td style = \"text-align: right;\">0.0047</td><td style = \"text-align: right;\">0.0039</td><td style = \"text-align: right;\">0.0061</td><td style = \"text-align: right;\">0.004</td><td style = \"text-align: right;\">0.0036</td><td style = \"text-align: right;\">0.0061</td><td style = \"text-align: right;\">0.0115</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">true</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccccc}\n",
       "\t& Column1 & Column2 & Column3 & Column4 & Column5 & Column6 & Column7 & Column8 & Column9 & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 0.02 & 0.0371 & 0.0428 & 0.0207 & 0.0954 & 0.0986 & 0.1539 & 0.1601 & 0.3109 & $\\dots$ \\\\\n",
       "\t2 & 0.0453 & 0.0523 & 0.0843 & 0.0689 & 0.1183 & 0.2583 & 0.2156 & 0.3481 & 0.3337 & $\\dots$ \\\\\n",
       "\t3 & 0.0262 & 0.0582 & 0.1099 & 0.1083 & 0.0974 & 0.228 & 0.2431 & 0.3771 & 0.5598 & $\\dots$ \\\\\n",
       "\t4 & 0.01 & 0.0171 & 0.0623 & 0.0205 & 0.0205 & 0.0368 & 0.1098 & 0.1276 & 0.0598 & $\\dots$ \\\\\n",
       "\t5 & 0.0762 & 0.0666 & 0.0481 & 0.0394 & 0.059 & 0.0649 & 0.1209 & 0.2467 & 0.3564 & $\\dots$ \\\\\n",
       "\t6 & 0.0286 & 0.0453 & 0.0277 & 0.0174 & 0.0384 & 0.099 & 0.1201 & 0.1833 & 0.2105 & $\\dots$ \\\\\n",
       "\t7 & 0.0317 & 0.0956 & 0.1321 & 0.1408 & 0.1674 & 0.171 & 0.0731 & 0.1401 & 0.2083 & $\\dots$ \\\\\n",
       "\t8 & 0.0519 & 0.0548 & 0.0842 & 0.0319 & 0.1158 & 0.0922 & 0.1027 & 0.0613 & 0.1465 & $\\dots$ \\\\\n",
       "\t9 & 0.0223 & 0.0375 & 0.0484 & 0.0475 & 0.0647 & 0.0591 & 0.0753 & 0.0098 & 0.0684 & $\\dots$ \\\\\n",
       "\t10 & 0.0164 & 0.0173 & 0.0347 & 0.007 & 0.0187 & 0.0671 & 0.1056 & 0.0697 & 0.0962 & $\\dots$ \\\\\n",
       "\t11 & 0.0039 & 0.0063 & 0.0152 & 0.0336 & 0.031 & 0.0284 & 0.0396 & 0.0272 & 0.0323 & $\\dots$ \\\\\n",
       "\t12 & 0.0123 & 0.0309 & 0.0169 & 0.0313 & 0.0358 & 0.0102 & 0.0182 & 0.0579 & 0.1122 & $\\dots$ \\\\\n",
       "\t13 & 0.0079 & 0.0086 & 0.0055 & 0.025 & 0.0344 & 0.0546 & 0.0528 & 0.0958 & 0.1009 & $\\dots$ \\\\\n",
       "\t14 & 0.009 & 0.0062 & 0.0253 & 0.0489 & 0.1197 & 0.1589 & 0.1392 & 0.0987 & 0.0955 & $\\dots$ \\\\\n",
       "\t15 & 0.0124 & 0.0433 & 0.0604 & 0.0449 & 0.0597 & 0.0355 & 0.0531 & 0.0343 & 0.1052 & $\\dots$ \\\\\n",
       "\t16 & 0.0298 & 0.0615 & 0.065 & 0.0921 & 0.1615 & 0.2294 & 0.2176 & 0.2033 & 0.1459 & $\\dots$ \\\\\n",
       "\t17 & 0.0352 & 0.0116 & 0.0191 & 0.0469 & 0.0737 & 0.1185 & 0.1683 & 0.1541 & 0.1466 & $\\dots$ \\\\\n",
       "\t18 & 0.0192 & 0.0607 & 0.0378 & 0.0774 & 0.1388 & 0.0809 & 0.0568 & 0.0219 & 0.1037 & $\\dots$ \\\\\n",
       "\t19 & 0.027 & 0.0092 & 0.0145 & 0.0278 & 0.0412 & 0.0757 & 0.1026 & 0.1138 & 0.0794 & $\\dots$ \\\\\n",
       "\t20 & 0.0126 & 0.0149 & 0.0641 & 0.1732 & 0.2565 & 0.2559 & 0.2947 & 0.411 & 0.4983 & $\\dots$ \\\\\n",
       "\t21 & 0.0473 & 0.0509 & 0.0819 & 0.1252 & 0.1783 & 0.307 & 0.3008 & 0.2362 & 0.383 & $\\dots$ \\\\\n",
       "\t22 & 0.0664 & 0.0575 & 0.0842 & 0.0372 & 0.0458 & 0.0771 & 0.0771 & 0.113 & 0.2353 & $\\dots$ \\\\\n",
       "\t23 & 0.0099 & 0.0484 & 0.0299 & 0.0297 & 0.0652 & 0.1077 & 0.2363 & 0.2385 & 0.0075 & $\\dots$ \\\\\n",
       "\t24 & 0.0115 & 0.015 & 0.0136 & 0.0076 & 0.0211 & 0.1058 & 0.1023 & 0.044 & 0.0931 & $\\dots$ \\\\\n",
       "\t25 & 0.0293 & 0.0644 & 0.039 & 0.0173 & 0.0476 & 0.0816 & 0.0993 & 0.0315 & 0.0736 & $\\dots$ \\\\\n",
       "\t26 & 0.0201 & 0.0026 & 0.0138 & 0.0062 & 0.0133 & 0.0151 & 0.0541 & 0.021 & 0.0505 & $\\dots$ \\\\\n",
       "\t27 & 0.0151 & 0.032 & 0.0599 & 0.105 & 0.1163 & 0.1734 & 0.1679 & 0.1119 & 0.0889 & $\\dots$ \\\\\n",
       "\t28 & 0.0177 & 0.03 & 0.0288 & 0.0394 & 0.063 & 0.0526 & 0.0688 & 0.0633 & 0.0624 & $\\dots$ \\\\\n",
       "\t29 & 0.01 & 0.0275 & 0.019 & 0.0371 & 0.0416 & 0.0201 & 0.0314 & 0.0651 & 0.1896 & $\\dots$ \\\\\n",
       "\t30 & 0.0189 & 0.0308 & 0.0197 & 0.0622 & 0.008 & 0.0789 & 0.144 & 0.1451 & 0.1789 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m208×62 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Column1 \u001b[0m\u001b[1m Column2 \u001b[0m\u001b[1m Column3 \u001b[0m\u001b[1m Column4 \u001b[0m\u001b[1m Column5 \u001b[0m\u001b[1m Column6 \u001b[0m\u001b[1m Column7 \u001b[0m\u001b[1m Column8 \u001b[0m\u001b[1m\u001b[0m ⋯\n",
       "     │\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │  0.02     0.0371   0.0428   0.0207   0.0954   0.0986   0.1539   0.1601  ⋯\n",
       "   2 │  0.0453   0.0523   0.0843   0.0689   0.1183   0.2583   0.2156   0.3481\n",
       "   3 │  0.0262   0.0582   0.1099   0.1083   0.0974   0.228    0.2431   0.3771\n",
       "   4 │  0.01     0.0171   0.0623   0.0205   0.0205   0.0368   0.1098   0.1276\n",
       "   5 │  0.0762   0.0666   0.0481   0.0394   0.059    0.0649   0.1209   0.2467  ⋯\n",
       "   6 │  0.0286   0.0453   0.0277   0.0174   0.0384   0.099    0.1201   0.1833\n",
       "   7 │  0.0317   0.0956   0.1321   0.1408   0.1674   0.171    0.0731   0.1401\n",
       "   8 │  0.0519   0.0548   0.0842   0.0319   0.1158   0.0922   0.1027   0.0613\n",
       "   9 │  0.0223   0.0375   0.0484   0.0475   0.0647   0.0591   0.0753   0.0098  ⋯\n",
       "  10 │  0.0164   0.0173   0.0347   0.007    0.0187   0.0671   0.1056   0.0697\n",
       "  11 │  0.0039   0.0063   0.0152   0.0336   0.031    0.0284   0.0396   0.0272\n",
       "  ⋮  │    ⋮        ⋮        ⋮        ⋮        ⋮        ⋮        ⋮        ⋮     ⋱\n",
       " 199 │  0.0238   0.0318   0.0422   0.0399   0.0788   0.0766   0.0881   0.1143\n",
       " 200 │  0.0116   0.0744   0.0367   0.0225   0.0076   0.0545   0.111    0.1069  ⋯\n",
       " 201 │  0.0131   0.0387   0.0329   0.0078   0.0721   0.1341   0.1626   0.1902\n",
       " 202 │  0.0335   0.0258   0.0398   0.057    0.0529   0.1091   0.1709   0.1684\n",
       " 203 │  0.0272   0.0378   0.0488   0.0848   0.1127   0.1103   0.1349   0.2337\n",
       " 204 │  0.0187   0.0346   0.0168   0.0177   0.0393   0.163    0.2028   0.1694  ⋯\n",
       " 205 │  0.0323   0.0101   0.0298   0.0564   0.076    0.0958   0.099    0.1018\n",
       " 206 │  0.0522   0.0437   0.018    0.0292   0.0351   0.1171   0.1257   0.1178\n",
       " 207 │  0.0303   0.0353   0.049    0.0608   0.0167   0.1354   0.1465   0.1123\n",
       " 208 │  0.026    0.0363   0.0136   0.0272   0.0214   0.0338   0.0655   0.14    ⋯\n",
       "\u001b[36m                                                 54 columns and 187 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insertcols!(data, :Mine => data[:, 61].==\"M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efa393f",
   "metadata": {},
   "source": [
    "Once the data is loaded in the DataFrame for the checking proposes and that any posible process has been applied on the data. As in previous tutorials, the data has to be put on a Matrix form, such as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "590ea8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 60)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = Matrix(data[!, 1:60]);\n",
    "output_data = data[!, :Mine];\n",
    "\n",
    "@assert input_data isa Matrix\n",
    "@assert output_data isa BitVector\n",
    "\n",
    "size(input_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a148c7",
   "metadata": {},
   "source": [
    "It is worth to mention that in a DataFrame when a set of lines is queried such as in the case of the `X`, the results is also a DataFrame. Therefore, in order to applied the remaining operations it is needed to applied the `Matrix` function to retrive a matrix where the previous operations can be used as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d272d3b3",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Now, the data is loaded and converted to the usual types. Now you should be able to apply in the next section and make asplit of the dataset in two subset, test and training, and apply the corresponding normalization. Put the code on the following section to perform both operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mModule model_selection has been ported to Julia - try `import ScikitLearn: CrossValidation` instead\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ ScikitLearn.Skcore ~/.julia/packages/ScikitLearn/sqLdT/src/Skcore.jl:259\u001b[39m\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mRunning `conda install -y -c anaconda conda` in root environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/martin/.julia/conda/3/x86_64\n",
      "\n",
      "  added / updated specs:\n",
      "    - conda\n",
      "\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2023.11.~ --> anaconda::ca-certificates-2023.08.22-h06a4308_0 \n",
      "  certifi            conda-forge/noarch::certifi-2023.11.1~ --> anaconda/linux-64::certifi-2023.7.22-py310h06a4308_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.3.1\n",
      "  latest version: 23.10.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.10.0\n",
      "\n",
      "\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mRunning `conda install -y -c conda-forge 'libstdcxx-ng>=3.4,<13.0'` in root environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/martin/.julia/conda/3/x86_64\n",
      "\n",
      "  added / updated specs:\n",
      "    - libstdcxx-ng[version='>=3.4,<13.0']\n",
      "\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    anaconda::ca-certificates-2023.08.22-~ --> conda-forge::ca-certificates-2023.11.17-hbcca054_0 \n",
      "  certifi            anaconda/linux-64::certifi-2023.7.22-~ --> conda-forge/noarch::certifi-2023.11.17-pyhd8ed1ab_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.3.1\n",
      "  latest version: 23.10.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.10.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4-element Vector{Array}:\n",
       " [0.0072 0.0027 … 0.0012 0.0037; 0.0065 0.0122 … 0.0006 0.0035; … ; 0.0235 0.0291 … 0.0108 0.009; 0.0388 0.0324 … 0.0097 0.0067]\n",
       " [0.01 0.0171 … 0.004 0.0117; 0.0123 0.0022 … 0.0047 0.0071; … ; 0.0363 0.0478 … 0.0073 0.0033; 0.018 0.0444 … 0.0073 0.0022]\n",
       " Bool[1, 0, 1, 1, 0, 1, 1, 0, 0, 1  …  1, 0, 0, 0, 0, 0, 1, 1, 0, 1]\n",
       " Bool[0, 0, 0, 1, 0, 1, 1, 1, 1, 0  …  1, 1, 0, 0, 0, 1, 1, 0, 1, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "using MLDataUtils\n",
    "@sk_import model_selection:train_test_split\n",
    "# Split the dataset into training and testing subsets\n",
    "train_input, test_input, train_output, test_output = train_test_split(input_data, output_data, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcbe5ef",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "As mentioned above, ensembles are a set of \"weaker\" classifiers that allow us to later overcome their limits by joining them together. That is why, before starting with ensembles, it will be necessary to have some reference models that will later be joined together in a meta-classifier. In the following example, some simple models, from `scikit-learn` library, are trained: an SVM with RBF kernel, a Linear Regression, a Naïve Bayes and a Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04b0a9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{String}:\n",
       " \"NB\"\n",
       " \"SVM\"\n",
       " \"LR\"\n",
       " \"DT\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "\n",
    "@sk_import svm:SVC\n",
    "@sk_import tree:DecisionTreeClassifier\n",
    "@sk_import linear_model:LogisticRegression\n",
    "@sk_import naive_bayes:GaussianNB \n",
    "\n",
    "#Define the models to train\n",
    "models = Dict( \"SVM\" => SVC(probability=true), \n",
    "         \"LR\" =>LogisticRegression(),\n",
    "         \"DT\"=> DecisionTreeClassifier(max_depth=4),\n",
    "         \"NB\"=> GaussianNB())\n",
    "\n",
    "base_models =  [ name for name in keys(models)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3a595d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB: 65.07936507936508 %\n",
      "SVM: 80.95238095238095 %\n",
      "LR: 77.77777777777779 %\n",
      "DT: 73.01587301587301 %\n"
     ]
    }
   ],
   "source": [
    "# Perform the training for each model and calculate the test values (accuracy)\n",
    "for key in keys(models)\n",
    "    model = models[key]\n",
    "    fit!(model,train_input, train_output)\n",
    "    acc = score(model,test_input, test_output)\n",
    "    println(\"$key: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ecb8c",
   "metadata": {},
   "source": [
    "## Combining weak models in an ensemble\n",
    "\n",
    "When it comes to combining the models, there are different strategies depending on the task of the model, i.e. whether we are classifying or regressing. In this particular case we are going to focus on classification, although for regression it would be similar, but the continuous nature of the values should be taken into account when combining the outputs.\n",
    "\n",
    "Regarding the combination of the classification, there are mainly two ways to combine the outputs of several classifiers. These combinations are called Majority voting and Weighted majority voting.\n",
    "\n",
    "### Majority Voting\n",
    "Although also known as Hard Voting, as the name suggests, they are based on selecting the most voted option among the predicted ones among the different models. The implementation available in scikit learn makes a sum of the predictions for each of the classes and then averages these estimates. The option selected by majority among the \"experts\" of which the emsemble consists is the one selected. In this way, the problem could be solved taking into account different results or points of view on the problem. See an example in the code below of constructing such a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72a2da84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB: 65.07936507936508 %\n",
      "SVM: 80.95238095238095 %\n",
      "LR: 77.77777777777779 %\n",
      "DT: 73.01587301587301 %\n",
      "Ensemble (Hard Voting): 80.95238095238095 %\n"
     ]
    }
   ],
   "source": [
    "@sk_import ensemble:VotingClassifier\n",
    "\n",
    "#Define the metaclassifier based on the base_models\n",
    "models[\"Ensemble (Hard Voting)\"] = VotingClassifier(estimators = [(name,models[name]) for name in base_models], \n",
    "                                                   n_jobs=-1)\n",
    "fit!(models[\"Ensemble (Hard Voting)\"], train_input, train_output)\n",
    "\n",
    "for key in keys(models)\n",
    "    model = models[key]\n",
    "    acc = score(model,test_input, test_output)\n",
    "    println(\"$key: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c3971c",
   "metadata": {},
   "source": [
    "As can be seen, while it does not improve on the best of the component models, this is because, firstly, this is not a particularly complex problem. In addition, another problem is that we rely equally on all models when deciding on the response class. To solve this problem, it is possible to make it so that not all models are of equal importance, as we will see in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf78117f",
   "metadata": {},
   "source": [
    "### Weigthed Mayority Voting \n",
    "As mentioned in the previous section, one of the problems of the classical *emsemble* model is that all outcomes are weighted equally and in each of the \"weak\" models only the most voted option is taken into account. To solve this, one of the proposals is the use of a weighting in the decision weights. This is because one model may be better than another or more reliable. In order to reflect this point, the output can be modified by multiplying it by a confidence factor within the rule used to make the decisions. This weighting procedure is sometimes also referred to as *Soft Voting* in contrast to *Hard Voting* or unweighted voting. Imagine that each of the classifiers is assigned the same weight, i.e. {1,1,1}. In an example like the following with an SVM, a Logarithmic regression and a Bayes-based model we would have the following outputs.\n",
    "\n",
    "|Classifier\t     |Mine\t        |Rock          |\n",
    "| :------------- | :----------: | -----------: |\n",
    "|SVM         \t | 0.9\t    | 0.1      |\t\n",
    "|LR         \t | 0.3\t    | 0.7      |\t\n",
    "|NB         \t | 0.2\t    | 0.8      |\n",
    "|Soft Voting      |0.47\t        |0.63          |\t\n",
    "\n",
    "Therefore, the selected class would be the Rock class since all models weigh the same in the decision making process when averaging. In contrast, if we know that one of the models is better, we can weight the response of that model. Imagine in the previous example if you knew that SVM is usually much better than the other two for this particular problem. In that case, you can increase its weight as seen below in order to take that model more into account. With the same example, but with SVM's answer being larger, the results would be:\n",
    "\n",
    "|Classifier\t     |Mine\t        |Rock          |\n",
    "| :------------- | :----------: | -----------: |\n",
    "|SVM         \t |2 * 0.9\t    |2 * 0.1      |\t\n",
    "|LR         \t |1 * 0.3\t    |1 * 0.7      |\t\n",
    "|NB         \t |1 * 0.2\t    |1 * 0.8      |\n",
    "|Soft Voting      |0.575\t        |0.425          |\n",
    "\n",
    "As can be seen from the results, if we have a higher quality model, the outputs of this model will be taken into account more in terms of making the corresponding decision.\n",
    "\n",
    "To implement this type of behaviour, you can simply add two additional parameters to the `VotingClassifier` function that was previously used to weight the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "260da3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB: 73.10344827586206 %\n",
      "SVM: 88.27586206896552 %\n",
      "LR: 84.13793103448276 %\n",
      "DT: 96.55172413793103 %\n",
      "Ensemble (Hard Voting): 90.3448275862069 %\n",
      "Ensemble (Soft Voting): 90.3448275862069 %\n"
     ]
    }
   ],
   "source": [
    "models[\"Ensemble (Soft Voting)\"] = VotingClassifier(estimators = [(name,models[name]) for name in base_models], \n",
    "                                                   n_jobs=-1, voting=\"soft\",weights=[1,2,2,1])\n",
    "fit!(models[\"Ensemble (Soft Voting)\"],train_input, train_output)\n",
    "\n",
    "for key in keys(models)\n",
    "    model = models[key]\n",
    "    acc = score(model,train_input, train_output)\n",
    "    println(\"$key: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa26ee5d",
   "metadata": {},
   "source": [
    "As you can see, the results are better when you combine several models that give good results. In fact, this procedure is the basis of other techniques such as the *Random Forest* that we will see a little later in this tutorial. The models to be used are the other key to the creation of _ensemble_, in the next section we will see the most common strategies for the creation of the models.\n",
    "\n",
    "The adjustment of these weights can be done in many different ways, for example, it can be done manually as we have done in the previous example. Another alternative would be to use a gradient descent technique to adjust them as if it were a neural network or an SVM. Another possibility is to use the fit value on the validation set (in this case a dataset has not been reserved for this purpose) as the weight of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12be268b",
   "metadata": {},
   "source": [
    "### Question\n",
    "We have perform every single test with a hold-out strategy, however, as it was appointed in a previous session, the application of a cross-validation approach is prefered to cut the dependency on the selection of the samples. In this case you could think that there are two different approaches one is apply the cross-validation to each model, choose the better one and combine those in a single ensemble. The other way arround would be applying the cross-validation at ensemble level before training the models. Which one is correct and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2fcbb5",
   "metadata": {},
   "source": [
    "`Answer here` The choice between these approaches depends on factors such as the size of your dataset, the computational resources available, and the nature of the models you are using. If your dataset is large, and computational resources are not a major constraint, the second approach (cross-validation at the ensemble level) may provide a more reliable estimate of overall performance. If resources are limited, or if you want more flexibility in tuning individual models, the first approach could be more practical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f2b304",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "\n",
    "This last approach to combining the models can be considered as a variant of Soft Voting. As mentioned in that section, soft voting allows the weights of each of the models to be fixed and this can be adjusted with a decaying gradient technique. Stacking is usually identified as creating a classification technique superior to a linear regression (which is what Soft Voting does) such as an ANN to combine the models.\n",
    "\n",
    "Thus, as has been done previously, the outputs of the different techniques could be taken and used as inputs to another classification model, allowing for the adjustment of the weights and the non-linear combinations of the responses of each one.\n",
    "\n",
    "You can see an example or this in the following code, which uses the implementation on `scikit=learn` whcih uses an SVC as compbining model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c70391c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingClassifier(estimators=[(&#x27;NB&#x27;, GaussianNB()),\n",
       "                               (&#x27;SVM&#x27;, SVC(probability=True)),\n",
       "                               (&#x27;LR&#x27;, LogisticRegression()),\n",
       "                               (&#x27;DT&#x27;, DecisionTreeClassifier(max_depth=4))],\n",
       "                   final_estimator=SVC(probability=True), n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StackingClassifier</label><div class=\"sk-toggleable__content\"><pre>StackingClassifier(estimators=[(&#x27;NB&#x27;, GaussianNB()),\n",
       "                               (&#x27;SVM&#x27;, SVC(probability=True)),\n",
       "                               (&#x27;LR&#x27;, LogisticRegression()),\n",
       "                               (&#x27;DT&#x27;, DecisionTreeClassifier(max_depth=4))],\n",
       "                   final_estimator=SVC(probability=True), n_jobs=-1)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>NB</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>SVM</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>LR</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>DT</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=4)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "PyObject StackingClassifier(estimators=[('NB', GaussianNB()),\n",
       "                               ('SVM', SVC(probability=True)),\n",
       "                               ('LR', LogisticRegression()),\n",
       "                               ('DT', DecisionTreeClassifier(max_depth=4))],\n",
       "                   final_estimator=SVC(probability=True), n_jobs=-1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@sk_import ensemble:StackingClassifier\n",
    "\n",
    "models[\"Ensemble (Stacking)\"] = StackingClassifier(estimators=[(name,models[name]) for name in base_models],\n",
    "    final_estimator=SVC(probability=true), n_jobs=-1)\n",
    "fit!(models[\"Ensemble (Stacking)\"], train_input, train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed598b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB: 73.10344827586206 %\n",
      "SVM: 88.27586206896552 %\n",
      "LR: 84.13793103448276 %\n",
      "DT: 96.55172413793103 %\n",
      "Ensemble (Hard Voting): 90.3448275862069 %\n",
      "Ensemble (Soft Voting): 90.3448275862069 %\n",
      "Ensemble (Stacking): 92.41379310344827 %\n"
     ]
    }
   ],
   "source": [
    "for key in keys(models)\n",
    "    model = models[key]\n",
    "    acc = score(model,train_input, train_output)\n",
    "    println(\"$key: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9273829",
   "metadata": {},
   "source": [
    "## Model creation\n",
    "\n",
    "One of the key elements that has not yet been addressed is the creation of the models that will compose the meta-classifier. So far, the approach that has been followed is not very adequate as the input dataset for all models is the same. This has the effect of an obvious lack of diversity in the models since whichever model we create, it will have the same information or \"point of view\" as the others. However, this is not the usual practice. Instead, the set of input patterns is usually divided into smaller sets with which to train one or more techniques in order to reduce the computational cost on the one hand, and to increase the diversity of the models on the other. It is necessary to remember at this point that \"weak\" models do not have to be perfect in all classes and do not even have to cover all possibilities, only models that are quick to train and offer a more or less consistent output.\n",
    "\n",
    "As for the way in which to partition the data for the creation of the models, most approaches usually consider two main approaches known as *Bagging* and *Boosting*. In the following, these two approaches will be briefly described.\n",
    "\n",
    "### Bagging or boostrap aggregation\n",
    "The technique known as _Bagging_ or selection with replacement was proposed by Breitman in 1996. It is based on the development of multiple models which can be trained in parallel. The key element of these models is that each model is trained on a subset of the training set. This subset of data is drawn randomly with replacement. This last point is particularly important because once an example has been selected from the possibilities, it is placed back among the possibilities so that it can be selected either in the subset being built, or in the subsets of the other models, i.e. non-disjoint sets of examples are created.\n",
    "\n",
    "![Bagging Example](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Ensemble_Bagging.svg/440px-Ensemble_Bagging.svg.png)\n",
    "\n",
    "The result is that \"experts\" are created on specialised data and depending on the partition. While common, or more frequent, data is correctly covered by all models, it is also true that less frequent data tends not to be in all partitions and may not be covered in all cases. Thus, you would get models that would be more specialised in certain data or have a different point of view, that would be experts in a particular region of the search space.\n",
    "\n",
    "Although it will be discussed in more detail later, a well-known technique that uses this approach for the construction of its \"weak\" models is RandomForest. It builds the decision trees that make up the metaclassifier in this way. Any classifier can be used as the basis of a *Bagging* with the class [BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html). \n",
    "\n",
    "For example, in the following code, 10 SVM for classication has been chosen as weak models. Each of those models habe been trained on only 50% of the training patterns, and therefore the variance among them should be increased.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd7750d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB: 73.10344827586206 %\n",
      "SVM: 88.27586206896552 %\n",
      "LR: 84.13793103448276 %\n",
      "DT: 96.55172413793103 %\n",
      "Ensemble (Hard Voting): 90.3448275862069 %\n",
      "Ensemble (Soft Voting): 90.3448275862069 %\n",
      "Ensemble (Stacking): 92.41379310344827 %\n",
      "Bagging (SVC): 83.44827586206897 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant SVC. This may fail, cause incorrect answers, or produce other errors.\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "@sk_import svm:SVC\n",
    "@sk_import ensemble:BaggingClassifier\n",
    "\n",
    "models[\"Bagging (SVC)\"] = BaggingClassifier(base_estimator=SVC(),n_estimators=10, max_samples=0.50, n_jobs=-1)\n",
    "fit!(models[\"Bagging (SVC)\"], train_input, train_output)\n",
    "\n",
    "for key in keys(models)\n",
    "    model = models[key]\n",
    "    acc = score(model,train_input, train_output)\n",
    "    println(\"$key: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5787a56",
   "metadata": {},
   "source": [
    "As an alternative to extracting complete examples, a vertical partition of the training _dataset_ could be performed, thus extracting features. To implement this alternative, in the `BaggingClassifier` function, the parameter *max_features* must be defined. This approach is used when the number of features is particularly high in order to create simpler models that do not use all the information that is often redundant. It should be noted that this feature extraction procedure for models is done without replacement, i.e. features extracted for one classifier are not re-entered into the list of possibilities until the set for the next classifier is created.\n",
    "\n",
    "### Boosting\n",
    "The other major family of techniques for ensemble metamodelling is what is known as *Boosting*. In this case, the approach is slightly different, since the aim is to create a chain of classifiers. The key element of this type of classifier is to find that each new classifier is more specialised in the patterns that the previous models have missed. Therefore, as in the previous case, a subset of patterns is selected from the original set. However, this process is done sequentially and without replacement. This point is crucial since, as mentioned above, the idea is to eliminate those patterns that are already correctly classified and to obtain more specific models that concentrate on those examples that are less frequent or that have been incorrectly classified in a previous step. Thus, as in *Bagging*, the underlying idea of this approach is that not all models have to have all patterns as a basis, but unlike _Bagging_, this process is linear because of the dependency in the construction of the models. \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Ensemble_Boosting.svg/1920px-Ensemble_Boosting.svg.png\" alt=\"Boosting examples\" width=\"600\"/>\n",
    "\n",
    "Subsequently, to obtain the combination of the models, the Majority Vote with weights is used. In this approach, the weights are established with an iterative approximation system. There are many examples that use this type of technique, such as [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) or [Gracient Tree Boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html). In both cases what is done is an adjustment of the weights with a technique based on the Descending Gradient. \n",
    "\n",
    "In the case of AdaBoost, the algorithm starts by giving a weight to all the instances of the training set. With this weighted set, a classifier is trained with the original data. Depending on the errors made, the weights of the original set are adjusted and a new copy of the classifier is trained, but on the adjusted data, which will focus more on the instances that have been classified incorrectly. In the case of `scikit learn`, the algorithm implemented is known as [AdaBoost-SAMME](https://hastie.su.domains/Papers/SII-2-3-A8-Zhu.pdf) proposed by Zhu et.al. in 2009. As a particularity of this implementation, the *loss* function used is an exponential one. This is the one that will be used to calculate the weighting of the errors made, as well as the weight of the classifiers in the meta-classifier. In general terms, the output will be the most voted by the classifiers based on the weighting of each of them. \n",
    "\n",
    "Gradient Tree Boosting is a different approach to the use of Boosting. It builds a tree where the nodes of the tree set the criteria for, for example, in the case of classification refer to the `logistic-likelihood` of a given pattern. In this way, each of the nodes of the tree makes a classification which is adjusted on the basis of the residual errors that are made by adjusting the weights of the different classifiers in the tree. This division is carried out for each of the available features, performing a recursive procedure by training several classifiers in this way. Subsequently, to make the decision, it is based on the responses of the classifiers it has passed through. The main difference with AdaBoost is that in this case the output is the probabilities of the classes which are summed to give the most likely answer rather than the answer over the instances.\n",
    "\n",
    "Below, we see an approach with an example of using these two metaclassifiers that make use of _Boosting_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "830a90e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB: 65.07936507936508 %\n",
      "SVM: 80.95238095238095 %\n",
      "LR: 77.77777777777779 %\n",
      "DT: 73.01587301587301 %\n",
      "Ensemble (Hard Voting): 80.95238095238095 %\n",
      "Ensemble (Soft Voting): 80.95238095238095 %\n",
      "Ada: 85.71428571428571 %\n",
      "Ensemble (Stacking): 79.36507936507937 %\n",
      "GTB: 85.71428571428571 %\n",
      "Bagging (SVC): 74.60317460317461 %\n"
     ]
    }
   ],
   "source": [
    "@sk_import ensemble:(AdaBoostClassifier, GradientBoostingClassifier)\n",
    "\n",
    "models[\"Ada\"] = AdaBoostClassifier(n_estimators=30)\n",
    "fit!(models[\"Ada\"], train_input, train_output)\n",
    "\n",
    "models[\"GTB\"] = GradientBoostingClassifier(n_estimators=30, learning_rate=1.0, max_depth=2, random_state=0)\n",
    "fit!(models[\"GTB\"], train_input, train_output)\n",
    "\n",
    "for key in keys(models)\n",
    "    model = models[key]\n",
    "    acc = score(model,test_input, test_output)\n",
    "    println(\"$key: $(acc*100) %\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43fc04b",
   "metadata": {},
   "source": [
    "### Question\n",
    "In a similar way as in the cross-validation section, develop a funtion to train ensembles. The function called trainClassEmsemble, would also follow an stratified cross-validation. As a quick remember of the steps need to cover in the function:\n",
    "1. Create a vector with k elements, which will contain the test results of the cross-validation process with the selected metric.  \n",
    "\n",
    "2. Make a loop with k iterations (k folds) where within each iteration from the matrices of desired inputs and outputs, by means of the vector of indices resulting from the previous function, 4 matrices are created: desired inputs and outputs for training and test. \n",
    "\n",
    "3. Within this another loop, add a call to generate the models, which can be any of the ones used in Unit 6. \n",
    "\n",
    "4. Train those models by using the corresponding training set, i. e., the remaining K subsets non used for testing.\n",
    "\n",
    "5. In case a validation set is needed, e.g. wi, split the training set into two parts. To do this, use the holdOut function. \n",
    "\n",
    "4. Build the ensemble following one of the strategies described above (any of them) and calculate the test.  \n",
    "\n",
    "\n",
    "6. Finally, provide the result of averaging the values of these vectors for each metric together with their standard deviations. \n",
    "\n",
    "As a result of this call, at least the test value in the selected metric(s) should be returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_model (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Random\n",
    "using Statistics\n",
    "\n",
    "function trainClassEnsemble(estimators::AbstractArray{String,1}, \n",
    "        modelsHyperParameters:: AbstractArray{Dict{String,Any}, 1},     \n",
    "        trainingDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,2}},    \n",
    "        kFoldIndices::Array{Int64,1})\n",
    "\n",
    "    # Check that the number of estimators and hyperparameters match\n",
    "    @assert length(estimators) == length(modelsHyperParameters)\n",
    "    \n",
    "\n",
    "    # 1. Create a vector with k elements for storing test results\n",
    "    testResults = zeros(maximum(kFoldIndices))\n",
    "    #Split the training dataset into inputs and targets\n",
    "    (inputs,targets)=trainingDataset\n",
    "\n",
    "    # Check that the number of inputs matches the number of targets\n",
    "    @assert length(inputs[:,1])==length(targets) \n",
    "\n",
    "    # Check that the number of inputs matches the number of folds\n",
    "    @assert length(inputs[:,1])==length(kFoldIndices)\n",
    "\n",
    "    targets=targets'\n",
    "    # 2. Loop over k folds\n",
    "    for i = 1:maximum(kFoldIndices)\n",
    "        # Split the dataset into training and test sets\n",
    "\n",
    "        train_input = inputs[kFoldIndices.!=i,:]\n",
    "        test_input = inputs[kFoldIndices.==i,:]\n",
    "\n",
    "        train_output = targets[kFoldIndices.!=i,:]\n",
    "        test_output = targets[kFoldIndices.==i,:]  \n",
    "\n",
    "        # 3. Loop over estimators and train models\n",
    "        models=Dict()\n",
    "\n",
    "        for i in 1:length(estimators)\n",
    "            # Create model using the specified hyperparameters\n",
    "            model = create_model(estimators[i], modelsHyperParameters[i])\n",
    "\n",
    "            # Train model on the training set\n",
    "            fit!(model, train_input, train_output)\n",
    "\n",
    "            # Add the trained model to the list\n",
    "            models[estimators[i]] = model\n",
    "            #push!(models,model)\n",
    "        end\n",
    "\n",
    "        for key in keys(models)\n",
    "            model = models[key]\n",
    "            acc = score(model,test_input, test_output)\n",
    "            println(\"$key: $(acc*100) %\")\n",
    "        end\n",
    "\n",
    "        # 4. Build the ensemble and calculate test results\n",
    "        base_models =  [ name for name in keys(models)]\n",
    "        \n",
    "        ###################### \n",
    "        #After implementing the three ensembles, the best model was the hard voting\n",
    "        ######################\n",
    "        \n",
    "        ensemble = VotingClassifier(estimators = [(name,models[name]) for name in base_models], n_jobs=-1)\n",
    "        #ensemble= StackingClassifier(estimators=[(name,models[name]) for name in base_models], final_estimator=SVC(probability=true), n_jobs=-1)\n",
    "        #ensemble=VotingClassifier(estimators = [(name,models[name]) for name in base_models], n_jobs=-1, voting=\"soft\",weights=[2,1,1,1])\n",
    "\n",
    "        train_output=reshape(train_output, size(train_output)[1])\n",
    "\n",
    "        # Train the ensemble on the training set\n",
    "        fit!(ensemble, train_input, train_output)\n",
    "\n",
    "        # Test the ensemble on the test set\n",
    "        predictions = ensemble.predict(test_input)\n",
    "        predictions=reshape(predictions, 1,size(predictions)[1])\n",
    "\n",
    "        # Calculate the accuracy of the predictions\n",
    "        accuracy, error_rate, sensitivity, specificity, positive_predictive_value, negative_predictive_value, f_score, confusion_matrix=confusionMatrix(predictions',test_output,weighted=false)\n",
    "\n",
    "\n",
    "        # Store the test result for this fold\n",
    "        testResults[i] = accuracy\n",
    "    end\n",
    "\n",
    "    # 5. Calculate the average and standard deviation of test results\n",
    "    avg_accuracy = mean(testResults)\n",
    "    std_accuracy = std(testResults)\n",
    "\n",
    "    # 6. Return the average and standard deviation\n",
    "    return avg_accuracy, std_accuracy\n",
    "end\n",
    "\n",
    "\n",
    "# Helper function to create models\n",
    "function create_model(estimator, hyperparameters)\n",
    "    # Implement the logic to create a model based on the estimator and hyperparameters\n",
    "    @assert estimator==\"SVC\" || estimator==\"DecisionTreeClassifier\" || estimator==\"KNeighborsClassifier\" || estimator==\"ANN\"\n",
    "\n",
    "    # Create model using the specified hyperparameters\n",
    "\n",
    "    if estimator==\"SVC\"\n",
    "        #Check that the hyperparameters are present\n",
    "        @assert haskey(hyperparameters, \"probability\") && haskey(hyperparameters, \"kernel\") && haskey(hyperparameters, \"C\") && haskey(hyperparameters, \"gamma\") && haskey(hyperparameters, \"degree\")\n",
    "        #Create the model\n",
    "        model = SVC(probability=hyperparameters[\"probability\"], kernel=hyperparameters[\"kernel\"],C=hyperparameters[\"C\"], gamma=hyperparameters[\"gamma\"], degree=hyperparameters[\"degree\"])\n",
    "    \n",
    "    elseif estimator==\"DecisionTreeClassifier\"\n",
    "        #Check that the hyperparameters are present\n",
    "        @assert haskey(hyperparameters, \"max_depth\") && haskey(hyperparameters, \"criterion\") && haskey(hyperparameters, \"splitter\")\n",
    "        #Create the model\n",
    "        model = DecisionTreeClassifier(max_depth=hyperparameters[\"max_depth\"],criterion=hyperparameters[\"criterion\"],splitter=hyperparameters[\"splitter\"])\n",
    "    \n",
    "    elseif estimator==\"KNeighborsClassifier\"\n",
    "        #Check that the hyperparameters are present\n",
    "        @assert haskey(hyperparameters, \"n_neighbors\") && haskey(hyperparameters, \"weights\")\n",
    "        #Create the model\n",
    "        model = KNeighborsClassifier(n_neighbors=hyperparameters[\"n_neighbors\"],weights=hyperparameters[\"weights\"])\n",
    "\n",
    "    \n",
    "    elseif estimator==\"ANN\"\n",
    "        #Check that the hyperparameters are present\n",
    "        @assert haskey(hyperparameters, \"hidden_layer_sizes\") && haskey(hyperparameters, \"activation\") && haskey(hyperparameters, \"learning_rate_init\") && haskey(hyperparameters, \"validation_fraction\") && haskey(hyperparameters, \"max_iter\")\n",
    "        #Create the model\n",
    "        model=MLPClassifier(hidden_layer_sizes=hyperparameters[\"hidden_layer_sizes\"],\n",
    "            activation=hyperparameters[\"activation\"],\n",
    "            learning_rate_init=hyperparameters[\"learning_rate_init\"],\n",
    "            validation_fraction=hyperparameters[\"validation_fraction\"],\n",
    "            max_iter=hyperparameters[\"max_iter\"])\n",
    "\n",
    "    end\n",
    "    return model\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant SVC. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant DecisionTreeClassifier. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant LogisticRegression. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant GaussianNB. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyObject <class 'sklearn.naive_bayes.GaussianNB'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "@sk_import neural_network: MLPClassifier\n",
    "@sk_import svm: SVC\n",
    "@sk_import tree: DecisionTreeClassifier\n",
    "@sk_import neighbors: KNeighborsClassifier\n",
    "@sk_import linear_model: LogisticRegression\n",
    "@sk_import naive_bayes: GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confusionMatrix (generic function with 6 methods)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"utils.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Onehot\n",
      "Vector{Dict{String, Any}}\n",
      "ANN: 45.45454545454545 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier: 86.36363636363636 %\n",
      "DecisionTreeClassifier: 72.72727272727273 %\n",
      "SVC: 77.27272727272727 %\n",
      "ANN: 85.71428571428571 %\n",
      "KNeighborsClassifier: 71.42857142857143 %\n",
      "DecisionTreeClassifier: 76.19047619047619 %\n",
      "SVC: 80.95238095238095 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN: 76.19047619047619 %\n",
      "KNeighborsClassifier: 80.95238095238095 %\n",
      "DecisionTreeClassifier: 66.66666666666666 %\n",
      "SVC: 71.42857142857143 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN: 90.47619047619048 %\n",
      "KNeighborsClassifier: 85.71428571428571 %\n",
      "DecisionTreeClassifier: 80.95238095238095 %\n",
      "SVC: 80.95238095238095 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN: 76.19047619047619 %\n",
      "KNeighborsClassifier: 80.95238095238095 %\n",
      "DecisionTreeClassifier: 61.904761904761905 %\n",
      "SVC: 71.42857142857143 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN: 42.857142857142854 %\n",
      "KNeighborsClassifier: 100.0 %\n",
      "DecisionTreeClassifier: 66.66666666666666 %\n",
      "SVC: 100.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN: 61.904761904761905 %\n",
      "KNeighborsClassifier: 80.95238095238095 %\n",
      "DecisionTreeClassifier: 61.904761904761905 %\n",
      "SVC: 80.95238095238095 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN: 75.0 %\n",
      "KNeighborsClassifier: 90.0 %\n",
      "DecisionTreeClassifier: 65.0 %\n",
      "SVC: 75.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN: 45.0 %\n",
      "KNeighborsClassifier: 85.0 %\n",
      "DecisionTreeClassifier: 70.0 %\n",
      "SVC: 85.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN: 85.0 %\n",
      "KNeighborsClassifier: 60.0 %\n",
      "DecisionTreeClassifier: 65.0 %\n",
      "SVC: 80.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9230303030303029, 0.056140007766714746)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Random\n",
    "using Statistics\n",
    "\n",
    "# Define the models to use in the ensemble\n",
    "estimators = [\"KNeighborsClassifier\", \"DecisionTreeClassifier\", \"SVC\",\"ANN\"]\n",
    "#Define the models to train\n",
    "KR_hyper=Dict(\"n_neighbors\"=>3,\"weights\"=>\"distance\")\n",
    "SVM_hyper=Dict(\"probability\"=>true,\"kernel\"=>\"rbf\",\"C\"=>1.0,\"gamma\"=>\"scale\",\"degree\"=>3)\n",
    "DT_hyper=Dict(\"max_depth\"=>8,\"criterion\"=>\"gini\",\"splitter\"=>\"best\")\n",
    "ANN_hyper=Dict(\"hidden_layer_sizes\"=>(20,1),\"activation\"=>\"tanh\",\"learning_rate_init\"=>0.001,\"validation_fraction\"=>0.0,\"max_iter\"=>1000)\n",
    "\n",
    "\n",
    "modelsHyperParameters=Vector{Dict{String, Any}}([KR_hyper, DT_hyper,SVM_hyper, ANN_hyper ]) \n",
    "\n",
    "# Define the input and output data for training the models\n",
    "trainInput = input_data\n",
    "trainOutput = output_data\n",
    "\n",
    "# Define the number of folds to use in the cross-validation process\n",
    "nFolds = 10\n",
    "\n",
    "# Define the indices for the cross-validation process\n",
    "kFoldIndices=crossvalidation(output_data,10)\n",
    "println(typeof(modelsHyperParameters))\n",
    "# Train the ensemble\n",
    "meanTestResult, stdTestResult = trainClassEnsemble(estimators, modelsHyperParameters, (trainInput, BitMatrix(trainOutput')), kFoldIndices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean test result: 92.30303030303028 %\n",
      "Standard deviation of test result: 5.614000776671475 %\n"
     ]
    }
   ],
   "source": [
    "println(\"Mean test result: $(meanTestResult*100) %\")\n",
    "println(\"Standard deviation of test result: $(stdTestResult*100) %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "plot_bars (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "function plot_bars(percentages, labels, error,colors)\n",
    "    #@assert length(percentages) == length(labels) == length(colors)\n",
    "\n",
    "    bar(labels, percentages, fill=colors, yerr=error, alpha=0.5, legend=false)\n",
    "    xlabel!(\"Estimators\")\n",
    "    ylabel!(\"Percentage (%)\")\n",
    "    title!(\"Accuracy results\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "In the function developed before, we used three types of ensembles (Hard Voting, Soft Voting and Stacking). Hard Voting ensemble obtained the best results (93.35% accuracy, 5.08% std). During training process, we detected that DT and specially ANN has presented a big desviation of its accuracy. This problem could affect to the mean of final accuracy and std. On the other hand, KNN and SVC obtained a better performance during the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mIndices Base.OneTo(3) of attribute `fillcolor` does not match data indices 2:9.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Plots ~/.julia/packages/Plots/sxUvK/src/utils.jl:141\u001b[39m\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mData contains NaNs or missing values, and indices of `fillcolor` vector do not match data indices.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mIf you intend elements of `fillcolor` to apply to individual NaN-separated segments in the data,\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mpass each segment in a separate vector instead, and use a row vector for `fillcolor`. Legend entries\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mmay be suppressed by passing an empty label.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mFor example,\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m    plot([1:2,1:3], [[4,5],[3,4,5]], label=[\"y\" \"\"], fillcolor=[1 2])\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mIndices Base.OneTo(3) of attribute `fillcolor` does not match data indices 2:9.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Plots ~/.julia/packages/Plots/sxUvK/src/utils.jl:141\u001b[39m\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mData contains NaNs or missing values, and indices of `fillcolor` vector do not match data indices.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mIf you intend elements of `fillcolor` to apply to individual NaN-separated segments in the data,\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mpass each segment in a separate vector instead, and use a row vector for `fillcolor`. Legend entries\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mmay be suppressed by passing an empty label.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mFor example,\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m    plot([1:2,1:3], [[4,5],[3,4,5]], label=[\"y\" \"\"], fillcolor=[1 2])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip350\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip350)\" d=\"M0 1600 L2400 1600 L2400 0 L0 0  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip351\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip350)\" d=\"M205.121 1423.18 L2352.76 1423.18 L2352.76 123.472 L205.121 123.472  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip352\">\n",
       "    <rect x=\"205\" y=\"123\" width=\"2149\" height=\"1301\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip352)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"596.3,1423.18 596.3,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip352)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1278.94,1423.18 1278.94,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip352)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1961.58,1423.18 1961.58,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip350)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 2352.76,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip350)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"596.3,1423.18 596.3,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip350)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1278.94,1423.18 1278.94,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip350)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1961.58,1423.18 1961.58,1404.28 \"/>\n",
       "<path clip-path=\"url(#clip350)\" d=\"M519.981 1452.15 L519.981 1456.71 Q517.319 1455.44 514.958 1454.82 Q512.597 1454.19 510.398 1454.19 Q506.578 1454.19 504.495 1455.67 Q502.435 1457.15 502.435 1459.89 Q502.435 1462.18 503.801 1463.36 Q505.189 1464.52 509.032 1465.23 L511.856 1465.81 Q517.088 1466.81 519.564 1469.33 Q522.064 1471.83 522.064 1476.04 Q522.064 1481.07 518.685 1483.66 Q515.328 1486.25 508.824 1486.25 Q506.37 1486.25 503.592 1485.7 Q500.838 1485.14 497.875 1484.05 L497.875 1479.24 Q500.722 1480.83 503.453 1481.64 Q506.185 1482.45 508.824 1482.45 Q512.828 1482.45 515.004 1480.88 Q517.18 1479.31 517.18 1476.39 Q517.18 1473.84 515.606 1472.41 Q514.055 1470.97 510.49 1470.26 L507.643 1469.7 Q502.412 1468.66 500.074 1466.44 Q497.736 1464.21 497.736 1460.26 Q497.736 1455.67 500.953 1453.03 Q504.194 1450.39 509.865 1450.39 Q512.296 1450.39 514.819 1450.83 Q517.342 1451.27 519.981 1452.15 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M533.384 1452.29 L533.384 1459.65 L542.157 1459.65 L542.157 1462.96 L533.384 1462.96 L533.384 1477.04 Q533.384 1480.21 534.24 1481.11 Q535.12 1482.02 537.782 1482.02 L542.157 1482.02 L542.157 1485.58 L537.782 1485.58 Q532.851 1485.58 530.976 1483.75 Q529.101 1481.9 529.101 1477.04 L529.101 1462.96 L525.976 1462.96 L525.976 1459.65 L529.101 1459.65 L529.101 1452.29 L533.384 1452.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M559.541 1472.55 Q554.379 1472.55 552.388 1473.73 Q550.398 1474.91 550.398 1477.76 Q550.398 1480.02 551.879 1481.37 Q553.384 1482.69 555.953 1482.69 Q559.495 1482.69 561.624 1480.19 Q563.777 1477.66 563.777 1473.5 L563.777 1472.55 L559.541 1472.55 M568.036 1470.79 L568.036 1485.58 L563.777 1485.58 L563.777 1481.64 Q562.319 1484.01 560.143 1485.14 Q557.967 1486.25 554.819 1486.25 Q550.837 1486.25 548.476 1484.03 Q546.138 1481.78 546.138 1478.03 Q546.138 1473.66 549.055 1471.44 Q551.995 1469.21 557.805 1469.21 L563.777 1469.21 L563.777 1468.8 Q563.777 1465.86 561.833 1464.26 Q559.911 1462.64 556.416 1462.64 Q554.194 1462.64 552.087 1463.17 Q549.981 1463.7 548.036 1464.77 L548.036 1460.83 Q550.374 1459.93 552.573 1459.49 Q554.773 1459.03 556.856 1459.03 Q562.481 1459.03 565.259 1461.95 Q568.036 1464.86 568.036 1470.79 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M595.467 1460.65 L595.467 1464.63 Q593.661 1463.64 591.832 1463.15 Q590.027 1462.64 588.175 1462.64 Q584.032 1462.64 581.74 1465.28 Q579.448 1467.89 579.448 1472.64 Q579.448 1477.39 581.74 1480.02 Q584.032 1482.64 588.175 1482.64 Q590.027 1482.64 591.832 1482.15 Q593.661 1481.64 595.467 1480.65 L595.467 1484.58 Q593.684 1485.42 591.763 1485.83 Q589.865 1486.25 587.712 1486.25 Q581.856 1486.25 578.407 1482.57 Q574.958 1478.89 574.958 1472.64 Q574.958 1466.3 578.43 1462.66 Q581.925 1459.03 587.99 1459.03 Q589.958 1459.03 591.832 1459.45 Q593.707 1459.84 595.467 1460.65 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M602.712 1449.56 L606.994 1449.56 L606.994 1470.83 L619.703 1459.65 L625.143 1459.65 L611.393 1471.78 L625.721 1485.58 L620.166 1485.58 L606.994 1472.92 L606.994 1485.58 L602.712 1485.58 L602.712 1449.56 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M630.328 1459.65 L634.587 1459.65 L634.587 1485.58 L630.328 1485.58 L630.328 1459.65 M630.328 1449.56 L634.587 1449.56 L634.587 1454.96 L630.328 1454.96 L630.328 1449.56 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M665.05 1469.93 L665.05 1485.58 L660.79 1485.58 L660.79 1470.07 Q660.79 1466.39 659.355 1464.56 Q657.92 1462.73 655.05 1462.73 Q651.601 1462.73 649.61 1464.93 Q647.619 1467.13 647.619 1470.93 L647.619 1485.58 L643.337 1485.58 L643.337 1459.65 L647.619 1459.65 L647.619 1463.68 Q649.147 1461.34 651.207 1460.19 Q653.291 1459.03 655.999 1459.03 Q660.466 1459.03 662.758 1461.81 Q665.05 1464.56 665.05 1469.93 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M690.605 1472.32 Q690.605 1467.69 688.684 1465.14 Q686.786 1462.59 683.337 1462.59 Q679.911 1462.59 677.989 1465.14 Q676.091 1467.69 676.091 1472.32 Q676.091 1476.92 677.989 1479.47 Q679.911 1482.02 683.337 1482.02 Q686.786 1482.02 688.684 1479.47 Q690.605 1476.92 690.605 1472.32 M694.864 1482.36 Q694.864 1488.98 691.925 1492.2 Q688.985 1495.44 682.92 1495.44 Q680.675 1495.44 678.684 1495.09 Q676.693 1494.77 674.818 1494.08 L674.818 1489.93 Q676.693 1490.95 678.522 1491.44 Q680.351 1491.92 682.249 1491.92 Q686.438 1491.92 688.522 1489.72 Q690.605 1487.55 690.605 1483.13 L690.605 1481.02 Q689.286 1483.31 687.226 1484.45 Q685.165 1485.58 682.295 1485.58 Q677.526 1485.58 674.61 1481.95 Q671.693 1478.31 671.693 1472.32 Q671.693 1466.3 674.61 1462.66 Q677.526 1459.03 682.295 1459.03 Q685.165 1459.03 687.226 1460.16 Q689.286 1461.3 690.605 1463.59 L690.605 1459.65 L694.864 1459.65 L694.864 1482.36 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1226.69 1451.02 L1231.37 1451.02 L1231.37 1465.19 L1248.36 1465.19 L1248.36 1451.02 L1253.04 1451.02 L1253.04 1485.58 L1248.36 1485.58 L1248.36 1469.12 L1231.37 1469.12 L1231.37 1485.58 L1226.69 1485.58 L1226.69 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1273.94 1472.55 Q1268.78 1472.55 1266.79 1473.73 Q1264.79 1474.91 1264.79 1477.76 Q1264.79 1480.02 1266.28 1481.37 Q1267.78 1482.69 1270.35 1482.69 Q1273.89 1482.69 1276.02 1480.19 Q1278.17 1477.66 1278.17 1473.5 L1278.17 1472.55 L1273.94 1472.55 M1282.43 1470.79 L1282.43 1485.58 L1278.17 1485.58 L1278.17 1481.64 Q1276.72 1484.01 1274.54 1485.14 Q1272.36 1486.25 1269.22 1486.25 Q1265.23 1486.25 1262.87 1484.03 Q1260.54 1481.78 1260.54 1478.03 Q1260.54 1473.66 1263.45 1471.44 Q1266.39 1469.21 1272.2 1469.21 L1278.17 1469.21 L1278.17 1468.8 Q1278.17 1465.86 1276.23 1464.26 Q1274.31 1462.64 1270.81 1462.64 Q1268.59 1462.64 1266.48 1463.17 Q1264.38 1463.7 1262.43 1464.77 L1262.43 1460.83 Q1264.77 1459.93 1266.97 1459.49 Q1269.17 1459.03 1271.25 1459.03 Q1276.88 1459.03 1279.66 1461.95 Q1282.43 1464.86 1282.43 1470.79 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1306.23 1463.64 Q1305.51 1463.22 1304.66 1463.03 Q1303.82 1462.83 1302.8 1462.83 Q1299.19 1462.83 1297.25 1465.19 Q1295.33 1467.52 1295.33 1471.92 L1295.33 1485.58 L1291.04 1485.58 L1291.04 1459.65 L1295.33 1459.65 L1295.33 1463.68 Q1296.67 1461.32 1298.82 1460.19 Q1300.98 1459.03 1304.05 1459.03 Q1304.49 1459.03 1305.03 1459.1 Q1305.56 1459.14 1306.21 1459.26 L1306.23 1463.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1326.92 1463.59 L1326.92 1449.56 L1331.18 1449.56 L1331.18 1485.58 L1326.92 1485.58 L1326.92 1481.69 Q1325.58 1484.01 1323.52 1485.14 Q1321.48 1486.25 1318.61 1486.25 Q1313.92 1486.25 1310.95 1482.5 Q1308.01 1478.75 1308.01 1472.64 Q1308.01 1466.53 1310.95 1462.78 Q1313.92 1459.03 1318.61 1459.03 Q1321.48 1459.03 1323.52 1460.16 Q1325.58 1461.27 1326.92 1463.59 M1312.41 1472.64 Q1312.41 1477.34 1314.33 1480.02 Q1316.28 1482.69 1319.66 1482.69 Q1323.04 1482.69 1324.98 1480.02 Q1326.92 1477.34 1326.92 1472.64 Q1326.92 1467.94 1324.98 1465.28 Q1323.04 1462.59 1319.66 1462.59 Q1316.28 1462.59 1314.33 1465.28 Q1312.41 1467.94 1312.41 1472.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1939.18 1452.15 L1939.18 1456.71 Q1936.52 1455.44 1934.16 1454.82 Q1931.8 1454.19 1929.6 1454.19 Q1925.78 1454.19 1923.7 1455.67 Q1921.63 1457.15 1921.63 1459.89 Q1921.63 1462.18 1923 1463.36 Q1924.39 1464.52 1928.23 1465.23 L1931.06 1465.81 Q1936.29 1466.81 1938.76 1469.33 Q1941.26 1471.83 1941.26 1476.04 Q1941.26 1481.07 1937.88 1483.66 Q1934.53 1486.25 1928.02 1486.25 Q1925.57 1486.25 1922.79 1485.7 Q1920.04 1485.14 1917.07 1484.05 L1917.07 1479.24 Q1919.92 1480.83 1922.65 1481.64 Q1925.38 1482.45 1928.02 1482.45 Q1932.03 1482.45 1934.2 1480.88 Q1936.38 1479.31 1936.38 1476.39 Q1936.38 1473.84 1934.81 1472.41 Q1933.26 1470.97 1929.69 1470.26 L1926.84 1469.7 Q1921.61 1468.66 1919.27 1466.44 Q1916.94 1464.21 1916.94 1460.26 Q1916.94 1455.67 1920.15 1453.03 Q1923.39 1450.39 1929.07 1450.39 Q1931.5 1450.39 1934.02 1450.83 Q1936.54 1451.27 1939.18 1452.15 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1958.42 1462.64 Q1954.99 1462.64 1953 1465.33 Q1951.01 1467.99 1951.01 1472.64 Q1951.01 1477.29 1952.98 1479.98 Q1954.97 1482.64 1958.42 1482.64 Q1961.82 1482.64 1963.81 1479.95 Q1965.8 1477.27 1965.8 1472.64 Q1965.8 1468.03 1963.81 1465.35 Q1961.82 1462.64 1958.42 1462.64 M1958.42 1459.03 Q1963.97 1459.03 1967.14 1462.64 Q1970.32 1466.25 1970.32 1472.64 Q1970.32 1479.01 1967.14 1482.64 Q1963.97 1486.25 1958.42 1486.25 Q1952.84 1486.25 1949.67 1482.64 Q1946.52 1479.01 1946.52 1472.64 Q1946.52 1466.25 1949.67 1462.64 Q1952.84 1459.03 1958.42 1459.03 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1990.5 1449.56 L1990.5 1453.1 L1986.43 1453.1 Q1984.13 1453.1 1983.23 1454.03 Q1982.35 1454.96 1982.35 1457.36 L1982.35 1459.65 L1989.37 1459.65 L1989.37 1462.96 L1982.35 1462.96 L1982.35 1485.58 L1978.07 1485.58 L1978.07 1462.96 L1974 1462.96 L1974 1459.65 L1978.07 1459.65 L1978.07 1457.85 Q1978.07 1453.52 1980.08 1451.55 Q1982.1 1449.56 1986.47 1449.56 L1990.5 1449.56 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1997.44 1452.29 L1997.44 1459.65 L2006.22 1459.65 L2006.22 1462.96 L1997.44 1462.96 L1997.44 1477.04 Q1997.44 1480.21 1998.3 1481.11 Q1999.18 1482.02 2001.84 1482.02 L2006.22 1482.02 L2006.22 1485.58 L2001.84 1485.58 Q1996.91 1485.58 1995.04 1483.75 Q1993.16 1481.9 1993.16 1477.04 L1993.16 1462.96 L1990.04 1462.96 L1990.04 1459.65 L1993.16 1459.65 L1993.16 1452.29 L1997.44 1452.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1109.5 1520.52 L1139.55 1520.52 L1139.55 1525.93 L1115.93 1525.93 L1115.93 1540 L1138.56 1540 L1138.56 1545.41 L1115.93 1545.41 L1115.93 1562.63 L1140.12 1562.63 L1140.12 1568.04 L1109.5 1568.04 L1109.5 1520.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1173.16 1533.45 L1173.16 1538.98 Q1170.67 1537.71 1168 1537.07 Q1165.33 1536.44 1162.46 1536.44 Q1158.1 1536.44 1155.91 1537.77 Q1153.74 1539.11 1153.74 1541.79 Q1153.74 1543.82 1155.3 1545 Q1156.86 1546.15 1161.57 1547.2 L1163.58 1547.64 Q1169.81 1548.98 1172.42 1551.43 Q1175.07 1553.85 1175.07 1558.21 Q1175.07 1563.17 1171.12 1566.07 Q1167.2 1568.97 1160.33 1568.97 Q1157.46 1568.97 1154.35 1568.39 Q1151.26 1567.85 1147.82 1566.74 L1147.82 1560.69 Q1151.07 1562.38 1154.22 1563.24 Q1157.37 1564.07 1160.46 1564.07 Q1164.59 1564.07 1166.82 1562.66 Q1169.05 1561.23 1169.05 1558.65 Q1169.05 1556.27 1167.43 1554.99 Q1165.84 1553.72 1160.39 1552.54 L1158.36 1552.07 Q1152.91 1550.92 1150.49 1548.56 Q1148.08 1546.18 1148.08 1542.04 Q1148.08 1537.01 1151.64 1534.27 Q1155.21 1531.54 1161.76 1531.54 Q1165.01 1531.54 1167.87 1532.01 Q1170.74 1532.49 1173.16 1533.45 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1190.18 1522.27 L1190.18 1532.4 L1202.25 1532.4 L1202.25 1536.95 L1190.18 1536.95 L1190.18 1556.3 Q1190.18 1560.66 1191.36 1561.9 Q1192.57 1563.14 1196.23 1563.14 L1202.25 1563.14 L1202.25 1568.04 L1196.23 1568.04 Q1189.45 1568.04 1186.87 1565.53 Q1184.3 1562.98 1184.3 1556.3 L1184.3 1536.95 L1180 1536.95 L1180 1532.4 L1184.3 1532.4 L1184.3 1522.27 L1190.18 1522.27 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1209.95 1532.4 L1215.81 1532.4 L1215.81 1568.04 L1209.95 1568.04 L1209.95 1532.4 M1209.95 1518.52 L1215.81 1518.52 L1215.81 1525.93 L1209.95 1525.93 L1209.95 1518.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1255.82 1539.24 Q1258.01 1535.29 1261.07 1533.41 Q1264.12 1531.54 1268.26 1531.54 Q1273.83 1531.54 1276.85 1535.45 Q1279.88 1539.33 1279.88 1546.53 L1279.88 1568.04 L1273.99 1568.04 L1273.99 1546.72 Q1273.99 1541.59 1272.17 1539.11 Q1270.36 1536.63 1266.64 1536.63 Q1262.09 1536.63 1259.44 1539.65 Q1256.8 1542.68 1256.8 1547.9 L1256.8 1568.04 L1250.91 1568.04 L1250.91 1546.72 Q1250.91 1541.56 1249.1 1539.11 Q1247.28 1536.63 1243.5 1536.63 Q1239.01 1536.63 1236.37 1539.68 Q1233.73 1542.71 1233.73 1547.9 L1233.73 1568.04 L1227.84 1568.04 L1227.84 1532.4 L1233.73 1532.4 L1233.73 1537.93 Q1235.73 1534.66 1238.53 1533.1 Q1241.33 1531.54 1245.18 1531.54 Q1249.07 1531.54 1251.77 1533.51 Q1254.51 1535.48 1255.82 1539.24 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1307.76 1550.12 Q1300.66 1550.12 1297.92 1551.75 Q1295.19 1553.37 1295.19 1557.29 Q1295.19 1560.4 1297.22 1562.25 Q1299.29 1564.07 1302.83 1564.07 Q1307.7 1564.07 1310.62 1560.63 Q1313.58 1557.16 1313.58 1551.43 L1313.58 1550.12 L1307.76 1550.12 M1319.44 1547.71 L1319.44 1568.04 L1313.58 1568.04 L1313.58 1562.63 Q1311.58 1565.88 1308.59 1567.44 Q1305.59 1568.97 1301.27 1568.97 Q1295.79 1568.97 1292.55 1565.91 Q1289.33 1562.82 1289.33 1557.67 Q1289.33 1551.65 1293.34 1548.6 Q1297.38 1545.54 1305.37 1545.54 L1313.58 1545.54 L1313.58 1544.97 Q1313.58 1540.93 1310.91 1538.73 Q1308.27 1536.5 1303.46 1536.5 Q1300.41 1536.5 1297.51 1537.23 Q1294.61 1537.97 1291.94 1539.43 L1291.94 1534.02 Q1295.16 1532.78 1298.18 1532.17 Q1301.2 1531.54 1304.07 1531.54 Q1311.8 1531.54 1315.62 1535.55 Q1319.44 1539.56 1319.44 1547.71 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1337.3 1522.27 L1337.3 1532.4 L1349.36 1532.4 L1349.36 1536.95 L1337.3 1536.95 L1337.3 1556.3 Q1337.3 1560.66 1338.47 1561.9 Q1339.68 1563.14 1343.34 1563.14 L1349.36 1563.14 L1349.36 1568.04 L1343.34 1568.04 Q1336.56 1568.04 1333.99 1565.53 Q1331.41 1562.98 1331.41 1556.3 L1331.41 1536.95 L1327.11 1536.95 L1327.11 1532.4 L1331.41 1532.4 L1331.41 1522.27 L1337.3 1522.27 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1370.88 1536.5 Q1366.16 1536.5 1363.43 1540.19 Q1360.69 1543.85 1360.69 1550.25 Q1360.69 1556.65 1363.4 1560.34 Q1366.13 1564 1370.88 1564 Q1375.55 1564 1378.29 1560.31 Q1381.03 1556.62 1381.03 1550.25 Q1381.03 1543.92 1378.29 1540.23 Q1375.55 1536.5 1370.88 1536.5 M1370.88 1531.54 Q1378.51 1531.54 1382.87 1536.5 Q1387.24 1541.47 1387.24 1550.25 Q1387.24 1559 1382.87 1564 Q1378.51 1568.97 1370.88 1568.97 Q1363.2 1568.97 1358.84 1564 Q1354.52 1559 1354.52 1550.25 Q1354.52 1541.47 1358.84 1536.5 Q1363.2 1531.54 1370.88 1531.54 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1417.6 1537.87 Q1416.61 1537.3 1415.44 1537.04 Q1414.29 1536.76 1412.89 1536.76 Q1407.92 1536.76 1405.25 1540 Q1402.61 1543.22 1402.61 1549.27 L1402.61 1568.04 L1396.72 1568.04 L1396.72 1532.4 L1402.61 1532.4 L1402.61 1537.93 Q1404.45 1534.69 1407.41 1533.13 Q1410.37 1531.54 1414.61 1531.54 Q1415.21 1531.54 1415.94 1531.63 Q1416.68 1531.7 1417.57 1531.85 L1417.6 1537.87 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1446.47 1533.45 L1446.47 1538.98 Q1443.99 1537.71 1441.31 1537.07 Q1438.64 1536.44 1435.77 1536.44 Q1431.41 1536.44 1429.22 1537.77 Q1427.05 1539.11 1427.05 1541.79 Q1427.05 1543.82 1428.61 1545 Q1430.17 1546.15 1434.88 1547.2 L1436.89 1547.64 Q1443.13 1548.98 1445.74 1551.43 Q1448.38 1553.85 1448.38 1558.21 Q1448.38 1563.17 1444.43 1566.07 Q1440.52 1568.97 1433.64 1568.97 Q1430.78 1568.97 1427.66 1568.39 Q1424.57 1567.85 1421.13 1566.74 L1421.13 1560.69 Q1424.38 1562.38 1427.53 1563.24 Q1430.68 1564.07 1433.77 1564.07 Q1437.91 1564.07 1440.13 1562.66 Q1442.36 1561.23 1442.36 1558.65 Q1442.36 1556.27 1440.74 1554.99 Q1439.15 1553.72 1433.7 1552.54 L1431.67 1552.07 Q1426.22 1550.92 1423.81 1548.56 Q1421.39 1546.18 1421.39 1542.04 Q1421.39 1537.01 1424.95 1534.27 Q1428.52 1531.54 1435.07 1531.54 Q1438.32 1531.54 1441.18 1532.01 Q1444.05 1532.49 1446.47 1533.45 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip352)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,1423.18 2352.76,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip352)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,1159.13 2352.76,1159.13 \"/>\n",
       "<polyline clip-path=\"url(#clip352)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,895.085 2352.76,895.085 \"/>\n",
       "<polyline clip-path=\"url(#clip352)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,631.037 2352.76,631.037 \"/>\n",
       "<polyline clip-path=\"url(#clip352)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,366.989 2352.76,366.989 \"/>\n",
       "<polyline clip-path=\"url(#clip350)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 205.121,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip350)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 224.019,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip350)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1159.13 224.019,1159.13 \"/>\n",
       "<polyline clip-path=\"url(#clip350)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,895.085 224.019,895.085 \"/>\n",
       "<polyline clip-path=\"url(#clip350)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,631.037 224.019,631.037 \"/>\n",
       "<polyline clip-path=\"url(#clip350)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,366.989 224.019,366.989 \"/>\n",
       "<path clip-path=\"url(#clip350)\" d=\"M157.177 1408.98 Q153.566 1408.98 151.737 1412.54 Q149.931 1416.08 149.931 1423.21 Q149.931 1430.32 151.737 1433.89 Q153.566 1437.43 157.177 1437.43 Q160.811 1437.43 162.616 1433.89 Q164.445 1430.32 164.445 1423.21 Q164.445 1416.08 162.616 1412.54 Q160.811 1408.98 157.177 1408.98 M157.177 1405.27 Q162.987 1405.27 166.042 1409.88 Q169.121 1414.46 169.121 1423.21 Q169.121 1431.94 166.042 1436.55 Q162.987 1441.13 157.177 1441.13 Q151.366 1441.13 148.288 1436.55 Q145.232 1431.94 145.232 1423.21 Q145.232 1414.46 148.288 1409.88 Q151.366 1405.27 157.177 1405.27 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M121.043 1172.48 L137.362 1172.48 L137.362 1176.41 L115.418 1176.41 L115.418 1172.48 Q118.08 1169.72 122.663 1165.09 Q127.269 1160.44 128.45 1159.1 Q130.695 1156.57 131.575 1154.84 Q132.478 1153.08 132.478 1151.39 Q132.478 1148.63 130.533 1146.9 Q128.612 1145.16 125.51 1145.16 Q123.311 1145.16 120.857 1145.93 Q118.427 1146.69 115.649 1148.24 L115.649 1143.52 Q118.473 1142.38 120.927 1141.81 Q123.38 1141.23 125.418 1141.23 Q130.788 1141.23 133.982 1143.91 Q137.177 1146.6 137.177 1151.09 Q137.177 1153.22 136.367 1155.14 Q135.579 1157.04 133.473 1159.63 Q132.894 1160.3 129.792 1163.52 Q126.691 1166.71 121.043 1172.48 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M157.177 1144.93 Q153.566 1144.93 151.737 1148.5 Q149.931 1152.04 149.931 1159.17 Q149.931 1166.27 151.737 1169.84 Q153.566 1173.38 157.177 1173.38 Q160.811 1173.38 162.616 1169.84 Q164.445 1166.27 164.445 1159.17 Q164.445 1152.04 162.616 1148.5 Q160.811 1144.93 157.177 1144.93 M157.177 1141.23 Q162.987 1141.23 166.042 1145.83 Q169.121 1150.42 169.121 1159.17 Q169.121 1167.89 166.042 1172.5 Q162.987 1177.08 157.177 1177.08 Q151.366 1177.08 148.288 1172.5 Q145.232 1167.89 145.232 1159.17 Q145.232 1150.42 148.288 1145.83 Q151.366 1141.23 157.177 1141.23 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M129.862 881.879 L118.056 900.328 L129.862 900.328 L129.862 881.879 M128.635 877.805 L134.515 877.805 L134.515 900.328 L139.445 900.328 L139.445 904.216 L134.515 904.216 L134.515 912.365 L129.862 912.365 L129.862 904.216 L114.26 904.216 L114.26 899.703 L128.635 877.805 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M157.177 880.883 Q153.566 880.883 151.737 884.448 Q149.931 887.99 149.931 895.119 Q149.931 902.226 151.737 905.79 Q153.566 909.332 157.177 909.332 Q160.811 909.332 162.616 905.79 Q164.445 902.226 164.445 895.119 Q164.445 887.99 162.616 884.448 Q160.811 880.883 157.177 880.883 M157.177 877.18 Q162.987 877.18 166.042 881.786 Q169.121 886.369 169.121 895.119 Q169.121 903.846 166.042 908.453 Q162.987 913.036 157.177 913.036 Q151.366 913.036 148.288 908.453 Q145.232 903.846 145.232 895.119 Q145.232 886.369 148.288 881.786 Q151.366 877.18 157.177 877.18 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M127.593 629.173 Q124.445 629.173 122.593 631.326 Q120.765 633.479 120.765 637.229 Q120.765 640.956 122.593 643.132 Q124.445 645.284 127.593 645.284 Q130.742 645.284 132.57 643.132 Q134.422 640.956 134.422 637.229 Q134.422 633.479 132.57 631.326 Q130.742 629.173 127.593 629.173 M136.876 614.521 L136.876 618.78 Q135.117 617.947 133.311 617.507 Q131.529 617.067 129.769 617.067 Q125.14 617.067 122.686 620.192 Q120.255 623.317 119.908 629.636 Q121.274 627.623 123.334 626.558 Q125.394 625.47 127.871 625.47 Q133.08 625.47 136.089 628.641 Q139.121 631.789 139.121 637.229 Q139.121 642.553 135.973 645.771 Q132.825 648.988 127.593 648.988 Q121.598 648.988 118.427 644.405 Q115.256 639.798 115.256 631.072 Q115.256 622.877 119.144 618.016 Q123.033 613.132 129.584 613.132 Q131.343 613.132 133.126 613.479 Q134.931 613.826 136.876 614.521 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M157.177 616.836 Q153.566 616.836 151.737 620.4 Q149.931 623.942 149.931 631.072 Q149.931 638.178 151.737 641.743 Q153.566 645.284 157.177 645.284 Q160.811 645.284 162.616 641.743 Q164.445 638.178 164.445 631.072 Q164.445 623.942 162.616 620.4 Q160.811 616.836 157.177 616.836 M157.177 613.132 Q162.987 613.132 166.042 617.738 Q169.121 622.322 169.121 631.072 Q169.121 639.798 166.042 644.405 Q162.987 648.988 157.177 648.988 Q151.366 648.988 148.288 644.405 Q145.232 639.798 145.232 631.072 Q145.232 622.322 148.288 617.738 Q151.366 613.132 157.177 613.132 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M127.015 367.857 Q123.681 367.857 121.76 369.64 Q119.862 371.422 119.862 374.547 Q119.862 377.672 121.76 379.454 Q123.681 381.237 127.015 381.237 Q130.348 381.237 132.269 379.454 Q134.191 377.649 134.191 374.547 Q134.191 371.422 132.269 369.64 Q130.371 367.857 127.015 367.857 M122.339 365.867 Q119.33 365.126 117.64 363.066 Q115.973 361.005 115.973 358.043 Q115.973 353.899 118.913 351.492 Q121.876 349.084 127.015 349.084 Q132.177 349.084 135.117 351.492 Q138.056 353.899 138.056 358.043 Q138.056 361.005 136.367 363.066 Q134.7 365.126 131.714 365.867 Q135.093 366.654 136.968 368.945 Q138.867 371.237 138.867 374.547 Q138.867 379.57 135.788 382.255 Q132.732 384.941 127.015 384.941 Q121.297 384.941 118.218 382.255 Q115.163 379.57 115.163 374.547 Q115.163 371.237 117.061 368.945 Q118.959 366.654 122.339 365.867 M120.626 358.482 Q120.626 361.167 122.293 362.672 Q123.982 364.177 127.015 364.177 Q130.024 364.177 131.714 362.672 Q133.427 361.167 133.427 358.482 Q133.427 355.797 131.714 354.293 Q130.024 352.788 127.015 352.788 Q123.982 352.788 122.293 354.293 Q120.626 355.797 120.626 358.482 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M157.177 352.788 Q153.566 352.788 151.737 356.353 Q149.931 359.894 149.931 367.024 Q149.931 374.13 151.737 377.695 Q153.566 381.237 157.177 381.237 Q160.811 381.237 162.616 377.695 Q164.445 374.13 164.445 367.024 Q164.445 359.894 162.616 356.353 Q160.811 352.788 157.177 352.788 M157.177 349.084 Q162.987 349.084 166.042 353.691 Q169.121 358.274 169.121 367.024 Q169.121 375.751 166.042 380.357 Q162.987 384.941 157.177 384.941 Q151.366 384.941 148.288 380.357 Q145.232 375.751 145.232 367.024 Q145.232 358.274 148.288 353.691 Q151.366 349.084 157.177 349.084 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M21.7677 1011.01 L39.6235 1011.01 L39.6235 1002.92 Q39.6235 998.433 37.3 995.982 Q34.9765 993.532 30.6797 993.532 Q26.4147 993.532 24.0912 995.982 Q21.7677 998.433 21.7677 1002.92 L21.7677 1011.01 M16.4842 1017.43 L16.4842 1002.92 Q16.4842 994.932 20.1126 990.858 Q23.7092 986.752 30.6797 986.752 Q37.7138 986.752 41.3104 990.858 Q44.907 994.932 44.907 1002.92 L44.907 1011.01 L64.0042 1011.01 L64.0042 1017.43 L16.4842 1017.43 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M44.7161 950.213 L47.5806 950.213 L47.5806 977.14 Q53.6281 976.758 56.8109 973.512 Q59.9619 970.233 59.9619 964.409 Q59.9619 961.035 59.1344 957.884 Q58.3069 954.701 56.6518 951.582 L62.1899 951.582 Q63.5267 954.733 64.227 958.043 Q64.9272 961.353 64.9272 964.759 Q64.9272 973.289 59.9619 978.286 Q54.9967 983.251 46.5303 983.251 Q37.7774 983.251 32.6531 978.54 Q27.4968 973.798 27.4968 965.777 Q27.4968 958.584 32.1438 954.414 Q36.7589 950.213 44.7161 950.213 M42.9973 956.07 Q38.1912 956.133 35.3266 958.775 Q32.4621 961.385 32.4621 965.714 Q32.4621 970.615 35.2312 973.575 Q38.0002 976.503 43.0292 976.949 L42.9973 956.07 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M33.8307 919.944 Q33.2578 920.931 33.0032 922.108 Q32.7167 923.254 32.7167 924.655 Q32.7167 929.62 35.9632 932.294 Q39.1779 934.935 45.2253 934.935 L64.0042 934.935 L64.0042 940.824 L28.3562 940.824 L28.3562 934.935 L33.8944 934.935 Q30.6479 933.089 29.0883 930.129 Q27.4968 927.169 27.4968 922.936 Q27.4968 922.331 27.5923 921.599 Q27.656 920.867 27.8151 919.976 L33.8307 919.944 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M29.7248 889.58 L35.1993 889.58 Q33.8307 892.062 33.1623 894.577 Q32.4621 897.059 32.4621 899.606 Q32.4621 905.303 36.0905 908.454 Q39.6872 911.605 46.212 911.605 Q52.7369 911.605 56.3653 908.454 Q59.9619 905.303 59.9619 899.606 Q59.9619 897.059 59.2935 894.577 Q58.5933 892.062 57.2247 889.58 L62.6355 889.58 Q63.7814 892.03 64.3543 894.672 Q64.9272 897.282 64.9272 900.242 Q64.9272 908.295 59.8664 913.037 Q54.8057 917.78 46.212 917.78 Q37.491 917.78 32.4939 913.006 Q27.4968 908.199 27.4968 899.86 Q27.4968 897.155 28.0697 894.577 Q28.6108 891.999 29.7248 889.58 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M44.7161 848.903 L47.5806 848.903 L47.5806 875.83 Q53.6281 875.448 56.8109 872.201 Q59.9619 868.923 59.9619 863.098 Q59.9619 859.725 59.1344 856.574 Q58.3069 853.391 56.6518 850.271 L62.1899 850.271 Q63.5267 853.422 64.227 856.733 Q64.9272 860.043 64.9272 863.448 Q64.9272 871.979 59.9619 876.976 Q54.9967 881.941 46.5303 881.941 Q37.7774 881.941 32.6531 877.23 Q27.4968 872.488 27.4968 864.467 Q27.4968 857.274 32.1438 853.104 Q36.7589 848.903 44.7161 848.903 M42.9973 854.759 Q38.1912 854.823 35.3266 857.465 Q32.4621 860.075 32.4621 864.403 Q32.4621 869.305 35.2312 872.265 Q38.0002 875.193 43.0292 875.639 L42.9973 854.759 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M42.4881 809.658 L64.0042 809.658 L64.0042 815.515 L42.679 815.515 Q37.6183 815.515 35.1038 817.488 Q32.5894 819.461 32.5894 823.408 Q32.5894 828.151 35.6131 830.888 Q38.6368 833.625 43.8567 833.625 L64.0042 833.625 L64.0042 839.513 L28.3562 839.513 L28.3562 833.625 L33.8944 833.625 Q30.6797 831.524 29.0883 828.692 Q27.4968 825.827 27.4968 822.103 Q27.4968 815.96 31.3163 812.809 Q35.1038 809.658 42.4881 809.658 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M18.2347 792.184 L28.3562 792.184 L28.3562 780.121 L32.9077 780.121 L32.9077 792.184 L52.2594 792.184 Q56.6199 792.184 57.8613 791.007 Q59.1026 789.797 59.1026 786.137 L59.1026 780.121 L64.0042 780.121 L64.0042 786.137 Q64.0042 792.916 61.4897 795.495 Q58.9434 798.073 52.2594 798.073 L32.9077 798.073 L32.9077 802.37 L28.3562 802.37 L28.3562 798.073 L18.2347 798.073 L18.2347 792.184 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M46.0847 756.218 Q46.0847 763.316 47.7079 766.053 Q49.3312 768.79 53.2461 768.79 Q56.3653 768.79 58.2114 766.753 Q60.0256 764.685 60.0256 761.152 Q60.0256 756.282 56.5881 753.354 Q53.1188 750.394 47.3897 750.394 L46.0847 750.394 L46.0847 756.218 M43.6657 744.537 L64.0042 744.537 L64.0042 750.394 L58.5933 750.394 Q61.8398 752.399 63.3994 755.391 Q64.9272 758.382 64.9272 762.711 Q64.9272 768.186 61.8716 771.432 Q58.7843 774.647 53.6281 774.647 Q47.6125 774.647 44.5569 770.636 Q41.5014 766.594 41.5014 758.605 L41.5014 750.394 L40.9285 750.394 Q36.8862 750.394 34.6901 753.067 Q32.4621 755.709 32.4621 760.515 Q32.4621 763.571 33.1941 766.467 Q33.9262 769.363 35.3903 772.037 L29.9795 772.037 Q28.7381 768.822 28.1334 765.799 Q27.4968 762.775 27.4968 759.91 Q27.4968 752.176 31.5072 748.356 Q35.5176 744.537 43.6657 744.537 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M45.7664 709.016 Q39.4007 709.016 35.8996 711.658 Q32.3984 714.268 32.3984 719.011 Q32.3984 723.721 35.8996 726.363 Q39.4007 728.973 45.7664 728.973 Q52.1003 728.973 55.6014 726.363 Q59.1026 723.721 59.1026 719.011 Q59.1026 714.268 55.6014 711.658 Q52.1003 709.016 45.7664 709.016 M59.58 703.16 Q68.683 703.16 73.1071 707.202 Q77.5631 711.244 77.5631 719.584 Q77.5631 722.671 77.0857 725.408 Q76.6401 728.145 75.6852 730.723 L69.9879 730.723 Q71.3884 728.145 72.0568 725.631 Q72.7252 723.116 72.7252 720.507 Q72.7252 714.746 69.7015 711.881 Q66.7096 709.016 60.6303 709.016 L57.7339 709.016 Q60.885 710.831 62.4446 713.663 Q64.0042 716.496 64.0042 720.443 Q64.0042 727 59.0071 731.01 Q54.01 735.02 45.7664 735.02 Q37.491 735.02 32.4939 731.01 Q27.4968 727 27.4968 720.443 Q27.4968 716.496 29.0564 713.663 Q30.616 710.831 33.7671 709.016 L28.3562 709.016 L28.3562 703.16 L59.58 703.16 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M44.7161 660.605 L47.5806 660.605 L47.5806 687.532 Q53.6281 687.15 56.8109 683.904 Q59.9619 680.625 59.9619 674.801 Q59.9619 671.427 59.1344 668.276 Q58.3069 665.093 56.6518 661.974 L62.1899 661.974 Q63.5267 665.125 64.227 668.435 Q64.9272 671.745 64.9272 675.151 Q64.9272 683.681 59.9619 688.678 Q54.9967 693.643 46.5303 693.643 Q37.7774 693.643 32.6531 688.933 Q27.4968 684.19 27.4968 676.169 Q27.4968 668.976 32.1438 664.807 Q36.7589 660.605 44.7161 660.605 M42.9973 666.462 Q38.1912 666.525 35.3266 669.167 Q32.4621 671.777 32.4621 676.106 Q32.4621 681.007 35.2312 683.967 Q38.0002 686.896 43.0292 687.341 L42.9973 666.462 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M14.5426 616.204 Q21.8632 620.469 29.0246 622.538 Q36.186 624.607 43.5384 624.607 Q50.8908 624.607 58.1159 622.538 Q65.3091 620.438 72.5979 616.204 L72.5979 621.297 Q65.1182 626.071 57.8931 628.458 Q50.668 630.814 43.5384 630.814 Q36.4406 630.814 29.2474 628.458 Q22.0542 626.103 14.5426 621.297 L14.5426 616.204 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M43.0928 563.592 Q43.0928 566.361 45.4481 567.952 Q47.8034 569.512 52.0048 569.512 Q56.1425 569.512 58.5296 567.952 Q60.885 566.361 60.885 563.592 Q60.885 560.886 58.5296 559.327 Q56.1425 557.735 52.0048 557.735 Q47.8353 557.735 45.48 559.327 Q43.0928 560.886 43.0928 563.592 M39.0506 563.592 Q39.0506 558.563 42.5517 555.603 Q46.0529 552.643 52.0048 552.643 Q57.9567 552.643 61.4579 555.635 Q64.9272 558.595 64.9272 563.592 Q64.9272 568.684 61.4579 571.644 Q57.9567 574.605 52.0048 574.605 Q46.021 574.605 42.5517 571.644 Q39.0506 568.653 39.0506 563.592 M19.667 596.439 Q19.667 599.176 22.0542 600.768 Q24.4095 602.327 28.5472 602.327 Q32.7485 602.327 35.1038 600.768 Q37.4592 599.208 37.4592 596.439 Q37.4592 593.67 35.1038 592.11 Q32.7485 590.519 28.5472 590.519 Q24.4413 590.519 22.0542 592.11 Q19.667 593.702 19.667 596.439 M15.6248 567.698 L15.6248 562.605 L64.9272 592.333 L64.9272 597.426 L15.6248 567.698 M15.6248 596.439 Q15.6248 591.41 19.1259 588.418 Q22.5952 585.426 28.5472 585.426 Q34.5628 585.426 38.0321 588.418 Q41.5014 591.378 41.5014 596.439 Q41.5014 601.5 38.0321 604.46 Q34.5309 607.388 28.5472 607.388 Q22.6271 607.388 19.1259 604.428 Q15.6248 601.468 15.6248 596.439 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M14.5426 543.826 L14.5426 538.734 Q22.0542 533.96 29.2474 531.604 Q36.4406 529.217 43.5384 529.217 Q50.668 529.217 57.8931 531.604 Q65.1182 533.96 72.5979 538.734 L72.5979 543.826 Q65.3091 539.593 58.1159 537.524 Q50.8908 535.424 43.5384 535.424 Q36.186 535.424 29.0246 537.524 Q21.8632 539.593 14.5426 543.826 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M967.464 20.1573 L956.365 50.2555 L978.604 50.2555 L967.464 20.1573 M962.846 12.096 L972.123 12.096 L995.173 72.576 L986.666 72.576 L981.156 57.061 L953.894 57.061 L948.385 72.576 L939.756 72.576 L962.846 12.096 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1034.87 28.9478 L1034.87 35.9153 Q1031.71 34.1734 1028.51 33.3227 Q1025.35 32.4315 1022.11 32.4315 Q1014.86 32.4315 1010.85 37.0496 Q1006.84 41.6271 1006.84 49.9314 Q1006.84 58.2358 1010.85 62.8538 Q1014.86 67.4314 1022.11 67.4314 Q1025.35 67.4314 1028.51 66.5807 Q1031.71 65.6895 1034.87 63.9476 L1034.87 70.8341 Q1031.75 72.2924 1028.39 73.0216 Q1025.07 73.7508 1021.3 73.7508 Q1011.05 73.7508 1005.02 67.3098 Q998.98 60.8689 998.98 49.9314 Q998.98 38.832 1005.06 32.472 Q1011.17 26.1121 1021.79 26.1121 Q1025.23 26.1121 1028.51 26.8413 Q1031.79 27.5299 1034.87 28.9478 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1080.48 28.9478 L1080.48 35.9153 Q1077.32 34.1734 1074.12 33.3227 Q1070.97 32.4315 1067.72 32.4315 Q1060.47 32.4315 1056.46 37.0496 Q1052.45 41.6271 1052.45 49.9314 Q1052.45 58.2358 1056.46 62.8538 Q1060.47 67.4314 1067.72 67.4314 Q1070.97 67.4314 1074.12 66.5807 Q1077.32 65.6895 1080.48 63.9476 L1080.48 70.8341 Q1077.37 72.2924 1074 73.0216 Q1070.68 73.7508 1066.91 73.7508 Q1056.67 73.7508 1050.63 67.3098 Q1044.59 60.8689 1044.59 49.9314 Q1044.59 38.832 1050.67 32.472 Q1056.79 26.1121 1067.4 26.1121 Q1070.84 26.1121 1074.12 26.8413 Q1077.41 27.5299 1080.48 28.9478 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1092.68 54.671 L1092.68 27.2059 L1100.13 27.2059 L1100.13 54.3874 Q1100.13 60.8284 1102.64 64.0691 Q1105.15 67.2693 1110.18 67.2693 Q1116.21 67.2693 1119.7 63.421 Q1123.22 59.5726 1123.22 52.9291 L1123.22 27.2059 L1130.68 27.2059 L1130.68 72.576 L1123.22 72.576 L1123.22 65.6084 Q1120.51 69.7404 1116.9 71.7658 Q1113.34 73.7508 1108.6 73.7508 Q1100.78 73.7508 1096.73 68.8897 Q1092.68 64.0286 1092.68 54.671 M1111.43 26.1121 L1111.43 26.1121 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1172.32 34.1734 Q1171.06 33.4443 1169.56 33.1202 Q1168.11 32.7556 1166.32 32.7556 Q1160 32.7556 1156.6 36.8875 Q1153.24 40.9789 1153.24 48.6757 L1153.24 72.576 L1145.74 72.576 L1145.74 27.2059 L1153.24 27.2059 L1153.24 34.2544 Q1155.59 30.1225 1159.36 28.1376 Q1163.12 26.1121 1168.51 26.1121 Q1169.28 26.1121 1170.21 26.2337 Q1171.14 26.3147 1172.28 26.5172 L1172.32 34.1734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1200.76 49.7694 Q1191.72 49.7694 1188.24 51.8354 Q1184.75 53.9013 1184.75 58.8839 Q1184.75 62.8538 1187.35 65.2034 Q1189.98 67.5124 1194.48 67.5124 Q1200.67 67.5124 1204.4 63.1374 Q1208.17 58.7219 1208.17 51.4303 L1208.17 49.7694 L1200.76 49.7694 M1215.62 46.6907 L1215.62 72.576 L1208.17 72.576 L1208.17 65.6895 Q1205.62 69.8214 1201.81 71.8063 Q1198 73.7508 1192.49 73.7508 Q1185.52 73.7508 1181.39 69.8619 Q1177.3 65.9325 1177.3 59.3701 Q1177.3 51.7138 1182.41 47.825 Q1187.55 43.9361 1197.72 43.9361 L1208.17 43.9361 L1208.17 43.2069 Q1208.17 38.0623 1204.77 35.2672 Q1201.4 32.4315 1195.29 32.4315 Q1191.4 32.4315 1187.71 33.3632 Q1184.03 34.295 1180.62 36.1584 L1180.62 29.2718 Q1184.71 27.692 1188.56 26.9223 Q1192.41 26.1121 1196.06 26.1121 Q1205.9 26.1121 1210.76 31.2163 Q1215.62 36.3204 1215.62 46.6907 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1263.63 28.9478 L1263.63 35.9153 Q1260.47 34.1734 1257.27 33.3227 Q1254.11 32.4315 1250.87 32.4315 Q1243.61 32.4315 1239.6 37.0496 Q1235.59 41.6271 1235.59 49.9314 Q1235.59 58.2358 1239.6 62.8538 Q1243.61 67.4314 1250.87 67.4314 Q1254.11 67.4314 1257.27 66.5807 Q1260.47 65.6895 1263.63 63.9476 L1263.63 70.8341 Q1260.51 72.2924 1257.14 73.0216 Q1253.82 73.7508 1250.06 73.7508 Q1239.81 73.7508 1233.77 67.3098 Q1227.74 60.8689 1227.74 49.9314 Q1227.74 38.832 1233.81 32.472 Q1239.93 26.1121 1250.54 26.1121 Q1253.98 26.1121 1257.27 26.8413 Q1260.55 27.5299 1263.63 28.9478 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1295.47 76.7889 Q1292.31 84.8907 1289.31 87.3618 Q1286.31 89.8329 1281.29 89.8329 L1275.33 89.8329 L1275.33 83.5945 L1279.71 83.5945 Q1282.79 83.5945 1284.49 82.1361 Q1286.19 80.6778 1288.26 75.2496 L1289.59 71.8468 L1271.24 27.2059 L1279.14 27.2059 L1293.32 62.6918 L1307.5 27.2059 L1315.4 27.2059 L1295.47 76.7889 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1378.35 34.1734 Q1377.09 33.4443 1375.59 33.1202 Q1374.13 32.7556 1372.35 32.7556 Q1366.03 32.7556 1362.63 36.8875 Q1359.27 40.9789 1359.27 48.6757 L1359.27 72.576 L1351.77 72.576 L1351.77 27.2059 L1359.27 27.2059 L1359.27 34.2544 Q1361.62 30.1225 1365.38 28.1376 Q1369.15 26.1121 1374.54 26.1121 Q1375.31 26.1121 1376.24 26.2337 Q1377.17 26.3147 1378.31 26.5172 L1378.35 34.1734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1423.15 48.0275 L1423.15 51.6733 L1388.88 51.6733 Q1389.37 59.3701 1393.5 63.421 Q1397.67 67.4314 1405.08 67.4314 Q1409.38 67.4314 1413.39 66.3781 Q1417.44 65.3249 1421.41 63.2184 L1421.41 70.267 Q1417.4 71.9684 1413.19 72.8596 Q1408.97 73.7508 1404.64 73.7508 Q1393.78 73.7508 1387.42 67.4314 Q1381.1 61.1119 1381.1 50.3365 Q1381.1 39.1965 1387.1 32.6746 Q1393.13 26.1121 1403.34 26.1121 Q1412.5 26.1121 1417.8 32.0264 Q1423.15 37.9003 1423.15 48.0275 M1415.7 45.84 Q1415.62 39.7232 1412.25 36.0774 Q1408.93 32.4315 1403.42 32.4315 Q1397.18 32.4315 1393.42 35.9558 Q1389.69 39.4801 1389.12 45.8805 L1415.7 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1464.31 28.5427 L1464.31 35.5912 Q1461.15 33.9709 1457.75 33.1607 Q1454.34 32.3505 1450.7 32.3505 Q1445.15 32.3505 1442.35 34.0519 Q1439.6 35.7533 1439.6 39.156 Q1439.6 41.7486 1441.58 43.2475 Q1443.57 44.7058 1449.56 46.0426 L1452.11 46.6097 Q1460.05 48.3111 1463.38 51.4303 Q1466.74 54.509 1466.74 60.0587 Q1466.74 66.3781 1461.72 70.0644 Q1456.73 73.7508 1447.98 73.7508 Q1444.34 73.7508 1440.37 73.0216 Q1436.44 72.3329 1432.06 70.9151 L1432.06 63.2184 Q1436.19 65.3654 1440.2 66.4591 Q1444.22 67.5124 1448.14 67.5124 Q1453.41 67.5124 1456.25 65.73 Q1459.08 63.9071 1459.08 60.6258 Q1459.08 57.5877 1457.02 55.9673 Q1454.99 54.3469 1448.06 52.8481 L1445.47 52.2405 Q1438.54 50.7821 1435.47 47.7845 Q1432.39 44.7463 1432.39 39.4801 Q1432.39 33.0797 1436.92 29.5959 Q1441.46 26.1121 1449.81 26.1121 Q1453.94 26.1121 1457.58 26.7198 Q1461.23 27.3274 1464.31 28.5427 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1477.84 54.671 L1477.84 27.2059 L1485.29 27.2059 L1485.29 54.3874 Q1485.29 60.8284 1487.8 64.0691 Q1490.31 67.2693 1495.34 67.2693 Q1501.37 67.2693 1504.86 63.421 Q1508.38 59.5726 1508.38 52.9291 L1508.38 27.2059 L1515.84 27.2059 L1515.84 72.576 L1508.38 72.576 L1508.38 65.6084 Q1505.67 69.7404 1502.06 71.7658 Q1498.5 73.7508 1493.76 73.7508 Q1485.94 73.7508 1481.89 68.8897 Q1477.84 64.0286 1477.84 54.671 M1496.59 26.1121 L1496.59 26.1121 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1531.19 9.54393 L1538.64 9.54393 L1538.64 72.576 L1531.19 72.576 L1531.19 9.54393 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1561.61 14.324 L1561.61 27.2059 L1576.96 27.2059 L1576.96 32.9987 L1561.61 32.9987 L1561.61 57.6282 Q1561.61 63.1779 1563.11 64.7578 Q1564.65 66.3376 1569.31 66.3376 L1576.96 66.3376 L1576.96 72.576 L1569.31 72.576 Q1560.68 72.576 1557.4 69.3758 Q1554.12 66.1351 1554.12 57.6282 L1554.12 32.9987 L1548.65 32.9987 L1548.65 27.2059 L1554.12 27.2059 L1554.12 14.324 L1561.61 14.324 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip350)\" d=\"M1615.69 28.5427 L1615.69 35.5912 Q1612.53 33.9709 1609.13 33.1607 Q1605.72 32.3505 1602.08 32.3505 Q1596.53 32.3505 1593.73 34.0519 Q1590.98 35.7533 1590.98 39.156 Q1590.98 41.7486 1592.96 43.2475 Q1594.95 44.7058 1600.94 46.0426 L1603.5 46.6097 Q1611.44 48.3111 1614.76 51.4303 Q1618.12 54.509 1618.12 60.0587 Q1618.12 66.3781 1613.1 70.0644 Q1608.11 73.7508 1599.36 73.7508 Q1595.72 73.7508 1591.75 73.0216 Q1587.82 72.3329 1583.44 70.9151 L1583.44 63.2184 Q1587.58 65.3654 1591.59 66.4591 Q1595.6 67.5124 1599.53 67.5124 Q1604.79 67.5124 1607.63 65.73 Q1610.46 63.9071 1610.46 60.6258 Q1610.46 57.5877 1608.4 55.9673 Q1606.37 54.3469 1599.45 52.8481 L1596.85 52.2405 Q1589.93 50.7821 1586.85 47.7845 Q1583.77 44.7463 1583.77 39.4801 Q1583.77 33.0797 1588.31 29.5959 Q1592.84 26.1121 1601.19 26.1121 Q1605.32 26.1121 1608.97 26.7198 Q1612.61 27.3274 1615.69 28.5427 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip352)\" d=\"M323.245 383.192 L323.245 1423.18 L869.355 1423.18 L869.355 383.192 L323.245 383.192 L323.245 383.192  Z\" fill=\"#add8e6\" fill-rule=\"evenodd\" fill-opacity=\"0.5\"/>\n",
       "<polyline clip-path=\"url(#clip352)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"323.245,383.192 323.245,1423.18 869.355,1423.18 869.355,383.192 323.245,383.192 \"/>\n",
       "<path clip-path=\"url(#clip352)\" d=\"M1005.88 190.65 L1005.88 1423.18 L1551.99 1423.18 L1551.99 190.65 L1005.88 190.65 L1005.88 190.65  Z\" fill=\"#008000\" fill-rule=\"evenodd\" fill-opacity=\"0.5\"/>\n",
       "<polyline clip-path=\"url(#clip352)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"1005.88,190.65 1005.88,1423.18 1551.99,1423.18 1551.99,190.65 1005.88,190.65 \"/>\n",
       "<path clip-path=\"url(#clip352)\" d=\"M1688.52 242.252 L1688.52 1423.18 L2234.63 1423.18 L2234.63 242.252 L1688.52 242.252 L1688.52 242.252  Z\" fill=\"#ff0000\" fill-rule=\"evenodd\" fill-opacity=\"0.5\"/>\n",
       "<polyline clip-path=\"url(#clip352)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"1688.52,242.252 1688.52,1423.18 2234.63,1423.18 2234.63,242.252 1688.52,242.252 \"/>\n",
       "<circle clip-path=\"url(#clip352)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"596.3\" cy=\"383.192\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip352)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1278.94\" cy=\"190.65\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip352)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1961.58\" cy=\"242.252\" r=\"2\"/>\n",
       "<polyline clip-path=\"url(#clip352)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"596.3,595.399 596.3,170.985 \"/>\n",
       "<polyline clip-path=\"url(#clip352)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"1278.94,257.828 1278.94,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip352)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"1961.58,313.84 1961.58,170.665 \"/>\n",
       "<line clip-path=\"url(#clip352)\" x1=\"612.3\" y1=\"595.399\" x2=\"580.3\" y2=\"595.399\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip352)\" x1=\"612.3\" y1=\"170.985\" x2=\"580.3\" y2=\"170.985\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip352)\" x1=\"1294.94\" y1=\"257.828\" x2=\"1262.94\" y2=\"257.828\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip352)\" x1=\"1294.94\" y1=\"123.472\" x2=\"1262.94\" y2=\"123.472\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip352)\" x1=\"1977.58\" y1=\"313.84\" x2=\"1945.58\" y2=\"313.84\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip352)\" x1=\"1977.58\" y1=\"170.665\" x2=\"1945.58\" y2=\"170.665\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "</svg>\n"
      ],
      "text/html": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip400\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip400)\" d=\"M0 1600 L2400 1600 L2400 0 L0 0  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip401\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip400)\" d=\"M205.121 1423.18 L2352.76 1423.18 L2352.76 123.472 L205.121 123.472  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip402\">\n",
       "    <rect x=\"205\" y=\"123\" width=\"2149\" height=\"1301\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"596.3,1423.18 596.3,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1278.94,1423.18 1278.94,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1961.58,1423.18 1961.58,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 2352.76,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"596.3,1423.18 596.3,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1278.94,1423.18 1278.94,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1961.58,1423.18 1961.58,1404.28 \"/>\n",
       "<path clip-path=\"url(#clip400)\" d=\"M519.981 1452.15 L519.981 1456.71 Q517.319 1455.44 514.958 1454.82 Q512.597 1454.19 510.398 1454.19 Q506.578 1454.19 504.495 1455.67 Q502.435 1457.15 502.435 1459.89 Q502.435 1462.18 503.801 1463.36 Q505.189 1464.52 509.032 1465.23 L511.856 1465.81 Q517.088 1466.81 519.564 1469.33 Q522.064 1471.83 522.064 1476.04 Q522.064 1481.07 518.685 1483.66 Q515.328 1486.25 508.824 1486.25 Q506.37 1486.25 503.592 1485.7 Q500.838 1485.14 497.875 1484.05 L497.875 1479.24 Q500.722 1480.83 503.453 1481.64 Q506.185 1482.45 508.824 1482.45 Q512.828 1482.45 515.004 1480.88 Q517.18 1479.31 517.18 1476.39 Q517.18 1473.84 515.606 1472.41 Q514.055 1470.97 510.49 1470.26 L507.643 1469.7 Q502.412 1468.66 500.074 1466.44 Q497.736 1464.21 497.736 1460.26 Q497.736 1455.67 500.953 1453.03 Q504.194 1450.39 509.865 1450.39 Q512.296 1450.39 514.819 1450.83 Q517.342 1451.27 519.981 1452.15 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M533.384 1452.29 L533.384 1459.65 L542.157 1459.65 L542.157 1462.96 L533.384 1462.96 L533.384 1477.04 Q533.384 1480.21 534.24 1481.11 Q535.12 1482.02 537.782 1482.02 L542.157 1482.02 L542.157 1485.58 L537.782 1485.58 Q532.851 1485.58 530.976 1483.75 Q529.101 1481.9 529.101 1477.04 L529.101 1462.96 L525.976 1462.96 L525.976 1459.65 L529.101 1459.65 L529.101 1452.29 L533.384 1452.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M559.541 1472.55 Q554.379 1472.55 552.388 1473.73 Q550.398 1474.91 550.398 1477.76 Q550.398 1480.02 551.879 1481.37 Q553.384 1482.69 555.953 1482.69 Q559.495 1482.69 561.624 1480.19 Q563.777 1477.66 563.777 1473.5 L563.777 1472.55 L559.541 1472.55 M568.036 1470.79 L568.036 1485.58 L563.777 1485.58 L563.777 1481.64 Q562.319 1484.01 560.143 1485.14 Q557.967 1486.25 554.819 1486.25 Q550.837 1486.25 548.476 1484.03 Q546.138 1481.78 546.138 1478.03 Q546.138 1473.66 549.055 1471.44 Q551.995 1469.21 557.805 1469.21 L563.777 1469.21 L563.777 1468.8 Q563.777 1465.86 561.833 1464.26 Q559.911 1462.64 556.416 1462.64 Q554.194 1462.64 552.087 1463.17 Q549.981 1463.7 548.036 1464.77 L548.036 1460.83 Q550.374 1459.93 552.573 1459.49 Q554.773 1459.03 556.856 1459.03 Q562.481 1459.03 565.259 1461.95 Q568.036 1464.86 568.036 1470.79 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M595.467 1460.65 L595.467 1464.63 Q593.661 1463.64 591.832 1463.15 Q590.027 1462.64 588.175 1462.64 Q584.032 1462.64 581.74 1465.28 Q579.448 1467.89 579.448 1472.64 Q579.448 1477.39 581.74 1480.02 Q584.032 1482.64 588.175 1482.64 Q590.027 1482.64 591.832 1482.15 Q593.661 1481.64 595.467 1480.65 L595.467 1484.58 Q593.684 1485.42 591.763 1485.83 Q589.865 1486.25 587.712 1486.25 Q581.856 1486.25 578.407 1482.57 Q574.958 1478.89 574.958 1472.64 Q574.958 1466.3 578.43 1462.66 Q581.925 1459.03 587.99 1459.03 Q589.958 1459.03 591.832 1459.45 Q593.707 1459.84 595.467 1460.65 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M602.712 1449.56 L606.994 1449.56 L606.994 1470.83 L619.703 1459.65 L625.143 1459.65 L611.393 1471.78 L625.721 1485.58 L620.166 1485.58 L606.994 1472.92 L606.994 1485.58 L602.712 1485.58 L602.712 1449.56 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M630.328 1459.65 L634.587 1459.65 L634.587 1485.58 L630.328 1485.58 L630.328 1459.65 M630.328 1449.56 L634.587 1449.56 L634.587 1454.96 L630.328 1454.96 L630.328 1449.56 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M665.05 1469.93 L665.05 1485.58 L660.79 1485.58 L660.79 1470.07 Q660.79 1466.39 659.355 1464.56 Q657.92 1462.73 655.05 1462.73 Q651.601 1462.73 649.61 1464.93 Q647.619 1467.13 647.619 1470.93 L647.619 1485.58 L643.337 1485.58 L643.337 1459.65 L647.619 1459.65 L647.619 1463.68 Q649.147 1461.34 651.207 1460.19 Q653.291 1459.03 655.999 1459.03 Q660.466 1459.03 662.758 1461.81 Q665.05 1464.56 665.05 1469.93 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M690.605 1472.32 Q690.605 1467.69 688.684 1465.14 Q686.786 1462.59 683.337 1462.59 Q679.911 1462.59 677.989 1465.14 Q676.091 1467.69 676.091 1472.32 Q676.091 1476.92 677.989 1479.47 Q679.911 1482.02 683.337 1482.02 Q686.786 1482.02 688.684 1479.47 Q690.605 1476.92 690.605 1472.32 M694.864 1482.36 Q694.864 1488.98 691.925 1492.2 Q688.985 1495.44 682.92 1495.44 Q680.675 1495.44 678.684 1495.09 Q676.693 1494.77 674.818 1494.08 L674.818 1489.93 Q676.693 1490.95 678.522 1491.44 Q680.351 1491.92 682.249 1491.92 Q686.438 1491.92 688.522 1489.72 Q690.605 1487.55 690.605 1483.13 L690.605 1481.02 Q689.286 1483.31 687.226 1484.45 Q685.165 1485.58 682.295 1485.58 Q677.526 1485.58 674.61 1481.95 Q671.693 1478.31 671.693 1472.32 Q671.693 1466.3 674.61 1462.66 Q677.526 1459.03 682.295 1459.03 Q685.165 1459.03 687.226 1460.16 Q689.286 1461.3 690.605 1463.59 L690.605 1459.65 L694.864 1459.65 L694.864 1482.36 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1226.69 1451.02 L1231.37 1451.02 L1231.37 1465.19 L1248.36 1465.19 L1248.36 1451.02 L1253.04 1451.02 L1253.04 1485.58 L1248.36 1485.58 L1248.36 1469.12 L1231.37 1469.12 L1231.37 1485.58 L1226.69 1485.58 L1226.69 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1273.94 1472.55 Q1268.78 1472.55 1266.79 1473.73 Q1264.79 1474.91 1264.79 1477.76 Q1264.79 1480.02 1266.28 1481.37 Q1267.78 1482.69 1270.35 1482.69 Q1273.89 1482.69 1276.02 1480.19 Q1278.17 1477.66 1278.17 1473.5 L1278.17 1472.55 L1273.94 1472.55 M1282.43 1470.79 L1282.43 1485.58 L1278.17 1485.58 L1278.17 1481.64 Q1276.72 1484.01 1274.54 1485.14 Q1272.36 1486.25 1269.22 1486.25 Q1265.23 1486.25 1262.87 1484.03 Q1260.54 1481.78 1260.54 1478.03 Q1260.54 1473.66 1263.45 1471.44 Q1266.39 1469.21 1272.2 1469.21 L1278.17 1469.21 L1278.17 1468.8 Q1278.17 1465.86 1276.23 1464.26 Q1274.31 1462.64 1270.81 1462.64 Q1268.59 1462.64 1266.48 1463.17 Q1264.38 1463.7 1262.43 1464.77 L1262.43 1460.83 Q1264.77 1459.93 1266.97 1459.49 Q1269.17 1459.03 1271.25 1459.03 Q1276.88 1459.03 1279.66 1461.95 Q1282.43 1464.86 1282.43 1470.79 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1306.23 1463.64 Q1305.51 1463.22 1304.66 1463.03 Q1303.82 1462.83 1302.8 1462.83 Q1299.19 1462.83 1297.25 1465.19 Q1295.33 1467.52 1295.33 1471.92 L1295.33 1485.58 L1291.04 1485.58 L1291.04 1459.65 L1295.33 1459.65 L1295.33 1463.68 Q1296.67 1461.32 1298.82 1460.19 Q1300.98 1459.03 1304.05 1459.03 Q1304.49 1459.03 1305.03 1459.1 Q1305.56 1459.14 1306.21 1459.26 L1306.23 1463.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1326.92 1463.59 L1326.92 1449.56 L1331.18 1449.56 L1331.18 1485.58 L1326.92 1485.58 L1326.92 1481.69 Q1325.58 1484.01 1323.52 1485.14 Q1321.48 1486.25 1318.61 1486.25 Q1313.92 1486.25 1310.95 1482.5 Q1308.01 1478.75 1308.01 1472.64 Q1308.01 1466.53 1310.95 1462.78 Q1313.92 1459.03 1318.61 1459.03 Q1321.48 1459.03 1323.52 1460.16 Q1325.58 1461.27 1326.92 1463.59 M1312.41 1472.64 Q1312.41 1477.34 1314.33 1480.02 Q1316.28 1482.69 1319.66 1482.69 Q1323.04 1482.69 1324.98 1480.02 Q1326.92 1477.34 1326.92 1472.64 Q1326.92 1467.94 1324.98 1465.28 Q1323.04 1462.59 1319.66 1462.59 Q1316.28 1462.59 1314.33 1465.28 Q1312.41 1467.94 1312.41 1472.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1939.18 1452.15 L1939.18 1456.71 Q1936.52 1455.44 1934.16 1454.82 Q1931.8 1454.19 1929.6 1454.19 Q1925.78 1454.19 1923.7 1455.67 Q1921.63 1457.15 1921.63 1459.89 Q1921.63 1462.18 1923 1463.36 Q1924.39 1464.52 1928.23 1465.23 L1931.06 1465.81 Q1936.29 1466.81 1938.76 1469.33 Q1941.26 1471.83 1941.26 1476.04 Q1941.26 1481.07 1937.88 1483.66 Q1934.53 1486.25 1928.02 1486.25 Q1925.57 1486.25 1922.79 1485.7 Q1920.04 1485.14 1917.07 1484.05 L1917.07 1479.24 Q1919.92 1480.83 1922.65 1481.64 Q1925.38 1482.45 1928.02 1482.45 Q1932.03 1482.45 1934.2 1480.88 Q1936.38 1479.31 1936.38 1476.39 Q1936.38 1473.84 1934.81 1472.41 Q1933.26 1470.97 1929.69 1470.26 L1926.84 1469.7 Q1921.61 1468.66 1919.27 1466.44 Q1916.94 1464.21 1916.94 1460.26 Q1916.94 1455.67 1920.15 1453.03 Q1923.39 1450.39 1929.07 1450.39 Q1931.5 1450.39 1934.02 1450.83 Q1936.54 1451.27 1939.18 1452.15 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1958.42 1462.64 Q1954.99 1462.64 1953 1465.33 Q1951.01 1467.99 1951.01 1472.64 Q1951.01 1477.29 1952.98 1479.98 Q1954.97 1482.64 1958.42 1482.64 Q1961.82 1482.64 1963.81 1479.95 Q1965.8 1477.27 1965.8 1472.64 Q1965.8 1468.03 1963.81 1465.35 Q1961.82 1462.64 1958.42 1462.64 M1958.42 1459.03 Q1963.97 1459.03 1967.14 1462.64 Q1970.32 1466.25 1970.32 1472.64 Q1970.32 1479.01 1967.14 1482.64 Q1963.97 1486.25 1958.42 1486.25 Q1952.84 1486.25 1949.67 1482.64 Q1946.52 1479.01 1946.52 1472.64 Q1946.52 1466.25 1949.67 1462.64 Q1952.84 1459.03 1958.42 1459.03 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1990.5 1449.56 L1990.5 1453.1 L1986.43 1453.1 Q1984.13 1453.1 1983.23 1454.03 Q1982.35 1454.96 1982.35 1457.36 L1982.35 1459.65 L1989.37 1459.65 L1989.37 1462.96 L1982.35 1462.96 L1982.35 1485.58 L1978.07 1485.58 L1978.07 1462.96 L1974 1462.96 L1974 1459.65 L1978.07 1459.65 L1978.07 1457.85 Q1978.07 1453.52 1980.08 1451.55 Q1982.1 1449.56 1986.47 1449.56 L1990.5 1449.56 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1997.44 1452.29 L1997.44 1459.65 L2006.22 1459.65 L2006.22 1462.96 L1997.44 1462.96 L1997.44 1477.04 Q1997.44 1480.21 1998.3 1481.11 Q1999.18 1482.02 2001.84 1482.02 L2006.22 1482.02 L2006.22 1485.58 L2001.84 1485.58 Q1996.91 1485.58 1995.04 1483.75 Q1993.16 1481.9 1993.16 1477.04 L1993.16 1462.96 L1990.04 1462.96 L1990.04 1459.65 L1993.16 1459.65 L1993.16 1452.29 L1997.44 1452.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1109.5 1520.52 L1139.55 1520.52 L1139.55 1525.93 L1115.93 1525.93 L1115.93 1540 L1138.56 1540 L1138.56 1545.41 L1115.93 1545.41 L1115.93 1562.63 L1140.12 1562.63 L1140.12 1568.04 L1109.5 1568.04 L1109.5 1520.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1173.16 1533.45 L1173.16 1538.98 Q1170.67 1537.71 1168 1537.07 Q1165.33 1536.44 1162.46 1536.44 Q1158.1 1536.44 1155.91 1537.77 Q1153.74 1539.11 1153.74 1541.79 Q1153.74 1543.82 1155.3 1545 Q1156.86 1546.15 1161.57 1547.2 L1163.58 1547.64 Q1169.81 1548.98 1172.42 1551.43 Q1175.07 1553.85 1175.07 1558.21 Q1175.07 1563.17 1171.12 1566.07 Q1167.2 1568.97 1160.33 1568.97 Q1157.46 1568.97 1154.35 1568.39 Q1151.26 1567.85 1147.82 1566.74 L1147.82 1560.69 Q1151.07 1562.38 1154.22 1563.24 Q1157.37 1564.07 1160.46 1564.07 Q1164.59 1564.07 1166.82 1562.66 Q1169.05 1561.23 1169.05 1558.65 Q1169.05 1556.27 1167.43 1554.99 Q1165.84 1553.72 1160.39 1552.54 L1158.36 1552.07 Q1152.91 1550.92 1150.49 1548.56 Q1148.08 1546.18 1148.08 1542.04 Q1148.08 1537.01 1151.64 1534.27 Q1155.21 1531.54 1161.76 1531.54 Q1165.01 1531.54 1167.87 1532.01 Q1170.74 1532.49 1173.16 1533.45 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1190.18 1522.27 L1190.18 1532.4 L1202.25 1532.4 L1202.25 1536.95 L1190.18 1536.95 L1190.18 1556.3 Q1190.18 1560.66 1191.36 1561.9 Q1192.57 1563.14 1196.23 1563.14 L1202.25 1563.14 L1202.25 1568.04 L1196.23 1568.04 Q1189.45 1568.04 1186.87 1565.53 Q1184.3 1562.98 1184.3 1556.3 L1184.3 1536.95 L1180 1536.95 L1180 1532.4 L1184.3 1532.4 L1184.3 1522.27 L1190.18 1522.27 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1209.95 1532.4 L1215.81 1532.4 L1215.81 1568.04 L1209.95 1568.04 L1209.95 1532.4 M1209.95 1518.52 L1215.81 1518.52 L1215.81 1525.93 L1209.95 1525.93 L1209.95 1518.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1255.82 1539.24 Q1258.01 1535.29 1261.07 1533.41 Q1264.12 1531.54 1268.26 1531.54 Q1273.83 1531.54 1276.85 1535.45 Q1279.88 1539.33 1279.88 1546.53 L1279.88 1568.04 L1273.99 1568.04 L1273.99 1546.72 Q1273.99 1541.59 1272.17 1539.11 Q1270.36 1536.63 1266.64 1536.63 Q1262.09 1536.63 1259.44 1539.65 Q1256.8 1542.68 1256.8 1547.9 L1256.8 1568.04 L1250.91 1568.04 L1250.91 1546.72 Q1250.91 1541.56 1249.1 1539.11 Q1247.28 1536.63 1243.5 1536.63 Q1239.01 1536.63 1236.37 1539.68 Q1233.73 1542.71 1233.73 1547.9 L1233.73 1568.04 L1227.84 1568.04 L1227.84 1532.4 L1233.73 1532.4 L1233.73 1537.93 Q1235.73 1534.66 1238.53 1533.1 Q1241.33 1531.54 1245.18 1531.54 Q1249.07 1531.54 1251.77 1533.51 Q1254.51 1535.48 1255.82 1539.24 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1307.76 1550.12 Q1300.66 1550.12 1297.92 1551.75 Q1295.19 1553.37 1295.19 1557.29 Q1295.19 1560.4 1297.22 1562.25 Q1299.29 1564.07 1302.83 1564.07 Q1307.7 1564.07 1310.62 1560.63 Q1313.58 1557.16 1313.58 1551.43 L1313.58 1550.12 L1307.76 1550.12 M1319.44 1547.71 L1319.44 1568.04 L1313.58 1568.04 L1313.58 1562.63 Q1311.58 1565.88 1308.59 1567.44 Q1305.59 1568.97 1301.27 1568.97 Q1295.79 1568.97 1292.55 1565.91 Q1289.33 1562.82 1289.33 1557.67 Q1289.33 1551.65 1293.34 1548.6 Q1297.38 1545.54 1305.37 1545.54 L1313.58 1545.54 L1313.58 1544.97 Q1313.58 1540.93 1310.91 1538.73 Q1308.27 1536.5 1303.46 1536.5 Q1300.41 1536.5 1297.51 1537.23 Q1294.61 1537.97 1291.94 1539.43 L1291.94 1534.02 Q1295.16 1532.78 1298.18 1532.17 Q1301.2 1531.54 1304.07 1531.54 Q1311.8 1531.54 1315.62 1535.55 Q1319.44 1539.56 1319.44 1547.71 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1337.3 1522.27 L1337.3 1532.4 L1349.36 1532.4 L1349.36 1536.95 L1337.3 1536.95 L1337.3 1556.3 Q1337.3 1560.66 1338.47 1561.9 Q1339.68 1563.14 1343.34 1563.14 L1349.36 1563.14 L1349.36 1568.04 L1343.34 1568.04 Q1336.56 1568.04 1333.99 1565.53 Q1331.41 1562.98 1331.41 1556.3 L1331.41 1536.95 L1327.11 1536.95 L1327.11 1532.4 L1331.41 1532.4 L1331.41 1522.27 L1337.3 1522.27 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1370.88 1536.5 Q1366.16 1536.5 1363.43 1540.19 Q1360.69 1543.85 1360.69 1550.25 Q1360.69 1556.65 1363.4 1560.34 Q1366.13 1564 1370.88 1564 Q1375.55 1564 1378.29 1560.31 Q1381.03 1556.62 1381.03 1550.25 Q1381.03 1543.92 1378.29 1540.23 Q1375.55 1536.5 1370.88 1536.5 M1370.88 1531.54 Q1378.51 1531.54 1382.87 1536.5 Q1387.24 1541.47 1387.24 1550.25 Q1387.24 1559 1382.87 1564 Q1378.51 1568.97 1370.88 1568.97 Q1363.2 1568.97 1358.84 1564 Q1354.52 1559 1354.52 1550.25 Q1354.52 1541.47 1358.84 1536.5 Q1363.2 1531.54 1370.88 1531.54 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1417.6 1537.87 Q1416.61 1537.3 1415.44 1537.04 Q1414.29 1536.76 1412.89 1536.76 Q1407.92 1536.76 1405.25 1540 Q1402.61 1543.22 1402.61 1549.27 L1402.61 1568.04 L1396.72 1568.04 L1396.72 1532.4 L1402.61 1532.4 L1402.61 1537.93 Q1404.45 1534.69 1407.41 1533.13 Q1410.37 1531.54 1414.61 1531.54 Q1415.21 1531.54 1415.94 1531.63 Q1416.68 1531.7 1417.57 1531.85 L1417.6 1537.87 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1446.47 1533.45 L1446.47 1538.98 Q1443.99 1537.71 1441.31 1537.07 Q1438.64 1536.44 1435.77 1536.44 Q1431.41 1536.44 1429.22 1537.77 Q1427.05 1539.11 1427.05 1541.79 Q1427.05 1543.82 1428.61 1545 Q1430.17 1546.15 1434.88 1547.2 L1436.89 1547.64 Q1443.13 1548.98 1445.74 1551.43 Q1448.38 1553.85 1448.38 1558.21 Q1448.38 1563.17 1444.43 1566.07 Q1440.52 1568.97 1433.64 1568.97 Q1430.78 1568.97 1427.66 1568.39 Q1424.57 1567.85 1421.13 1566.74 L1421.13 1560.69 Q1424.38 1562.38 1427.53 1563.24 Q1430.68 1564.07 1433.77 1564.07 Q1437.91 1564.07 1440.13 1562.66 Q1442.36 1561.23 1442.36 1558.65 Q1442.36 1556.27 1440.74 1554.99 Q1439.15 1553.72 1433.7 1552.54 L1431.67 1552.07 Q1426.22 1550.92 1423.81 1548.56 Q1421.39 1546.18 1421.39 1542.04 Q1421.39 1537.01 1424.95 1534.27 Q1428.52 1531.54 1435.07 1531.54 Q1438.32 1531.54 1441.18 1532.01 Q1444.05 1532.49 1446.47 1533.45 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,1423.18 2352.76,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,1159.13 2352.76,1159.13 \"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,895.085 2352.76,895.085 \"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,631.037 2352.76,631.037 \"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,366.989 2352.76,366.989 \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 205.121,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 224.019,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1159.13 224.019,1159.13 \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,895.085 224.019,895.085 \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,631.037 224.019,631.037 \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,366.989 224.019,366.989 \"/>\n",
       "<path clip-path=\"url(#clip400)\" d=\"M157.177 1408.98 Q153.566 1408.98 151.737 1412.54 Q149.931 1416.08 149.931 1423.21 Q149.931 1430.32 151.737 1433.89 Q153.566 1437.43 157.177 1437.43 Q160.811 1437.43 162.616 1433.89 Q164.445 1430.32 164.445 1423.21 Q164.445 1416.08 162.616 1412.54 Q160.811 1408.98 157.177 1408.98 M157.177 1405.27 Q162.987 1405.27 166.042 1409.88 Q169.121 1414.46 169.121 1423.21 Q169.121 1431.94 166.042 1436.55 Q162.987 1441.13 157.177 1441.13 Q151.366 1441.13 148.288 1436.55 Q145.232 1431.94 145.232 1423.21 Q145.232 1414.46 148.288 1409.88 Q151.366 1405.27 157.177 1405.27 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M121.043 1172.48 L137.362 1172.48 L137.362 1176.41 L115.418 1176.41 L115.418 1172.48 Q118.08 1169.72 122.663 1165.09 Q127.269 1160.44 128.45 1159.1 Q130.695 1156.57 131.575 1154.84 Q132.478 1153.08 132.478 1151.39 Q132.478 1148.63 130.533 1146.9 Q128.612 1145.16 125.51 1145.16 Q123.311 1145.16 120.857 1145.93 Q118.427 1146.69 115.649 1148.24 L115.649 1143.52 Q118.473 1142.38 120.927 1141.81 Q123.38 1141.23 125.418 1141.23 Q130.788 1141.23 133.982 1143.91 Q137.177 1146.6 137.177 1151.09 Q137.177 1153.22 136.367 1155.14 Q135.579 1157.04 133.473 1159.63 Q132.894 1160.3 129.792 1163.52 Q126.691 1166.71 121.043 1172.48 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M157.177 1144.93 Q153.566 1144.93 151.737 1148.5 Q149.931 1152.04 149.931 1159.17 Q149.931 1166.27 151.737 1169.84 Q153.566 1173.38 157.177 1173.38 Q160.811 1173.38 162.616 1169.84 Q164.445 1166.27 164.445 1159.17 Q164.445 1152.04 162.616 1148.5 Q160.811 1144.93 157.177 1144.93 M157.177 1141.23 Q162.987 1141.23 166.042 1145.83 Q169.121 1150.42 169.121 1159.17 Q169.121 1167.89 166.042 1172.5 Q162.987 1177.08 157.177 1177.08 Q151.366 1177.08 148.288 1172.5 Q145.232 1167.89 145.232 1159.17 Q145.232 1150.42 148.288 1145.83 Q151.366 1141.23 157.177 1141.23 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M129.862 881.879 L118.056 900.328 L129.862 900.328 L129.862 881.879 M128.635 877.805 L134.515 877.805 L134.515 900.328 L139.445 900.328 L139.445 904.216 L134.515 904.216 L134.515 912.365 L129.862 912.365 L129.862 904.216 L114.26 904.216 L114.26 899.703 L128.635 877.805 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M157.177 880.883 Q153.566 880.883 151.737 884.448 Q149.931 887.99 149.931 895.119 Q149.931 902.226 151.737 905.79 Q153.566 909.332 157.177 909.332 Q160.811 909.332 162.616 905.79 Q164.445 902.226 164.445 895.119 Q164.445 887.99 162.616 884.448 Q160.811 880.883 157.177 880.883 M157.177 877.18 Q162.987 877.18 166.042 881.786 Q169.121 886.369 169.121 895.119 Q169.121 903.846 166.042 908.453 Q162.987 913.036 157.177 913.036 Q151.366 913.036 148.288 908.453 Q145.232 903.846 145.232 895.119 Q145.232 886.369 148.288 881.786 Q151.366 877.18 157.177 877.18 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M127.593 629.173 Q124.445 629.173 122.593 631.326 Q120.765 633.479 120.765 637.229 Q120.765 640.956 122.593 643.132 Q124.445 645.284 127.593 645.284 Q130.742 645.284 132.57 643.132 Q134.422 640.956 134.422 637.229 Q134.422 633.479 132.57 631.326 Q130.742 629.173 127.593 629.173 M136.876 614.521 L136.876 618.78 Q135.117 617.947 133.311 617.507 Q131.529 617.067 129.769 617.067 Q125.14 617.067 122.686 620.192 Q120.255 623.317 119.908 629.636 Q121.274 627.623 123.334 626.558 Q125.394 625.47 127.871 625.47 Q133.08 625.47 136.089 628.641 Q139.121 631.789 139.121 637.229 Q139.121 642.553 135.973 645.771 Q132.825 648.988 127.593 648.988 Q121.598 648.988 118.427 644.405 Q115.256 639.798 115.256 631.072 Q115.256 622.877 119.144 618.016 Q123.033 613.132 129.584 613.132 Q131.343 613.132 133.126 613.479 Q134.931 613.826 136.876 614.521 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M157.177 616.836 Q153.566 616.836 151.737 620.4 Q149.931 623.942 149.931 631.072 Q149.931 638.178 151.737 641.743 Q153.566 645.284 157.177 645.284 Q160.811 645.284 162.616 641.743 Q164.445 638.178 164.445 631.072 Q164.445 623.942 162.616 620.4 Q160.811 616.836 157.177 616.836 M157.177 613.132 Q162.987 613.132 166.042 617.738 Q169.121 622.322 169.121 631.072 Q169.121 639.798 166.042 644.405 Q162.987 648.988 157.177 648.988 Q151.366 648.988 148.288 644.405 Q145.232 639.798 145.232 631.072 Q145.232 622.322 148.288 617.738 Q151.366 613.132 157.177 613.132 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M127.015 367.857 Q123.681 367.857 121.76 369.64 Q119.862 371.422 119.862 374.547 Q119.862 377.672 121.76 379.454 Q123.681 381.237 127.015 381.237 Q130.348 381.237 132.269 379.454 Q134.191 377.649 134.191 374.547 Q134.191 371.422 132.269 369.64 Q130.371 367.857 127.015 367.857 M122.339 365.867 Q119.33 365.126 117.64 363.066 Q115.973 361.005 115.973 358.043 Q115.973 353.899 118.913 351.492 Q121.876 349.084 127.015 349.084 Q132.177 349.084 135.117 351.492 Q138.056 353.899 138.056 358.043 Q138.056 361.005 136.367 363.066 Q134.7 365.126 131.714 365.867 Q135.093 366.654 136.968 368.945 Q138.867 371.237 138.867 374.547 Q138.867 379.57 135.788 382.255 Q132.732 384.941 127.015 384.941 Q121.297 384.941 118.218 382.255 Q115.163 379.57 115.163 374.547 Q115.163 371.237 117.061 368.945 Q118.959 366.654 122.339 365.867 M120.626 358.482 Q120.626 361.167 122.293 362.672 Q123.982 364.177 127.015 364.177 Q130.024 364.177 131.714 362.672 Q133.427 361.167 133.427 358.482 Q133.427 355.797 131.714 354.293 Q130.024 352.788 127.015 352.788 Q123.982 352.788 122.293 354.293 Q120.626 355.797 120.626 358.482 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M157.177 352.788 Q153.566 352.788 151.737 356.353 Q149.931 359.894 149.931 367.024 Q149.931 374.13 151.737 377.695 Q153.566 381.237 157.177 381.237 Q160.811 381.237 162.616 377.695 Q164.445 374.13 164.445 367.024 Q164.445 359.894 162.616 356.353 Q160.811 352.788 157.177 352.788 M157.177 349.084 Q162.987 349.084 166.042 353.691 Q169.121 358.274 169.121 367.024 Q169.121 375.751 166.042 380.357 Q162.987 384.941 157.177 384.941 Q151.366 384.941 148.288 380.357 Q145.232 375.751 145.232 367.024 Q145.232 358.274 148.288 353.691 Q151.366 349.084 157.177 349.084 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M21.7677 1011.01 L39.6235 1011.01 L39.6235 1002.92 Q39.6235 998.433 37.3 995.982 Q34.9765 993.532 30.6797 993.532 Q26.4147 993.532 24.0912 995.982 Q21.7677 998.433 21.7677 1002.92 L21.7677 1011.01 M16.4842 1017.43 L16.4842 1002.92 Q16.4842 994.932 20.1126 990.858 Q23.7092 986.752 30.6797 986.752 Q37.7138 986.752 41.3104 990.858 Q44.907 994.932 44.907 1002.92 L44.907 1011.01 L64.0042 1011.01 L64.0042 1017.43 L16.4842 1017.43 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M44.7161 950.213 L47.5806 950.213 L47.5806 977.14 Q53.6281 976.758 56.8109 973.512 Q59.9619 970.233 59.9619 964.409 Q59.9619 961.035 59.1344 957.884 Q58.3069 954.701 56.6518 951.582 L62.1899 951.582 Q63.5267 954.733 64.227 958.043 Q64.9272 961.353 64.9272 964.759 Q64.9272 973.289 59.9619 978.286 Q54.9967 983.251 46.5303 983.251 Q37.7774 983.251 32.6531 978.54 Q27.4968 973.798 27.4968 965.777 Q27.4968 958.584 32.1438 954.414 Q36.7589 950.213 44.7161 950.213 M42.9973 956.07 Q38.1912 956.133 35.3266 958.775 Q32.4621 961.385 32.4621 965.714 Q32.4621 970.615 35.2312 973.575 Q38.0002 976.503 43.0292 976.949 L42.9973 956.07 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M33.8307 919.944 Q33.2578 920.931 33.0032 922.108 Q32.7167 923.254 32.7167 924.655 Q32.7167 929.62 35.9632 932.294 Q39.1779 934.935 45.2253 934.935 L64.0042 934.935 L64.0042 940.824 L28.3562 940.824 L28.3562 934.935 L33.8944 934.935 Q30.6479 933.089 29.0883 930.129 Q27.4968 927.169 27.4968 922.936 Q27.4968 922.331 27.5923 921.599 Q27.656 920.867 27.8151 919.976 L33.8307 919.944 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M29.7248 889.58 L35.1993 889.58 Q33.8307 892.062 33.1623 894.577 Q32.4621 897.059 32.4621 899.606 Q32.4621 905.303 36.0905 908.454 Q39.6872 911.605 46.212 911.605 Q52.7369 911.605 56.3653 908.454 Q59.9619 905.303 59.9619 899.606 Q59.9619 897.059 59.2935 894.577 Q58.5933 892.062 57.2247 889.58 L62.6355 889.58 Q63.7814 892.03 64.3543 894.672 Q64.9272 897.282 64.9272 900.242 Q64.9272 908.295 59.8664 913.037 Q54.8057 917.78 46.212 917.78 Q37.491 917.78 32.4939 913.006 Q27.4968 908.199 27.4968 899.86 Q27.4968 897.155 28.0697 894.577 Q28.6108 891.999 29.7248 889.58 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M44.7161 848.903 L47.5806 848.903 L47.5806 875.83 Q53.6281 875.448 56.8109 872.201 Q59.9619 868.923 59.9619 863.098 Q59.9619 859.725 59.1344 856.574 Q58.3069 853.391 56.6518 850.271 L62.1899 850.271 Q63.5267 853.422 64.227 856.733 Q64.9272 860.043 64.9272 863.448 Q64.9272 871.979 59.9619 876.976 Q54.9967 881.941 46.5303 881.941 Q37.7774 881.941 32.6531 877.23 Q27.4968 872.488 27.4968 864.467 Q27.4968 857.274 32.1438 853.104 Q36.7589 848.903 44.7161 848.903 M42.9973 854.759 Q38.1912 854.823 35.3266 857.465 Q32.4621 860.075 32.4621 864.403 Q32.4621 869.305 35.2312 872.265 Q38.0002 875.193 43.0292 875.639 L42.9973 854.759 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M42.4881 809.658 L64.0042 809.658 L64.0042 815.515 L42.679 815.515 Q37.6183 815.515 35.1038 817.488 Q32.5894 819.461 32.5894 823.408 Q32.5894 828.151 35.6131 830.888 Q38.6368 833.625 43.8567 833.625 L64.0042 833.625 L64.0042 839.513 L28.3562 839.513 L28.3562 833.625 L33.8944 833.625 Q30.6797 831.524 29.0883 828.692 Q27.4968 825.827 27.4968 822.103 Q27.4968 815.96 31.3163 812.809 Q35.1038 809.658 42.4881 809.658 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M18.2347 792.184 L28.3562 792.184 L28.3562 780.121 L32.9077 780.121 L32.9077 792.184 L52.2594 792.184 Q56.6199 792.184 57.8613 791.007 Q59.1026 789.797 59.1026 786.137 L59.1026 780.121 L64.0042 780.121 L64.0042 786.137 Q64.0042 792.916 61.4897 795.495 Q58.9434 798.073 52.2594 798.073 L32.9077 798.073 L32.9077 802.37 L28.3562 802.37 L28.3562 798.073 L18.2347 798.073 L18.2347 792.184 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M46.0847 756.218 Q46.0847 763.316 47.7079 766.053 Q49.3312 768.79 53.2461 768.79 Q56.3653 768.79 58.2114 766.753 Q60.0256 764.685 60.0256 761.152 Q60.0256 756.282 56.5881 753.354 Q53.1188 750.394 47.3897 750.394 L46.0847 750.394 L46.0847 756.218 M43.6657 744.537 L64.0042 744.537 L64.0042 750.394 L58.5933 750.394 Q61.8398 752.399 63.3994 755.391 Q64.9272 758.382 64.9272 762.711 Q64.9272 768.186 61.8716 771.432 Q58.7843 774.647 53.6281 774.647 Q47.6125 774.647 44.5569 770.636 Q41.5014 766.594 41.5014 758.605 L41.5014 750.394 L40.9285 750.394 Q36.8862 750.394 34.6901 753.067 Q32.4621 755.709 32.4621 760.515 Q32.4621 763.571 33.1941 766.467 Q33.9262 769.363 35.3903 772.037 L29.9795 772.037 Q28.7381 768.822 28.1334 765.799 Q27.4968 762.775 27.4968 759.91 Q27.4968 752.176 31.5072 748.356 Q35.5176 744.537 43.6657 744.537 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M45.7664 709.016 Q39.4007 709.016 35.8996 711.658 Q32.3984 714.268 32.3984 719.011 Q32.3984 723.721 35.8996 726.363 Q39.4007 728.973 45.7664 728.973 Q52.1003 728.973 55.6014 726.363 Q59.1026 723.721 59.1026 719.011 Q59.1026 714.268 55.6014 711.658 Q52.1003 709.016 45.7664 709.016 M59.58 703.16 Q68.683 703.16 73.1071 707.202 Q77.5631 711.244 77.5631 719.584 Q77.5631 722.671 77.0857 725.408 Q76.6401 728.145 75.6852 730.723 L69.9879 730.723 Q71.3884 728.145 72.0568 725.631 Q72.7252 723.116 72.7252 720.507 Q72.7252 714.746 69.7015 711.881 Q66.7096 709.016 60.6303 709.016 L57.7339 709.016 Q60.885 710.831 62.4446 713.663 Q64.0042 716.496 64.0042 720.443 Q64.0042 727 59.0071 731.01 Q54.01 735.02 45.7664 735.02 Q37.491 735.02 32.4939 731.01 Q27.4968 727 27.4968 720.443 Q27.4968 716.496 29.0564 713.663 Q30.616 710.831 33.7671 709.016 L28.3562 709.016 L28.3562 703.16 L59.58 703.16 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M44.7161 660.605 L47.5806 660.605 L47.5806 687.532 Q53.6281 687.15 56.8109 683.904 Q59.9619 680.625 59.9619 674.801 Q59.9619 671.427 59.1344 668.276 Q58.3069 665.093 56.6518 661.974 L62.1899 661.974 Q63.5267 665.125 64.227 668.435 Q64.9272 671.745 64.9272 675.151 Q64.9272 683.681 59.9619 688.678 Q54.9967 693.643 46.5303 693.643 Q37.7774 693.643 32.6531 688.933 Q27.4968 684.19 27.4968 676.169 Q27.4968 668.976 32.1438 664.807 Q36.7589 660.605 44.7161 660.605 M42.9973 666.462 Q38.1912 666.525 35.3266 669.167 Q32.4621 671.777 32.4621 676.106 Q32.4621 681.007 35.2312 683.967 Q38.0002 686.896 43.0292 687.341 L42.9973 666.462 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M14.5426 616.204 Q21.8632 620.469 29.0246 622.538 Q36.186 624.607 43.5384 624.607 Q50.8908 624.607 58.1159 622.538 Q65.3091 620.438 72.5979 616.204 L72.5979 621.297 Q65.1182 626.071 57.8931 628.458 Q50.668 630.814 43.5384 630.814 Q36.4406 630.814 29.2474 628.458 Q22.0542 626.103 14.5426 621.297 L14.5426 616.204 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M43.0928 563.592 Q43.0928 566.361 45.4481 567.952 Q47.8034 569.512 52.0048 569.512 Q56.1425 569.512 58.5296 567.952 Q60.885 566.361 60.885 563.592 Q60.885 560.886 58.5296 559.327 Q56.1425 557.735 52.0048 557.735 Q47.8353 557.735 45.48 559.327 Q43.0928 560.886 43.0928 563.592 M39.0506 563.592 Q39.0506 558.563 42.5517 555.603 Q46.0529 552.643 52.0048 552.643 Q57.9567 552.643 61.4579 555.635 Q64.9272 558.595 64.9272 563.592 Q64.9272 568.684 61.4579 571.644 Q57.9567 574.605 52.0048 574.605 Q46.021 574.605 42.5517 571.644 Q39.0506 568.653 39.0506 563.592 M19.667 596.439 Q19.667 599.176 22.0542 600.768 Q24.4095 602.327 28.5472 602.327 Q32.7485 602.327 35.1038 600.768 Q37.4592 599.208 37.4592 596.439 Q37.4592 593.67 35.1038 592.11 Q32.7485 590.519 28.5472 590.519 Q24.4413 590.519 22.0542 592.11 Q19.667 593.702 19.667 596.439 M15.6248 567.698 L15.6248 562.605 L64.9272 592.333 L64.9272 597.426 L15.6248 567.698 M15.6248 596.439 Q15.6248 591.41 19.1259 588.418 Q22.5952 585.426 28.5472 585.426 Q34.5628 585.426 38.0321 588.418 Q41.5014 591.378 41.5014 596.439 Q41.5014 601.5 38.0321 604.46 Q34.5309 607.388 28.5472 607.388 Q22.6271 607.388 19.1259 604.428 Q15.6248 601.468 15.6248 596.439 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M14.5426 543.826 L14.5426 538.734 Q22.0542 533.96 29.2474 531.604 Q36.4406 529.217 43.5384 529.217 Q50.668 529.217 57.8931 531.604 Q65.1182 533.96 72.5979 538.734 L72.5979 543.826 Q65.3091 539.593 58.1159 537.524 Q50.8908 535.424 43.5384 535.424 Q36.186 535.424 29.0246 537.524 Q21.8632 539.593 14.5426 543.826 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M967.464 20.1573 L956.365 50.2555 L978.604 50.2555 L967.464 20.1573 M962.846 12.096 L972.123 12.096 L995.173 72.576 L986.666 72.576 L981.156 57.061 L953.894 57.061 L948.385 72.576 L939.756 72.576 L962.846 12.096 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1034.87 28.9478 L1034.87 35.9153 Q1031.71 34.1734 1028.51 33.3227 Q1025.35 32.4315 1022.11 32.4315 Q1014.86 32.4315 1010.85 37.0496 Q1006.84 41.6271 1006.84 49.9314 Q1006.84 58.2358 1010.85 62.8538 Q1014.86 67.4314 1022.11 67.4314 Q1025.35 67.4314 1028.51 66.5807 Q1031.71 65.6895 1034.87 63.9476 L1034.87 70.8341 Q1031.75 72.2924 1028.39 73.0216 Q1025.07 73.7508 1021.3 73.7508 Q1011.05 73.7508 1005.02 67.3098 Q998.98 60.8689 998.98 49.9314 Q998.98 38.832 1005.06 32.472 Q1011.17 26.1121 1021.79 26.1121 Q1025.23 26.1121 1028.51 26.8413 Q1031.79 27.5299 1034.87 28.9478 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1080.48 28.9478 L1080.48 35.9153 Q1077.32 34.1734 1074.12 33.3227 Q1070.97 32.4315 1067.72 32.4315 Q1060.47 32.4315 1056.46 37.0496 Q1052.45 41.6271 1052.45 49.9314 Q1052.45 58.2358 1056.46 62.8538 Q1060.47 67.4314 1067.72 67.4314 Q1070.97 67.4314 1074.12 66.5807 Q1077.32 65.6895 1080.48 63.9476 L1080.48 70.8341 Q1077.37 72.2924 1074 73.0216 Q1070.68 73.7508 1066.91 73.7508 Q1056.67 73.7508 1050.63 67.3098 Q1044.59 60.8689 1044.59 49.9314 Q1044.59 38.832 1050.67 32.472 Q1056.79 26.1121 1067.4 26.1121 Q1070.84 26.1121 1074.12 26.8413 Q1077.41 27.5299 1080.48 28.9478 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1092.68 54.671 L1092.68 27.2059 L1100.13 27.2059 L1100.13 54.3874 Q1100.13 60.8284 1102.64 64.0691 Q1105.15 67.2693 1110.18 67.2693 Q1116.21 67.2693 1119.7 63.421 Q1123.22 59.5726 1123.22 52.9291 L1123.22 27.2059 L1130.68 27.2059 L1130.68 72.576 L1123.22 72.576 L1123.22 65.6084 Q1120.51 69.7404 1116.9 71.7658 Q1113.34 73.7508 1108.6 73.7508 Q1100.78 73.7508 1096.73 68.8897 Q1092.68 64.0286 1092.68 54.671 M1111.43 26.1121 L1111.43 26.1121 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1172.32 34.1734 Q1171.06 33.4443 1169.56 33.1202 Q1168.11 32.7556 1166.32 32.7556 Q1160 32.7556 1156.6 36.8875 Q1153.24 40.9789 1153.24 48.6757 L1153.24 72.576 L1145.74 72.576 L1145.74 27.2059 L1153.24 27.2059 L1153.24 34.2544 Q1155.59 30.1225 1159.36 28.1376 Q1163.12 26.1121 1168.51 26.1121 Q1169.28 26.1121 1170.21 26.2337 Q1171.14 26.3147 1172.28 26.5172 L1172.32 34.1734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1200.76 49.7694 Q1191.72 49.7694 1188.24 51.8354 Q1184.75 53.9013 1184.75 58.8839 Q1184.75 62.8538 1187.35 65.2034 Q1189.98 67.5124 1194.48 67.5124 Q1200.67 67.5124 1204.4 63.1374 Q1208.17 58.7219 1208.17 51.4303 L1208.17 49.7694 L1200.76 49.7694 M1215.62 46.6907 L1215.62 72.576 L1208.17 72.576 L1208.17 65.6895 Q1205.62 69.8214 1201.81 71.8063 Q1198 73.7508 1192.49 73.7508 Q1185.52 73.7508 1181.39 69.8619 Q1177.3 65.9325 1177.3 59.3701 Q1177.3 51.7138 1182.41 47.825 Q1187.55 43.9361 1197.72 43.9361 L1208.17 43.9361 L1208.17 43.2069 Q1208.17 38.0623 1204.77 35.2672 Q1201.4 32.4315 1195.29 32.4315 Q1191.4 32.4315 1187.71 33.3632 Q1184.03 34.295 1180.62 36.1584 L1180.62 29.2718 Q1184.71 27.692 1188.56 26.9223 Q1192.41 26.1121 1196.06 26.1121 Q1205.9 26.1121 1210.76 31.2163 Q1215.62 36.3204 1215.62 46.6907 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1263.63 28.9478 L1263.63 35.9153 Q1260.47 34.1734 1257.27 33.3227 Q1254.11 32.4315 1250.87 32.4315 Q1243.61 32.4315 1239.6 37.0496 Q1235.59 41.6271 1235.59 49.9314 Q1235.59 58.2358 1239.6 62.8538 Q1243.61 67.4314 1250.87 67.4314 Q1254.11 67.4314 1257.27 66.5807 Q1260.47 65.6895 1263.63 63.9476 L1263.63 70.8341 Q1260.51 72.2924 1257.14 73.0216 Q1253.82 73.7508 1250.06 73.7508 Q1239.81 73.7508 1233.77 67.3098 Q1227.74 60.8689 1227.74 49.9314 Q1227.74 38.832 1233.81 32.472 Q1239.93 26.1121 1250.54 26.1121 Q1253.98 26.1121 1257.27 26.8413 Q1260.55 27.5299 1263.63 28.9478 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1295.47 76.7889 Q1292.31 84.8907 1289.31 87.3618 Q1286.31 89.8329 1281.29 89.8329 L1275.33 89.8329 L1275.33 83.5945 L1279.71 83.5945 Q1282.79 83.5945 1284.49 82.1361 Q1286.19 80.6778 1288.26 75.2496 L1289.59 71.8468 L1271.24 27.2059 L1279.14 27.2059 L1293.32 62.6918 L1307.5 27.2059 L1315.4 27.2059 L1295.47 76.7889 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1378.35 34.1734 Q1377.09 33.4443 1375.59 33.1202 Q1374.13 32.7556 1372.35 32.7556 Q1366.03 32.7556 1362.63 36.8875 Q1359.27 40.9789 1359.27 48.6757 L1359.27 72.576 L1351.77 72.576 L1351.77 27.2059 L1359.27 27.2059 L1359.27 34.2544 Q1361.62 30.1225 1365.38 28.1376 Q1369.15 26.1121 1374.54 26.1121 Q1375.31 26.1121 1376.24 26.2337 Q1377.17 26.3147 1378.31 26.5172 L1378.35 34.1734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1423.15 48.0275 L1423.15 51.6733 L1388.88 51.6733 Q1389.37 59.3701 1393.5 63.421 Q1397.67 67.4314 1405.08 67.4314 Q1409.38 67.4314 1413.39 66.3781 Q1417.44 65.3249 1421.41 63.2184 L1421.41 70.267 Q1417.4 71.9684 1413.19 72.8596 Q1408.97 73.7508 1404.64 73.7508 Q1393.78 73.7508 1387.42 67.4314 Q1381.1 61.1119 1381.1 50.3365 Q1381.1 39.1965 1387.1 32.6746 Q1393.13 26.1121 1403.34 26.1121 Q1412.5 26.1121 1417.8 32.0264 Q1423.15 37.9003 1423.15 48.0275 M1415.7 45.84 Q1415.62 39.7232 1412.25 36.0774 Q1408.93 32.4315 1403.42 32.4315 Q1397.18 32.4315 1393.42 35.9558 Q1389.69 39.4801 1389.12 45.8805 L1415.7 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1464.31 28.5427 L1464.31 35.5912 Q1461.15 33.9709 1457.75 33.1607 Q1454.34 32.3505 1450.7 32.3505 Q1445.15 32.3505 1442.35 34.0519 Q1439.6 35.7533 1439.6 39.156 Q1439.6 41.7486 1441.58 43.2475 Q1443.57 44.7058 1449.56 46.0426 L1452.11 46.6097 Q1460.05 48.3111 1463.38 51.4303 Q1466.74 54.509 1466.74 60.0587 Q1466.74 66.3781 1461.72 70.0644 Q1456.73 73.7508 1447.98 73.7508 Q1444.34 73.7508 1440.37 73.0216 Q1436.44 72.3329 1432.06 70.9151 L1432.06 63.2184 Q1436.19 65.3654 1440.2 66.4591 Q1444.22 67.5124 1448.14 67.5124 Q1453.41 67.5124 1456.25 65.73 Q1459.08 63.9071 1459.08 60.6258 Q1459.08 57.5877 1457.02 55.9673 Q1454.99 54.3469 1448.06 52.8481 L1445.47 52.2405 Q1438.54 50.7821 1435.47 47.7845 Q1432.39 44.7463 1432.39 39.4801 Q1432.39 33.0797 1436.92 29.5959 Q1441.46 26.1121 1449.81 26.1121 Q1453.94 26.1121 1457.58 26.7198 Q1461.23 27.3274 1464.31 28.5427 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1477.84 54.671 L1477.84 27.2059 L1485.29 27.2059 L1485.29 54.3874 Q1485.29 60.8284 1487.8 64.0691 Q1490.31 67.2693 1495.34 67.2693 Q1501.37 67.2693 1504.86 63.421 Q1508.38 59.5726 1508.38 52.9291 L1508.38 27.2059 L1515.84 27.2059 L1515.84 72.576 L1508.38 72.576 L1508.38 65.6084 Q1505.67 69.7404 1502.06 71.7658 Q1498.5 73.7508 1493.76 73.7508 Q1485.94 73.7508 1481.89 68.8897 Q1477.84 64.0286 1477.84 54.671 M1496.59 26.1121 L1496.59 26.1121 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1531.19 9.54393 L1538.64 9.54393 L1538.64 72.576 L1531.19 72.576 L1531.19 9.54393 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1561.61 14.324 L1561.61 27.2059 L1576.96 27.2059 L1576.96 32.9987 L1561.61 32.9987 L1561.61 57.6282 Q1561.61 63.1779 1563.11 64.7578 Q1564.65 66.3376 1569.31 66.3376 L1576.96 66.3376 L1576.96 72.576 L1569.31 72.576 Q1560.68 72.576 1557.4 69.3758 Q1554.12 66.1351 1554.12 57.6282 L1554.12 32.9987 L1548.65 32.9987 L1548.65 27.2059 L1554.12 27.2059 L1554.12 14.324 L1561.61 14.324 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M1615.69 28.5427 L1615.69 35.5912 Q1612.53 33.9709 1609.13 33.1607 Q1605.72 32.3505 1602.08 32.3505 Q1596.53 32.3505 1593.73 34.0519 Q1590.98 35.7533 1590.98 39.156 Q1590.98 41.7486 1592.96 43.2475 Q1594.95 44.7058 1600.94 46.0426 L1603.5 46.6097 Q1611.44 48.3111 1614.76 51.4303 Q1618.12 54.509 1618.12 60.0587 Q1618.12 66.3781 1613.1 70.0644 Q1608.11 73.7508 1599.36 73.7508 Q1595.72 73.7508 1591.75 73.0216 Q1587.82 72.3329 1583.44 70.9151 L1583.44 63.2184 Q1587.58 65.3654 1591.59 66.4591 Q1595.6 67.5124 1599.53 67.5124 Q1604.79 67.5124 1607.63 65.73 Q1610.46 63.9071 1610.46 60.6258 Q1610.46 57.5877 1608.4 55.9673 Q1606.37 54.3469 1599.45 52.8481 L1596.85 52.2405 Q1589.93 50.7821 1586.85 47.7845 Q1583.77 44.7463 1583.77 39.4801 Q1583.77 33.0797 1588.31 29.5959 Q1592.84 26.1121 1601.19 26.1121 Q1605.32 26.1121 1608.97 26.7198 Q1612.61 27.3274 1615.69 28.5427 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip402)\" d=\"M323.245 383.192 L323.245 1423.18 L869.355 1423.18 L869.355 383.192 L323.245 383.192 L323.245 383.192  Z\" fill=\"#add8e6\" fill-rule=\"evenodd\" fill-opacity=\"0.5\"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"323.245,383.192 323.245,1423.18 869.355,1423.18 869.355,383.192 323.245,383.192 \"/>\n",
       "<path clip-path=\"url(#clip402)\" d=\"M1005.88 190.65 L1005.88 1423.18 L1551.99 1423.18 L1551.99 190.65 L1005.88 190.65 L1005.88 190.65  Z\" fill=\"#008000\" fill-rule=\"evenodd\" fill-opacity=\"0.5\"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"1005.88,190.65 1005.88,1423.18 1551.99,1423.18 1551.99,190.65 1005.88,190.65 \"/>\n",
       "<path clip-path=\"url(#clip402)\" d=\"M1688.52 242.252 L1688.52 1423.18 L2234.63 1423.18 L2234.63 242.252 L1688.52 242.252 L1688.52 242.252  Z\" fill=\"#ff0000\" fill-rule=\"evenodd\" fill-opacity=\"0.5\"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"1688.52,242.252 1688.52,1423.18 2234.63,1423.18 2234.63,242.252 1688.52,242.252 \"/>\n",
       "<circle clip-path=\"url(#clip402)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"596.3\" cy=\"383.192\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip402)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1278.94\" cy=\"190.65\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip402)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1961.58\" cy=\"242.252\" r=\"2\"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"596.3,595.399 596.3,170.985 \"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"1278.94,257.828 1278.94,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"1961.58,313.84 1961.58,170.665 \"/>\n",
       "<line clip-path=\"url(#clip402)\" x1=\"612.3\" y1=\"595.399\" x2=\"580.3\" y2=\"595.399\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip402)\" x1=\"612.3\" y1=\"170.985\" x2=\"580.3\" y2=\"170.985\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip402)\" x1=\"1294.94\" y1=\"257.828\" x2=\"1262.94\" y2=\"257.828\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip402)\" x1=\"1294.94\" y1=\"123.472\" x2=\"1262.94\" y2=\"123.472\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip402)\" x1=\"1977.58\" y1=\"313.84\" x2=\"1945.58\" y2=\"313.84\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip402)\" x1=\"1977.58\" y1=\"170.665\" x2=\"1945.58\" y2=\"170.665\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mIndices Base.OneTo(3) of attribute `fillcolor` does not match data indices 2:9.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Plots ~/.julia/packages/Plots/sxUvK/src/utils.jl:141\u001b[39m\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mData contains NaNs or missing values, and indices of `fillcolor` vector do not match data indices.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mIf you intend elements of `fillcolor` to apply to individual NaN-separated segments in the data,\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mpass each segment in a separate vector instead, and use a row vector for `fillcolor`. Legend entries\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mmay be suppressed by passing an empty label.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mFor example,\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m    plot([1:2,1:3], [[4,5],[3,4,5]], label=[\"y\" \"\"], fillcolor=[1 2])\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mIndices Base.OneTo(3) of attribute `fillcolor` does not match data indices 2:9.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Plots ~/.julia/packages/Plots/sxUvK/src/utils.jl:141\u001b[39m\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mData contains NaNs or missing values, and indices of `fillcolor` vector do not match data indices.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mIf you intend elements of `fillcolor` to apply to individual NaN-separated segments in the data,\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mpass each segment in a separate vector instead, and use a row vector for `fillcolor`. Legend entries\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mmay be suppressed by passing an empty label.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mFor example,\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m    plot([1:2,1:3], [[4,5],[3,4,5]], label=[\"y\" \"\"], fillcolor=[1 2])\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mIndices Base.OneTo(3) of attribute `fillcolor` does not match data indices 2:9.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Plots ~/.julia/packages/Plots/sxUvK/src/utils.jl:141\u001b[39m\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mData contains NaNs or missing values, and indices of `fillcolor` vector do not match data indices.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mIf you intend elements of `fillcolor` to apply to individual NaN-separated segments in the data,\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mpass each segment in a separate vector instead, and use a row vector for `fillcolor`. Legend entries\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mmay be suppressed by passing an empty label.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mFor example,\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m    plot([1:2,1:3], [[4,5],[3,4,5]], label=[\"y\" \"\"], fillcolor=[1 2])\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mIndices Base.OneTo(3) of attribute `fillcolor` does not match data indices 2:9.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Plots ~/.julia/packages/Plots/sxUvK/src/utils.jl:141\u001b[39m\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mData contains NaNs or missing values, and indices of `fillcolor` vector do not match data indices.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mIf you intend elements of `fillcolor` to apply to individual NaN-separated segments in the data,\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mpass each segment in a separate vector instead, and use a row vector for `fillcolor`. Legend entries\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mmay be suppressed by passing an empty label.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mFor example,\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m    plot([1:2,1:3], [[4,5],[3,4,5]], label=[\"y\" \"\"], fillcolor=[1 2])\n"
     ]
    }
   ],
   "source": [
    "plot_bars([78.77272727272728, 93.35662337662337, 89.44805194805194],[\"Stacking\",\"Hard\", \"Soft\"],[16.07340403398106, 5.088312167552783, 5.422347938079722],[:lightblue, :green, :red])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a265be0c",
   "metadata": {},
   "source": [
    "### Question\n",
    "Repeted the previous function, but this time allowing to pass only one estimator as base. it can be replicated and pass to the previous function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "128d7aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainClassEnsemble (generic function with 3 methods)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function trainClassEnsemble(baseEstimator::Symbol, \n",
    "        modelsHyperParameters::Dict,\n",
    "        trainingDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,2}},     \n",
    "        kFoldIndices::Array{Int64,1},\n",
    "        NumEstimators::Int=100)\n",
    "    \n",
    "    #Check that the base estimator is valid\n",
    "    @assert baseEstimator==:SVC || baseEstimator==:DecisionTreeClassifier || baseEstimator==:KNeighborsClassifier || baseEstimator==:ANN\n",
    "    \n",
    "    # Create a vector with the estimator and number of estimators\n",
    "    if baseEstimator==:SVC\n",
    "        estimators = fill(\"SVC\", NumEstimators)\n",
    "    elseif baseEstimator==:DecisionTreeClassifier\n",
    "        estimators = fill(\"DecisionTreeClassifier\", NumEstimators)\n",
    "    elseif baseEstimator==:KNeighborsClassifier\n",
    "        estimators = fill(\"KNeighborsClassifier\", NumEstimators)\n",
    "    elseif baseEstimator==:ANN\n",
    "        estimators = fill(\"ANN\", NumEstimators)\n",
    "    end\n",
    "    \n",
    "    # Convert the dictionary to a vector of dictionaries\n",
    "    modelsHyperParametersVector = Vector{Dict{String, Any}}(fill(modelsHyperParameters, NumEstimators))\n",
    "    \n",
    "    return trainClassEnsemble(estimators, modelsHyperParametersVector, trainingDataset, kFoldIndices)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier: 86.36363636363636 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier: 71.42857142857143 %\n",
      "KNeighborsClassifier: 80.95238095238095 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier: 85.71428571428571 %\n",
      "KNeighborsClassifier: 80.95238095238095 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier: 100.0 %\n",
      "KNeighborsClassifier: 80.95238095238095 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier: 90.0 %\n",
      "KNeighborsClassifier: 85.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/home/martin/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier: 60.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8792207792207792, 0.08152460110699424)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVCestimator=:SVC\n",
    "KRestimator=:KNeighborsClassifier\n",
    "DTestimator=:DecisionTreeClassifier\n",
    "ANNestimator=:ANN\n",
    "\n",
    "KR_hyper=Dict(\"n_neighbors\"=>3,\"weights\"=>\"distance\")\n",
    "SVM_hyper=Dict(\"probability\"=>true,\"kernel\"=>\"rbf\",\"C\"=>1.0,\"gamma\"=>\"scale\",\"degree\"=>3)\n",
    "DT_hyper=Dict(\"max_depth\"=>8,\"criterion\"=>\"gini\",\"splitter\"=>\"best\")\n",
    "ANN_hyper=Dict(\"hidden_layer_sizes\"=>(20,1),\"activation\"=>\"tanh\",\"learning_rate_init\"=>0.001,\"validation_fraction\"=>0.0,\"max_iter\"=>1000)\n",
    "\n",
    "\n",
    "meanTestResult, stdTestResult = trainClassEnsemble(KRestimator, KR_hyper,(trainInput, BitMatrix(trainOutput')), kFoldIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean test result: 87.92207792207792 %\n",
      "Standard deviation of test result: 8.152460110699424 %\n"
     ]
    }
   ],
   "source": [
    "println(\"Mean test result: $(meanTestResult*100) %\")\n",
    "println(\"Standard deviation of test result: $(stdTestResult*100) %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we used KNN to train the ensemble, because it obtained the best results both in accuracy and in std. All the models have a similar accuracy, but KNN has the lower std as we can see in the graphic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip440\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip440)\" d=\"M0 1600 L2400 1600 L2400 0 L0 0  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip441\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip440)\" d=\"M205.121 1423.18 L2352.76 1423.18 L2352.76 123.472 L205.121 123.472  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip442\">\n",
       "    <rect x=\"205\" y=\"123\" width=\"2149\" height=\"1301\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"524.443,1423.18 524.443,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1027.44,1423.18 1027.44,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1530.44,1423.18 1530.44,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2033.43,1423.18 2033.43,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 2352.76,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"524.443,1423.18 524.443,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1027.44,1423.18 1027.44,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1530.44,1423.18 1530.44,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2033.43,1423.18 2033.43,1404.28 \"/>\n",
       "<path clip-path=\"url(#clip440)\" d=\"M498.853 1452.15 L498.853 1456.71 Q496.191 1455.44 493.83 1454.82 Q491.469 1454.19 489.27 1454.19 Q485.451 1454.19 483.367 1455.67 Q481.307 1457.15 481.307 1459.89 Q481.307 1462.18 482.673 1463.36 Q484.062 1464.52 487.904 1465.23 L490.728 1465.81 Q495.96 1466.81 498.437 1469.33 Q500.937 1471.83 500.937 1476.04 Q500.937 1481.07 497.557 1483.66 Q494.201 1486.25 487.696 1486.25 Q485.242 1486.25 482.464 1485.7 Q479.71 1485.14 476.747 1484.05 L476.747 1479.24 Q479.594 1480.83 482.326 1481.64 Q485.057 1482.45 487.696 1482.45 Q491.701 1482.45 493.876 1480.88 Q496.052 1479.31 496.052 1476.39 Q496.052 1473.84 494.478 1472.41 Q492.927 1470.97 489.363 1470.26 L486.515 1469.7 Q481.284 1468.66 478.946 1466.44 Q476.608 1464.21 476.608 1460.26 Q476.608 1455.67 479.826 1453.03 Q483.066 1450.39 488.738 1450.39 Q491.168 1450.39 493.691 1450.83 Q496.214 1451.27 498.853 1452.15 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M517.14 1485.58 L503.946 1451.02 L508.83 1451.02 L519.779 1480.12 L530.751 1451.02 L535.612 1451.02 L522.441 1485.58 L517.14 1485.58 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M540.659 1451.02 L547.626 1451.02 L556.446 1474.54 L565.311 1451.02 L572.279 1451.02 L572.279 1485.58 L567.719 1485.58 L567.719 1455.23 L558.807 1478.94 L554.108 1478.94 L545.196 1455.23 L545.196 1485.58 L540.659 1485.58 L540.659 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M981.086 1451.02 L985.762 1451.02 L985.762 1465.63 L1001.27 1451.02 L1007.29 1451.02 L990.137 1467.13 L1008.52 1485.58 L1002.36 1485.58 L985.762 1468.94 L985.762 1485.58 L981.086 1485.58 L981.086 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1012.17 1451.02 L1018.47 1451.02 L1033.79 1479.93 L1033.79 1451.02 L1038.33 1451.02 L1038.33 1485.58 L1032.03 1485.58 L1016.71 1456.67 L1016.71 1485.58 L1012.17 1485.58 L1012.17 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1047.64 1451.02 L1053.93 1451.02 L1069.26 1479.93 L1069.26 1451.02 L1073.79 1451.02 L1073.79 1485.58 L1067.5 1485.58 L1052.17 1456.67 L1052.17 1485.58 L1047.64 1485.58 L1047.64 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1504.64 1454.86 L1504.64 1481.74 L1510.29 1481.74 Q1517.44 1481.74 1520.75 1478.5 Q1524.08 1475.26 1524.08 1468.27 Q1524.08 1461.32 1520.75 1458.1 Q1517.44 1454.86 1510.29 1454.86 L1504.64 1454.86 M1499.96 1451.02 L1509.57 1451.02 Q1519.62 1451.02 1524.31 1455.21 Q1529.01 1459.38 1529.01 1468.27 Q1529.01 1477.2 1524.29 1481.39 Q1519.57 1485.58 1509.57 1485.58 L1499.96 1485.58 L1499.96 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1531.68 1451.02 L1560.91 1451.02 L1560.91 1454.96 L1548.64 1454.96 L1548.64 1485.58 L1543.94 1485.58 L1543.94 1454.96 L1531.68 1454.96 L1531.68 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M2000.1 1455.63 L1993.76 1472.83 L2006.47 1472.83 L2000.1 1455.63 M1997.46 1451.02 L2002.76 1451.02 L2015.93 1485.58 L2011.07 1485.58 L2007.92 1476.71 L1992.35 1476.71 L1989.2 1485.58 L1984.27 1485.58 L1997.46 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M2020.98 1451.02 L2027.28 1451.02 L2042.6 1479.93 L2042.6 1451.02 L2047.14 1451.02 L2047.14 1485.58 L2040.84 1485.58 L2025.52 1456.67 L2025.52 1485.58 L2020.98 1485.58 L2020.98 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M2056.44 1451.02 L2062.74 1451.02 L2078.06 1479.93 L2078.06 1451.02 L2082.6 1451.02 L2082.6 1485.58 L2076.3 1485.58 L2060.98 1456.67 L2060.98 1485.58 L2056.44 1485.58 L2056.44 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1109.5 1520.52 L1139.55 1520.52 L1139.55 1525.93 L1115.93 1525.93 L1115.93 1540 L1138.56 1540 L1138.56 1545.41 L1115.93 1545.41 L1115.93 1562.63 L1140.12 1562.63 L1140.12 1568.04 L1109.5 1568.04 L1109.5 1520.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1173.16 1533.45 L1173.16 1538.98 Q1170.67 1537.71 1168 1537.07 Q1165.33 1536.44 1162.46 1536.44 Q1158.1 1536.44 1155.91 1537.77 Q1153.74 1539.11 1153.74 1541.79 Q1153.74 1543.82 1155.3 1545 Q1156.86 1546.15 1161.57 1547.2 L1163.58 1547.64 Q1169.81 1548.98 1172.42 1551.43 Q1175.07 1553.85 1175.07 1558.21 Q1175.07 1563.17 1171.12 1566.07 Q1167.2 1568.97 1160.33 1568.97 Q1157.46 1568.97 1154.35 1568.39 Q1151.26 1567.85 1147.82 1566.74 L1147.82 1560.69 Q1151.07 1562.38 1154.22 1563.24 Q1157.37 1564.07 1160.46 1564.07 Q1164.59 1564.07 1166.82 1562.66 Q1169.05 1561.23 1169.05 1558.65 Q1169.05 1556.27 1167.43 1554.99 Q1165.84 1553.72 1160.39 1552.54 L1158.36 1552.07 Q1152.91 1550.92 1150.49 1548.56 Q1148.08 1546.18 1148.08 1542.04 Q1148.08 1537.01 1151.64 1534.27 Q1155.21 1531.54 1161.76 1531.54 Q1165.01 1531.54 1167.87 1532.01 Q1170.74 1532.49 1173.16 1533.45 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1190.18 1522.27 L1190.18 1532.4 L1202.25 1532.4 L1202.25 1536.95 L1190.18 1536.95 L1190.18 1556.3 Q1190.18 1560.66 1191.36 1561.9 Q1192.57 1563.14 1196.23 1563.14 L1202.25 1563.14 L1202.25 1568.04 L1196.23 1568.04 Q1189.45 1568.04 1186.87 1565.53 Q1184.3 1562.98 1184.3 1556.3 L1184.3 1536.95 L1180 1536.95 L1180 1532.4 L1184.3 1532.4 L1184.3 1522.27 L1190.18 1522.27 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1209.95 1532.4 L1215.81 1532.4 L1215.81 1568.04 L1209.95 1568.04 L1209.95 1532.4 M1209.95 1518.52 L1215.81 1518.52 L1215.81 1525.93 L1209.95 1525.93 L1209.95 1518.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1255.82 1539.24 Q1258.01 1535.29 1261.07 1533.41 Q1264.12 1531.54 1268.26 1531.54 Q1273.83 1531.54 1276.85 1535.45 Q1279.88 1539.33 1279.88 1546.53 L1279.88 1568.04 L1273.99 1568.04 L1273.99 1546.72 Q1273.99 1541.59 1272.17 1539.11 Q1270.36 1536.63 1266.64 1536.63 Q1262.09 1536.63 1259.44 1539.65 Q1256.8 1542.68 1256.8 1547.9 L1256.8 1568.04 L1250.91 1568.04 L1250.91 1546.72 Q1250.91 1541.56 1249.1 1539.11 Q1247.28 1536.63 1243.5 1536.63 Q1239.01 1536.63 1236.37 1539.68 Q1233.73 1542.71 1233.73 1547.9 L1233.73 1568.04 L1227.84 1568.04 L1227.84 1532.4 L1233.73 1532.4 L1233.73 1537.93 Q1235.73 1534.66 1238.53 1533.1 Q1241.33 1531.54 1245.18 1531.54 Q1249.07 1531.54 1251.77 1533.51 Q1254.51 1535.48 1255.82 1539.24 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1307.76 1550.12 Q1300.66 1550.12 1297.92 1551.75 Q1295.19 1553.37 1295.19 1557.29 Q1295.19 1560.4 1297.22 1562.25 Q1299.29 1564.07 1302.83 1564.07 Q1307.7 1564.07 1310.62 1560.63 Q1313.58 1557.16 1313.58 1551.43 L1313.58 1550.12 L1307.76 1550.12 M1319.44 1547.71 L1319.44 1568.04 L1313.58 1568.04 L1313.58 1562.63 Q1311.58 1565.88 1308.59 1567.44 Q1305.59 1568.97 1301.27 1568.97 Q1295.79 1568.97 1292.55 1565.91 Q1289.33 1562.82 1289.33 1557.67 Q1289.33 1551.65 1293.34 1548.6 Q1297.38 1545.54 1305.37 1545.54 L1313.58 1545.54 L1313.58 1544.97 Q1313.58 1540.93 1310.91 1538.73 Q1308.27 1536.5 1303.46 1536.5 Q1300.41 1536.5 1297.51 1537.23 Q1294.61 1537.97 1291.94 1539.43 L1291.94 1534.02 Q1295.16 1532.78 1298.18 1532.17 Q1301.2 1531.54 1304.07 1531.54 Q1311.8 1531.54 1315.62 1535.55 Q1319.44 1539.56 1319.44 1547.71 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1337.3 1522.27 L1337.3 1532.4 L1349.36 1532.4 L1349.36 1536.95 L1337.3 1536.95 L1337.3 1556.3 Q1337.3 1560.66 1338.47 1561.9 Q1339.68 1563.14 1343.34 1563.14 L1349.36 1563.14 L1349.36 1568.04 L1343.34 1568.04 Q1336.56 1568.04 1333.99 1565.53 Q1331.41 1562.98 1331.41 1556.3 L1331.41 1536.95 L1327.11 1536.95 L1327.11 1532.4 L1331.41 1532.4 L1331.41 1522.27 L1337.3 1522.27 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1370.88 1536.5 Q1366.16 1536.5 1363.43 1540.19 Q1360.69 1543.85 1360.69 1550.25 Q1360.69 1556.65 1363.4 1560.34 Q1366.13 1564 1370.88 1564 Q1375.55 1564 1378.29 1560.31 Q1381.03 1556.62 1381.03 1550.25 Q1381.03 1543.92 1378.29 1540.23 Q1375.55 1536.5 1370.88 1536.5 M1370.88 1531.54 Q1378.51 1531.54 1382.87 1536.5 Q1387.24 1541.47 1387.24 1550.25 Q1387.24 1559 1382.87 1564 Q1378.51 1568.97 1370.88 1568.97 Q1363.2 1568.97 1358.84 1564 Q1354.52 1559 1354.52 1550.25 Q1354.52 1541.47 1358.84 1536.5 Q1363.2 1531.54 1370.88 1531.54 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1417.6 1537.87 Q1416.61 1537.3 1415.44 1537.04 Q1414.29 1536.76 1412.89 1536.76 Q1407.92 1536.76 1405.25 1540 Q1402.61 1543.22 1402.61 1549.27 L1402.61 1568.04 L1396.72 1568.04 L1396.72 1532.4 L1402.61 1532.4 L1402.61 1537.93 Q1404.45 1534.69 1407.41 1533.13 Q1410.37 1531.54 1414.61 1531.54 Q1415.21 1531.54 1415.94 1531.63 Q1416.68 1531.7 1417.57 1531.85 L1417.6 1537.87 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1446.47 1533.45 L1446.47 1538.98 Q1443.99 1537.71 1441.31 1537.07 Q1438.64 1536.44 1435.77 1536.44 Q1431.41 1536.44 1429.22 1537.77 Q1427.05 1539.11 1427.05 1541.79 Q1427.05 1543.82 1428.61 1545 Q1430.17 1546.15 1434.88 1547.2 L1436.89 1547.64 Q1443.13 1548.98 1445.74 1551.43 Q1448.38 1553.85 1448.38 1558.21 Q1448.38 1563.17 1444.43 1566.07 Q1440.52 1568.97 1433.64 1568.97 Q1430.78 1568.97 1427.66 1568.39 Q1424.57 1567.85 1421.13 1566.74 L1421.13 1560.69 Q1424.38 1562.38 1427.53 1563.24 Q1430.68 1564.07 1433.77 1564.07 Q1437.91 1564.07 1440.13 1562.66 Q1442.36 1561.23 1442.36 1558.65 Q1442.36 1556.27 1440.74 1554.99 Q1439.15 1553.72 1433.7 1552.54 L1431.67 1552.07 Q1426.22 1550.92 1423.81 1548.56 Q1421.39 1546.18 1421.39 1542.04 Q1421.39 1537.01 1424.95 1534.27 Q1428.52 1531.54 1435.07 1531.54 Q1438.32 1531.54 1441.18 1532.01 Q1444.05 1532.49 1446.47 1533.45 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,1423.18 2352.76,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,1158.18 2352.76,1158.18 \"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,893.189 2352.76,893.189 \"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,628.194 2352.76,628.194 \"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,363.198 2352.76,363.198 \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 205.121,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 224.019,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1158.18 224.019,1158.18 \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,893.189 224.019,893.189 \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,628.194 224.019,628.194 \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,363.198 224.019,363.198 \"/>\n",
       "<path clip-path=\"url(#clip440)\" d=\"M157.177 1408.98 Q153.566 1408.98 151.737 1412.54 Q149.931 1416.08 149.931 1423.21 Q149.931 1430.32 151.737 1433.89 Q153.566 1437.43 157.177 1437.43 Q160.811 1437.43 162.616 1433.89 Q164.445 1430.32 164.445 1423.21 Q164.445 1416.08 162.616 1412.54 Q160.811 1408.98 157.177 1408.98 M157.177 1405.27 Q162.987 1405.27 166.042 1409.88 Q169.121 1414.46 169.121 1423.21 Q169.121 1431.94 166.042 1436.55 Q162.987 1441.13 157.177 1441.13 Q151.366 1441.13 148.288 1436.55 Q145.232 1431.94 145.232 1423.21 Q145.232 1414.46 148.288 1409.88 Q151.366 1405.27 157.177 1405.27 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M121.043 1171.53 L137.362 1171.53 L137.362 1175.46 L115.418 1175.46 L115.418 1171.53 Q118.08 1168.77 122.663 1164.15 Q127.269 1159.49 128.45 1158.15 Q130.695 1155.63 131.575 1153.89 Q132.478 1152.13 132.478 1150.44 Q132.478 1147.69 130.533 1145.95 Q128.612 1144.21 125.51 1144.21 Q123.311 1144.21 120.857 1144.98 Q118.427 1145.74 115.649 1147.29 L115.649 1142.57 Q118.473 1141.44 120.927 1140.86 Q123.38 1140.28 125.418 1140.28 Q130.788 1140.28 133.982 1142.96 Q137.177 1145.65 137.177 1150.14 Q137.177 1152.27 136.367 1154.19 Q135.579 1156.09 133.473 1158.68 Q132.894 1159.35 129.792 1162.57 Q126.691 1165.77 121.043 1171.53 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M157.177 1143.98 Q153.566 1143.98 151.737 1147.55 Q149.931 1151.09 149.931 1158.22 Q149.931 1165.33 151.737 1168.89 Q153.566 1172.43 157.177 1172.43 Q160.811 1172.43 162.616 1168.89 Q164.445 1165.33 164.445 1158.22 Q164.445 1151.09 162.616 1147.55 Q160.811 1143.98 157.177 1143.98 M157.177 1140.28 Q162.987 1140.28 166.042 1144.89 Q169.121 1149.47 169.121 1158.22 Q169.121 1166.95 166.042 1171.55 Q162.987 1176.14 157.177 1176.14 Q151.366 1176.14 148.288 1171.55 Q145.232 1166.95 145.232 1158.22 Q145.232 1149.47 148.288 1144.89 Q151.366 1140.28 157.177 1140.28 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M129.862 879.983 L118.056 898.432 L129.862 898.432 L129.862 879.983 M128.635 875.909 L134.515 875.909 L134.515 898.432 L139.445 898.432 L139.445 902.321 L134.515 902.321 L134.515 910.469 L129.862 910.469 L129.862 902.321 L114.26 902.321 L114.26 897.807 L128.635 875.909 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M157.177 878.988 Q153.566 878.988 151.737 882.553 Q149.931 886.094 149.931 893.224 Q149.931 900.33 151.737 903.895 Q153.566 907.437 157.177 907.437 Q160.811 907.437 162.616 903.895 Q164.445 900.33 164.445 893.224 Q164.445 886.094 162.616 882.553 Q160.811 878.988 157.177 878.988 M157.177 875.284 Q162.987 875.284 166.042 879.891 Q169.121 884.474 169.121 893.224 Q169.121 901.951 166.042 906.557 Q162.987 911.14 157.177 911.14 Q151.366 911.14 148.288 906.557 Q145.232 901.951 145.232 893.224 Q145.232 884.474 148.288 879.891 Q151.366 875.284 157.177 875.284 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M127.593 626.33 Q124.445 626.33 122.593 628.483 Q120.765 630.636 120.765 634.386 Q120.765 638.113 122.593 640.289 Q124.445 642.441 127.593 642.441 Q130.742 642.441 132.57 640.289 Q134.422 638.113 134.422 634.386 Q134.422 630.636 132.57 628.483 Q130.742 626.33 127.593 626.33 M136.876 611.678 L136.876 615.937 Q135.117 615.103 133.311 614.664 Q131.529 614.224 129.769 614.224 Q125.14 614.224 122.686 617.349 Q120.255 620.474 119.908 626.793 Q121.274 624.779 123.334 623.715 Q125.394 622.627 127.871 622.627 Q133.08 622.627 136.089 625.798 Q139.121 628.946 139.121 634.386 Q139.121 639.71 135.973 642.927 Q132.825 646.145 127.593 646.145 Q121.598 646.145 118.427 641.562 Q115.256 636.955 115.256 628.228 Q115.256 620.034 119.144 615.173 Q123.033 610.289 129.584 610.289 Q131.343 610.289 133.126 610.636 Q134.931 610.983 136.876 611.678 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M157.177 613.992 Q153.566 613.992 151.737 617.557 Q149.931 621.099 149.931 628.228 Q149.931 635.335 151.737 638.9 Q153.566 642.441 157.177 642.441 Q160.811 642.441 162.616 638.9 Q164.445 635.335 164.445 628.228 Q164.445 621.099 162.616 617.557 Q160.811 613.992 157.177 613.992 M157.177 610.289 Q162.987 610.289 166.042 614.895 Q169.121 619.478 169.121 628.228 Q169.121 636.955 166.042 641.562 Q162.987 646.145 157.177 646.145 Q151.366 646.145 148.288 641.562 Q145.232 636.955 145.232 628.228 Q145.232 619.478 148.288 614.895 Q151.366 610.289 157.177 610.289 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M127.015 364.066 Q123.681 364.066 121.76 365.849 Q119.862 367.631 119.862 370.756 Q119.862 373.881 121.76 375.664 Q123.681 377.446 127.015 377.446 Q130.348 377.446 132.269 375.664 Q134.191 373.858 134.191 370.756 Q134.191 367.631 132.269 365.849 Q130.371 364.066 127.015 364.066 M122.339 362.076 Q119.33 361.335 117.64 359.275 Q115.973 357.215 115.973 354.252 Q115.973 350.108 118.913 347.701 Q121.876 345.293 127.015 345.293 Q132.177 345.293 135.117 347.701 Q138.056 350.108 138.056 354.252 Q138.056 357.215 136.367 359.275 Q134.7 361.335 131.714 362.076 Q135.093 362.863 136.968 365.154 Q138.867 367.446 138.867 370.756 Q138.867 375.779 135.788 378.464 Q132.732 381.15 127.015 381.15 Q121.297 381.15 118.218 378.464 Q115.163 375.779 115.163 370.756 Q115.163 367.446 117.061 365.154 Q118.959 362.863 122.339 362.076 M120.626 354.691 Q120.626 357.377 122.293 358.881 Q123.982 360.386 127.015 360.386 Q130.024 360.386 131.714 358.881 Q133.427 357.377 133.427 354.691 Q133.427 352.006 131.714 350.502 Q130.024 348.997 127.015 348.997 Q123.982 348.997 122.293 350.502 Q120.626 352.006 120.626 354.691 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M157.177 348.997 Q153.566 348.997 151.737 352.562 Q149.931 356.103 149.931 363.233 Q149.931 370.339 151.737 373.904 Q153.566 377.446 157.177 377.446 Q160.811 377.446 162.616 373.904 Q164.445 370.339 164.445 363.233 Q164.445 356.103 162.616 352.562 Q160.811 348.997 157.177 348.997 M157.177 345.293 Q162.987 345.293 166.042 349.9 Q169.121 354.483 169.121 363.233 Q169.121 371.96 166.042 376.566 Q162.987 381.15 157.177 381.15 Q151.366 381.15 148.288 376.566 Q145.232 371.96 145.232 363.233 Q145.232 354.483 148.288 349.9 Q151.366 345.293 157.177 345.293 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M21.7677 1011.01 L39.6235 1011.01 L39.6235 1002.92 Q39.6235 998.433 37.3 995.982 Q34.9765 993.532 30.6797 993.532 Q26.4147 993.532 24.0912 995.982 Q21.7677 998.433 21.7677 1002.92 L21.7677 1011.01 M16.4842 1017.43 L16.4842 1002.92 Q16.4842 994.932 20.1126 990.858 Q23.7092 986.752 30.6797 986.752 Q37.7138 986.752 41.3104 990.858 Q44.907 994.932 44.907 1002.92 L44.907 1011.01 L64.0042 1011.01 L64.0042 1017.43 L16.4842 1017.43 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M44.7161 950.213 L47.5806 950.213 L47.5806 977.14 Q53.6281 976.758 56.8109 973.512 Q59.9619 970.233 59.9619 964.409 Q59.9619 961.035 59.1344 957.884 Q58.3069 954.701 56.6518 951.582 L62.1899 951.582 Q63.5267 954.733 64.227 958.043 Q64.9272 961.353 64.9272 964.759 Q64.9272 973.289 59.9619 978.286 Q54.9967 983.251 46.5303 983.251 Q37.7774 983.251 32.6531 978.54 Q27.4968 973.798 27.4968 965.777 Q27.4968 958.584 32.1438 954.414 Q36.7589 950.213 44.7161 950.213 M42.9973 956.07 Q38.1912 956.133 35.3266 958.775 Q32.4621 961.385 32.4621 965.714 Q32.4621 970.615 35.2312 973.575 Q38.0002 976.503 43.0292 976.949 L42.9973 956.07 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M33.8307 919.944 Q33.2578 920.931 33.0032 922.108 Q32.7167 923.254 32.7167 924.655 Q32.7167 929.62 35.9632 932.294 Q39.1779 934.935 45.2253 934.935 L64.0042 934.935 L64.0042 940.824 L28.3562 940.824 L28.3562 934.935 L33.8944 934.935 Q30.6479 933.089 29.0883 930.129 Q27.4968 927.169 27.4968 922.936 Q27.4968 922.331 27.5923 921.599 Q27.656 920.867 27.8151 919.976 L33.8307 919.944 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M29.7248 889.58 L35.1993 889.58 Q33.8307 892.062 33.1623 894.577 Q32.4621 897.059 32.4621 899.606 Q32.4621 905.303 36.0905 908.454 Q39.6872 911.605 46.212 911.605 Q52.7369 911.605 56.3653 908.454 Q59.9619 905.303 59.9619 899.606 Q59.9619 897.059 59.2935 894.577 Q58.5933 892.062 57.2247 889.58 L62.6355 889.58 Q63.7814 892.03 64.3543 894.672 Q64.9272 897.282 64.9272 900.242 Q64.9272 908.295 59.8664 913.037 Q54.8057 917.78 46.212 917.78 Q37.491 917.78 32.4939 913.006 Q27.4968 908.199 27.4968 899.86 Q27.4968 897.155 28.0697 894.577 Q28.6108 891.999 29.7248 889.58 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M44.7161 848.903 L47.5806 848.903 L47.5806 875.83 Q53.6281 875.448 56.8109 872.201 Q59.9619 868.923 59.9619 863.098 Q59.9619 859.725 59.1344 856.574 Q58.3069 853.391 56.6518 850.271 L62.1899 850.271 Q63.5267 853.422 64.227 856.733 Q64.9272 860.043 64.9272 863.448 Q64.9272 871.979 59.9619 876.976 Q54.9967 881.941 46.5303 881.941 Q37.7774 881.941 32.6531 877.23 Q27.4968 872.488 27.4968 864.467 Q27.4968 857.274 32.1438 853.104 Q36.7589 848.903 44.7161 848.903 M42.9973 854.759 Q38.1912 854.823 35.3266 857.465 Q32.4621 860.075 32.4621 864.403 Q32.4621 869.305 35.2312 872.265 Q38.0002 875.193 43.0292 875.639 L42.9973 854.759 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M42.4881 809.658 L64.0042 809.658 L64.0042 815.515 L42.679 815.515 Q37.6183 815.515 35.1038 817.488 Q32.5894 819.461 32.5894 823.408 Q32.5894 828.151 35.6131 830.888 Q38.6368 833.625 43.8567 833.625 L64.0042 833.625 L64.0042 839.513 L28.3562 839.513 L28.3562 833.625 L33.8944 833.625 Q30.6797 831.524 29.0883 828.692 Q27.4968 825.827 27.4968 822.103 Q27.4968 815.96 31.3163 812.809 Q35.1038 809.658 42.4881 809.658 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M18.2347 792.184 L28.3562 792.184 L28.3562 780.121 L32.9077 780.121 L32.9077 792.184 L52.2594 792.184 Q56.6199 792.184 57.8613 791.007 Q59.1026 789.797 59.1026 786.137 L59.1026 780.121 L64.0042 780.121 L64.0042 786.137 Q64.0042 792.916 61.4897 795.495 Q58.9434 798.073 52.2594 798.073 L32.9077 798.073 L32.9077 802.37 L28.3562 802.37 L28.3562 798.073 L18.2347 798.073 L18.2347 792.184 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M46.0847 756.218 Q46.0847 763.316 47.7079 766.053 Q49.3312 768.79 53.2461 768.79 Q56.3653 768.79 58.2114 766.753 Q60.0256 764.685 60.0256 761.152 Q60.0256 756.282 56.5881 753.354 Q53.1188 750.394 47.3897 750.394 L46.0847 750.394 L46.0847 756.218 M43.6657 744.537 L64.0042 744.537 L64.0042 750.394 L58.5933 750.394 Q61.8398 752.399 63.3994 755.391 Q64.9272 758.382 64.9272 762.711 Q64.9272 768.186 61.8716 771.432 Q58.7843 774.647 53.6281 774.647 Q47.6125 774.647 44.5569 770.636 Q41.5014 766.594 41.5014 758.605 L41.5014 750.394 L40.9285 750.394 Q36.8862 750.394 34.6901 753.067 Q32.4621 755.709 32.4621 760.515 Q32.4621 763.571 33.1941 766.467 Q33.9262 769.363 35.3903 772.037 L29.9795 772.037 Q28.7381 768.822 28.1334 765.799 Q27.4968 762.775 27.4968 759.91 Q27.4968 752.176 31.5072 748.356 Q35.5176 744.537 43.6657 744.537 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M45.7664 709.016 Q39.4007 709.016 35.8996 711.658 Q32.3984 714.268 32.3984 719.011 Q32.3984 723.721 35.8996 726.363 Q39.4007 728.973 45.7664 728.973 Q52.1003 728.973 55.6014 726.363 Q59.1026 723.721 59.1026 719.011 Q59.1026 714.268 55.6014 711.658 Q52.1003 709.016 45.7664 709.016 M59.58 703.16 Q68.683 703.16 73.1071 707.202 Q77.5631 711.244 77.5631 719.584 Q77.5631 722.671 77.0857 725.408 Q76.6401 728.145 75.6852 730.723 L69.9879 730.723 Q71.3884 728.145 72.0568 725.631 Q72.7252 723.116 72.7252 720.507 Q72.7252 714.746 69.7015 711.881 Q66.7096 709.016 60.6303 709.016 L57.7339 709.016 Q60.885 710.831 62.4446 713.663 Q64.0042 716.496 64.0042 720.443 Q64.0042 727 59.0071 731.01 Q54.01 735.02 45.7664 735.02 Q37.491 735.02 32.4939 731.01 Q27.4968 727 27.4968 720.443 Q27.4968 716.496 29.0564 713.663 Q30.616 710.831 33.7671 709.016 L28.3562 709.016 L28.3562 703.16 L59.58 703.16 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M44.7161 660.605 L47.5806 660.605 L47.5806 687.532 Q53.6281 687.15 56.8109 683.904 Q59.9619 680.625 59.9619 674.801 Q59.9619 671.427 59.1344 668.276 Q58.3069 665.093 56.6518 661.974 L62.1899 661.974 Q63.5267 665.125 64.227 668.435 Q64.9272 671.745 64.9272 675.151 Q64.9272 683.681 59.9619 688.678 Q54.9967 693.643 46.5303 693.643 Q37.7774 693.643 32.6531 688.933 Q27.4968 684.19 27.4968 676.169 Q27.4968 668.976 32.1438 664.807 Q36.7589 660.605 44.7161 660.605 M42.9973 666.462 Q38.1912 666.525 35.3266 669.167 Q32.4621 671.777 32.4621 676.106 Q32.4621 681.007 35.2312 683.967 Q38.0002 686.896 43.0292 687.341 L42.9973 666.462 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M14.5426 616.204 Q21.8632 620.469 29.0246 622.538 Q36.186 624.607 43.5384 624.607 Q50.8908 624.607 58.1159 622.538 Q65.3091 620.438 72.5979 616.204 L72.5979 621.297 Q65.1182 626.071 57.8931 628.458 Q50.668 630.814 43.5384 630.814 Q36.4406 630.814 29.2474 628.458 Q22.0542 626.103 14.5426 621.297 L14.5426 616.204 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M43.0928 563.592 Q43.0928 566.361 45.4481 567.952 Q47.8034 569.512 52.0048 569.512 Q56.1425 569.512 58.5296 567.952 Q60.885 566.361 60.885 563.592 Q60.885 560.886 58.5296 559.327 Q56.1425 557.735 52.0048 557.735 Q47.8353 557.735 45.48 559.327 Q43.0928 560.886 43.0928 563.592 M39.0506 563.592 Q39.0506 558.563 42.5517 555.603 Q46.0529 552.643 52.0048 552.643 Q57.9567 552.643 61.4579 555.635 Q64.9272 558.595 64.9272 563.592 Q64.9272 568.684 61.4579 571.644 Q57.9567 574.605 52.0048 574.605 Q46.021 574.605 42.5517 571.644 Q39.0506 568.653 39.0506 563.592 M19.667 596.439 Q19.667 599.176 22.0542 600.768 Q24.4095 602.327 28.5472 602.327 Q32.7485 602.327 35.1038 600.768 Q37.4592 599.208 37.4592 596.439 Q37.4592 593.67 35.1038 592.11 Q32.7485 590.519 28.5472 590.519 Q24.4413 590.519 22.0542 592.11 Q19.667 593.702 19.667 596.439 M15.6248 567.698 L15.6248 562.605 L64.9272 592.333 L64.9272 597.426 L15.6248 567.698 M15.6248 596.439 Q15.6248 591.41 19.1259 588.418 Q22.5952 585.426 28.5472 585.426 Q34.5628 585.426 38.0321 588.418 Q41.5014 591.378 41.5014 596.439 Q41.5014 601.5 38.0321 604.46 Q34.5309 607.388 28.5472 607.388 Q22.6271 607.388 19.1259 604.428 Q15.6248 601.468 15.6248 596.439 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M14.5426 543.826 L14.5426 538.734 Q22.0542 533.96 29.2474 531.604 Q36.4406 529.217 43.5384 529.217 Q50.668 529.217 57.8931 531.604 Q65.1182 533.96 72.5979 538.734 L72.5979 543.826 Q65.3091 539.593 58.1159 537.524 Q50.8908 535.424 43.5384 535.424 Q36.186 535.424 29.0246 537.524 Q21.8632 539.593 14.5426 543.826 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M967.464 20.1573 L956.365 50.2555 L978.604 50.2555 L967.464 20.1573 M962.846 12.096 L972.123 12.096 L995.173 72.576 L986.666 72.576 L981.156 57.061 L953.894 57.061 L948.385 72.576 L939.756 72.576 L962.846 12.096 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1034.87 28.9478 L1034.87 35.9153 Q1031.71 34.1734 1028.51 33.3227 Q1025.35 32.4315 1022.11 32.4315 Q1014.86 32.4315 1010.85 37.0496 Q1006.84 41.6271 1006.84 49.9314 Q1006.84 58.2358 1010.85 62.8538 Q1014.86 67.4314 1022.11 67.4314 Q1025.35 67.4314 1028.51 66.5807 Q1031.71 65.6895 1034.87 63.9476 L1034.87 70.8341 Q1031.75 72.2924 1028.39 73.0216 Q1025.07 73.7508 1021.3 73.7508 Q1011.05 73.7508 1005.02 67.3098 Q998.98 60.8689 998.98 49.9314 Q998.98 38.832 1005.06 32.472 Q1011.17 26.1121 1021.79 26.1121 Q1025.23 26.1121 1028.51 26.8413 Q1031.79 27.5299 1034.87 28.9478 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1080.48 28.9478 L1080.48 35.9153 Q1077.32 34.1734 1074.12 33.3227 Q1070.97 32.4315 1067.72 32.4315 Q1060.47 32.4315 1056.46 37.0496 Q1052.45 41.6271 1052.45 49.9314 Q1052.45 58.2358 1056.46 62.8538 Q1060.47 67.4314 1067.72 67.4314 Q1070.97 67.4314 1074.12 66.5807 Q1077.32 65.6895 1080.48 63.9476 L1080.48 70.8341 Q1077.37 72.2924 1074 73.0216 Q1070.68 73.7508 1066.91 73.7508 Q1056.67 73.7508 1050.63 67.3098 Q1044.59 60.8689 1044.59 49.9314 Q1044.59 38.832 1050.67 32.472 Q1056.79 26.1121 1067.4 26.1121 Q1070.84 26.1121 1074.12 26.8413 Q1077.41 27.5299 1080.48 28.9478 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1092.68 54.671 L1092.68 27.2059 L1100.13 27.2059 L1100.13 54.3874 Q1100.13 60.8284 1102.64 64.0691 Q1105.15 67.2693 1110.18 67.2693 Q1116.21 67.2693 1119.7 63.421 Q1123.22 59.5726 1123.22 52.9291 L1123.22 27.2059 L1130.68 27.2059 L1130.68 72.576 L1123.22 72.576 L1123.22 65.6084 Q1120.51 69.7404 1116.9 71.7658 Q1113.34 73.7508 1108.6 73.7508 Q1100.78 73.7508 1096.73 68.8897 Q1092.68 64.0286 1092.68 54.671 M1111.43 26.1121 L1111.43 26.1121 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1172.32 34.1734 Q1171.06 33.4443 1169.56 33.1202 Q1168.11 32.7556 1166.32 32.7556 Q1160 32.7556 1156.6 36.8875 Q1153.24 40.9789 1153.24 48.6757 L1153.24 72.576 L1145.74 72.576 L1145.74 27.2059 L1153.24 27.2059 L1153.24 34.2544 Q1155.59 30.1225 1159.36 28.1376 Q1163.12 26.1121 1168.51 26.1121 Q1169.28 26.1121 1170.21 26.2337 Q1171.14 26.3147 1172.28 26.5172 L1172.32 34.1734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1200.76 49.7694 Q1191.72 49.7694 1188.24 51.8354 Q1184.75 53.9013 1184.75 58.8839 Q1184.75 62.8538 1187.35 65.2034 Q1189.98 67.5124 1194.48 67.5124 Q1200.67 67.5124 1204.4 63.1374 Q1208.17 58.7219 1208.17 51.4303 L1208.17 49.7694 L1200.76 49.7694 M1215.62 46.6907 L1215.62 72.576 L1208.17 72.576 L1208.17 65.6895 Q1205.62 69.8214 1201.81 71.8063 Q1198 73.7508 1192.49 73.7508 Q1185.52 73.7508 1181.39 69.8619 Q1177.3 65.9325 1177.3 59.3701 Q1177.3 51.7138 1182.41 47.825 Q1187.55 43.9361 1197.72 43.9361 L1208.17 43.9361 L1208.17 43.2069 Q1208.17 38.0623 1204.77 35.2672 Q1201.4 32.4315 1195.29 32.4315 Q1191.4 32.4315 1187.71 33.3632 Q1184.03 34.295 1180.62 36.1584 L1180.62 29.2718 Q1184.71 27.692 1188.56 26.9223 Q1192.41 26.1121 1196.06 26.1121 Q1205.9 26.1121 1210.76 31.2163 Q1215.62 36.3204 1215.62 46.6907 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1263.63 28.9478 L1263.63 35.9153 Q1260.47 34.1734 1257.27 33.3227 Q1254.11 32.4315 1250.87 32.4315 Q1243.61 32.4315 1239.6 37.0496 Q1235.59 41.6271 1235.59 49.9314 Q1235.59 58.2358 1239.6 62.8538 Q1243.61 67.4314 1250.87 67.4314 Q1254.11 67.4314 1257.27 66.5807 Q1260.47 65.6895 1263.63 63.9476 L1263.63 70.8341 Q1260.51 72.2924 1257.14 73.0216 Q1253.82 73.7508 1250.06 73.7508 Q1239.81 73.7508 1233.77 67.3098 Q1227.74 60.8689 1227.74 49.9314 Q1227.74 38.832 1233.81 32.472 Q1239.93 26.1121 1250.54 26.1121 Q1253.98 26.1121 1257.27 26.8413 Q1260.55 27.5299 1263.63 28.9478 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1295.47 76.7889 Q1292.31 84.8907 1289.31 87.3618 Q1286.31 89.8329 1281.29 89.8329 L1275.33 89.8329 L1275.33 83.5945 L1279.71 83.5945 Q1282.79 83.5945 1284.49 82.1361 Q1286.19 80.6778 1288.26 75.2496 L1289.59 71.8468 L1271.24 27.2059 L1279.14 27.2059 L1293.32 62.6918 L1307.5 27.2059 L1315.4 27.2059 L1295.47 76.7889 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1378.35 34.1734 Q1377.09 33.4443 1375.59 33.1202 Q1374.13 32.7556 1372.35 32.7556 Q1366.03 32.7556 1362.63 36.8875 Q1359.27 40.9789 1359.27 48.6757 L1359.27 72.576 L1351.77 72.576 L1351.77 27.2059 L1359.27 27.2059 L1359.27 34.2544 Q1361.62 30.1225 1365.38 28.1376 Q1369.15 26.1121 1374.54 26.1121 Q1375.31 26.1121 1376.24 26.2337 Q1377.17 26.3147 1378.31 26.5172 L1378.35 34.1734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1423.15 48.0275 L1423.15 51.6733 L1388.88 51.6733 Q1389.37 59.3701 1393.5 63.421 Q1397.67 67.4314 1405.08 67.4314 Q1409.38 67.4314 1413.39 66.3781 Q1417.44 65.3249 1421.41 63.2184 L1421.41 70.267 Q1417.4 71.9684 1413.19 72.8596 Q1408.97 73.7508 1404.64 73.7508 Q1393.78 73.7508 1387.42 67.4314 Q1381.1 61.1119 1381.1 50.3365 Q1381.1 39.1965 1387.1 32.6746 Q1393.13 26.1121 1403.34 26.1121 Q1412.5 26.1121 1417.8 32.0264 Q1423.15 37.9003 1423.15 48.0275 M1415.7 45.84 Q1415.62 39.7232 1412.25 36.0774 Q1408.93 32.4315 1403.42 32.4315 Q1397.18 32.4315 1393.42 35.9558 Q1389.69 39.4801 1389.12 45.8805 L1415.7 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1464.31 28.5427 L1464.31 35.5912 Q1461.15 33.9709 1457.75 33.1607 Q1454.34 32.3505 1450.7 32.3505 Q1445.15 32.3505 1442.35 34.0519 Q1439.6 35.7533 1439.6 39.156 Q1439.6 41.7486 1441.58 43.2475 Q1443.57 44.7058 1449.56 46.0426 L1452.11 46.6097 Q1460.05 48.3111 1463.38 51.4303 Q1466.74 54.509 1466.74 60.0587 Q1466.74 66.3781 1461.72 70.0644 Q1456.73 73.7508 1447.98 73.7508 Q1444.34 73.7508 1440.37 73.0216 Q1436.44 72.3329 1432.06 70.9151 L1432.06 63.2184 Q1436.19 65.3654 1440.2 66.4591 Q1444.22 67.5124 1448.14 67.5124 Q1453.41 67.5124 1456.25 65.73 Q1459.08 63.9071 1459.08 60.6258 Q1459.08 57.5877 1457.02 55.9673 Q1454.99 54.3469 1448.06 52.8481 L1445.47 52.2405 Q1438.54 50.7821 1435.47 47.7845 Q1432.39 44.7463 1432.39 39.4801 Q1432.39 33.0797 1436.92 29.5959 Q1441.46 26.1121 1449.81 26.1121 Q1453.94 26.1121 1457.58 26.7198 Q1461.23 27.3274 1464.31 28.5427 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1477.84 54.671 L1477.84 27.2059 L1485.29 27.2059 L1485.29 54.3874 Q1485.29 60.8284 1487.8 64.0691 Q1490.31 67.2693 1495.34 67.2693 Q1501.37 67.2693 1504.86 63.421 Q1508.38 59.5726 1508.38 52.9291 L1508.38 27.2059 L1515.84 27.2059 L1515.84 72.576 L1508.38 72.576 L1508.38 65.6084 Q1505.67 69.7404 1502.06 71.7658 Q1498.5 73.7508 1493.76 73.7508 Q1485.94 73.7508 1481.89 68.8897 Q1477.84 64.0286 1477.84 54.671 M1496.59 26.1121 L1496.59 26.1121 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1531.19 9.54393 L1538.64 9.54393 L1538.64 72.576 L1531.19 72.576 L1531.19 9.54393 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1561.61 14.324 L1561.61 27.2059 L1576.96 27.2059 L1576.96 32.9987 L1561.61 32.9987 L1561.61 57.6282 Q1561.61 63.1779 1563.11 64.7578 Q1564.65 66.3376 1569.31 66.3376 L1576.96 66.3376 L1576.96 72.576 L1569.31 72.576 Q1560.68 72.576 1557.4 69.3758 Q1554.12 66.1351 1554.12 57.6282 L1554.12 32.9987 L1548.65 32.9987 L1548.65 27.2059 L1554.12 27.2059 L1554.12 14.324 L1561.61 14.324 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M1615.69 28.5427 L1615.69 35.5912 Q1612.53 33.9709 1609.13 33.1607 Q1605.72 32.3505 1602.08 32.3505 Q1596.53 32.3505 1593.73 34.0519 Q1590.98 35.7533 1590.98 39.156 Q1590.98 41.7486 1592.96 43.2475 Q1594.95 44.7058 1600.94 46.0426 L1603.5 46.6097 Q1611.44 48.3111 1614.76 51.4303 Q1618.12 54.509 1618.12 60.0587 Q1618.12 66.3781 1613.1 70.0644 Q1608.11 73.7508 1599.36 73.7508 Q1595.72 73.7508 1591.75 73.0216 Q1587.82 72.3329 1583.44 70.9151 L1583.44 63.2184 Q1587.58 65.3654 1591.59 66.4591 Q1595.6 67.5124 1599.53 67.5124 Q1604.79 67.5124 1607.63 65.73 Q1610.46 63.9071 1610.46 60.6258 Q1610.46 57.5877 1608.4 55.9673 Q1606.37 54.3469 1599.45 52.8481 L1596.85 52.2405 Q1589.93 50.7821 1586.85 47.7845 Q1583.77 44.7463 1583.77 39.4801 Q1583.77 33.0797 1588.31 29.5959 Q1592.84 26.1121 1601.19 26.1121 Q1605.32 26.1121 1608.97 26.7198 Q1612.61 27.3274 1615.69 28.5427 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip442)\" d=\"M323.245 295.774 L323.245 1423.18 L725.642 1423.18 L725.642 295.774 L323.245 295.774 L323.245 295.774  Z\" fill=\"#add8e6\" fill-rule=\"evenodd\" fill-opacity=\"0.5\"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"323.245,295.774 323.245,1423.18 725.642,1423.18 725.642,295.774 323.245,295.774 \"/>\n",
       "<path clip-path=\"url(#clip442)\" d=\"M826.241 277.132 L826.241 1423.18 L1228.64 1423.18 L1228.64 277.132 L826.241 277.132 L826.241 277.132  Z\" fill=\"#008000\" fill-rule=\"evenodd\" fill-opacity=\"0.5\"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"826.241,277.132 826.241,1423.18 1228.64,1423.18 1228.64,277.132 826.241,277.132 \"/>\n",
       "<path clip-path=\"url(#clip442)\" d=\"M1329.24 288.546 L1329.24 1423.18 L1731.64 1423.18 L1731.64 288.546 L1329.24 288.546 L1329.24 288.546  Z\" fill=\"#ff0000\" fill-rule=\"evenodd\" fill-opacity=\"0.5\"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"1329.24,288.546 1329.24,1423.18 1731.64,1423.18 1731.64,288.546 1329.24,288.546 \"/>\n",
       "<path clip-path=\"url(#clip442)\" d=\"M1832.23 315.304 L1832.23 1423.18 L2234.63 1423.18 L2234.63 315.304 L1832.23 315.304 L1832.23 315.304  Z\" fill=\"#0000ff\" fill-rule=\"evenodd\" fill-opacity=\"0.5\"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"1832.23,315.304 1832.23,1423.18 2234.63,1423.18 2234.63,315.304 1832.23,315.304 \"/>\n",
       "<circle clip-path=\"url(#clip442)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"524.443\" cy=\"295.774\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1027.44\" cy=\"277.132\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1530.44\" cy=\"288.546\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2033.43\" cy=\"315.304\" r=\"2\"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"524.443,468.075 524.443,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"1027.44,367.844 1027.44,186.421 \"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"1530.44,399.274 1530.44,177.818 \"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"2033.43,475.137 2033.43,155.471 \"/>\n",
       "<line clip-path=\"url(#clip442)\" x1=\"540.443\" y1=\"468.075\" x2=\"508.443\" y2=\"468.075\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip442)\" x1=\"540.443\" y1=\"123.472\" x2=\"508.443\" y2=\"123.472\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip442)\" x1=\"1043.44\" y1=\"367.844\" x2=\"1011.44\" y2=\"367.844\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip442)\" x1=\"1043.44\" y1=\"186.421\" x2=\"1011.44\" y2=\"186.421\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip442)\" x1=\"1546.44\" y1=\"399.274\" x2=\"1514.44\" y2=\"399.274\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip442)\" x1=\"1546.44\" y1=\"177.818\" x2=\"1514.44\" y2=\"177.818\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip442)\" x1=\"2049.43\" y1=\"475.137\" x2=\"2017.43\" y2=\"475.137\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip442)\" x1=\"2049.43\" y1=\"155.471\" x2=\"2017.43\" y2=\"155.471\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "</svg>\n"
      ],
      "text/html": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip490\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip490)\" d=\"M0 1600 L2400 1600 L2400 0 L0 0  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip491\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip490)\" d=\"M205.121 1423.18 L2352.76 1423.18 L2352.76 123.472 L205.121 123.472  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip492\">\n",
       "    <rect x=\"205\" y=\"123\" width=\"2149\" height=\"1301\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"524.443,1423.18 524.443,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1027.44,1423.18 1027.44,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1530.44,1423.18 1530.44,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2033.43,1423.18 2033.43,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 2352.76,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"524.443,1423.18 524.443,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1027.44,1423.18 1027.44,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1530.44,1423.18 1530.44,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2033.43,1423.18 2033.43,1404.28 \"/>\n",
       "<path clip-path=\"url(#clip490)\" d=\"M498.853 1452.15 L498.853 1456.71 Q496.191 1455.44 493.83 1454.82 Q491.469 1454.19 489.27 1454.19 Q485.451 1454.19 483.367 1455.67 Q481.307 1457.15 481.307 1459.89 Q481.307 1462.18 482.673 1463.36 Q484.062 1464.52 487.904 1465.23 L490.728 1465.81 Q495.96 1466.81 498.437 1469.33 Q500.937 1471.83 500.937 1476.04 Q500.937 1481.07 497.557 1483.66 Q494.201 1486.25 487.696 1486.25 Q485.242 1486.25 482.464 1485.7 Q479.71 1485.14 476.747 1484.05 L476.747 1479.24 Q479.594 1480.83 482.326 1481.64 Q485.057 1482.45 487.696 1482.45 Q491.701 1482.45 493.876 1480.88 Q496.052 1479.31 496.052 1476.39 Q496.052 1473.84 494.478 1472.41 Q492.927 1470.97 489.363 1470.26 L486.515 1469.7 Q481.284 1468.66 478.946 1466.44 Q476.608 1464.21 476.608 1460.26 Q476.608 1455.67 479.826 1453.03 Q483.066 1450.39 488.738 1450.39 Q491.168 1450.39 493.691 1450.83 Q496.214 1451.27 498.853 1452.15 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M517.14 1485.58 L503.946 1451.02 L508.83 1451.02 L519.779 1480.12 L530.751 1451.02 L535.612 1451.02 L522.441 1485.58 L517.14 1485.58 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M540.659 1451.02 L547.626 1451.02 L556.446 1474.54 L565.311 1451.02 L572.279 1451.02 L572.279 1485.58 L567.719 1485.58 L567.719 1455.23 L558.807 1478.94 L554.108 1478.94 L545.196 1455.23 L545.196 1485.58 L540.659 1485.58 L540.659 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M981.086 1451.02 L985.762 1451.02 L985.762 1465.63 L1001.27 1451.02 L1007.29 1451.02 L990.137 1467.13 L1008.52 1485.58 L1002.36 1485.58 L985.762 1468.94 L985.762 1485.58 L981.086 1485.58 L981.086 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1012.17 1451.02 L1018.47 1451.02 L1033.79 1479.93 L1033.79 1451.02 L1038.33 1451.02 L1038.33 1485.58 L1032.03 1485.58 L1016.71 1456.67 L1016.71 1485.58 L1012.17 1485.58 L1012.17 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1047.64 1451.02 L1053.93 1451.02 L1069.26 1479.93 L1069.26 1451.02 L1073.79 1451.02 L1073.79 1485.58 L1067.5 1485.58 L1052.17 1456.67 L1052.17 1485.58 L1047.64 1485.58 L1047.64 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1504.64 1454.86 L1504.64 1481.74 L1510.29 1481.74 Q1517.44 1481.74 1520.75 1478.5 Q1524.08 1475.26 1524.08 1468.27 Q1524.08 1461.32 1520.75 1458.1 Q1517.44 1454.86 1510.29 1454.86 L1504.64 1454.86 M1499.96 1451.02 L1509.57 1451.02 Q1519.62 1451.02 1524.31 1455.21 Q1529.01 1459.38 1529.01 1468.27 Q1529.01 1477.2 1524.29 1481.39 Q1519.57 1485.58 1509.57 1485.58 L1499.96 1485.58 L1499.96 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1531.68 1451.02 L1560.91 1451.02 L1560.91 1454.96 L1548.64 1454.96 L1548.64 1485.58 L1543.94 1485.58 L1543.94 1454.96 L1531.68 1454.96 L1531.68 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M2000.1 1455.63 L1993.76 1472.83 L2006.47 1472.83 L2000.1 1455.63 M1997.46 1451.02 L2002.76 1451.02 L2015.93 1485.58 L2011.07 1485.58 L2007.92 1476.71 L1992.35 1476.71 L1989.2 1485.58 L1984.27 1485.58 L1997.46 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M2020.98 1451.02 L2027.28 1451.02 L2042.6 1479.93 L2042.6 1451.02 L2047.14 1451.02 L2047.14 1485.58 L2040.84 1485.58 L2025.52 1456.67 L2025.52 1485.58 L2020.98 1485.58 L2020.98 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M2056.44 1451.02 L2062.74 1451.02 L2078.06 1479.93 L2078.06 1451.02 L2082.6 1451.02 L2082.6 1485.58 L2076.3 1485.58 L2060.98 1456.67 L2060.98 1485.58 L2056.44 1485.58 L2056.44 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1109.5 1520.52 L1139.55 1520.52 L1139.55 1525.93 L1115.93 1525.93 L1115.93 1540 L1138.56 1540 L1138.56 1545.41 L1115.93 1545.41 L1115.93 1562.63 L1140.12 1562.63 L1140.12 1568.04 L1109.5 1568.04 L1109.5 1520.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1173.16 1533.45 L1173.16 1538.98 Q1170.67 1537.71 1168 1537.07 Q1165.33 1536.44 1162.46 1536.44 Q1158.1 1536.44 1155.91 1537.77 Q1153.74 1539.11 1153.74 1541.79 Q1153.74 1543.82 1155.3 1545 Q1156.86 1546.15 1161.57 1547.2 L1163.58 1547.64 Q1169.81 1548.98 1172.42 1551.43 Q1175.07 1553.85 1175.07 1558.21 Q1175.07 1563.17 1171.12 1566.07 Q1167.2 1568.97 1160.33 1568.97 Q1157.46 1568.97 1154.35 1568.39 Q1151.26 1567.85 1147.82 1566.74 L1147.82 1560.69 Q1151.07 1562.38 1154.22 1563.24 Q1157.37 1564.07 1160.46 1564.07 Q1164.59 1564.07 1166.82 1562.66 Q1169.05 1561.23 1169.05 1558.65 Q1169.05 1556.27 1167.43 1554.99 Q1165.84 1553.72 1160.39 1552.54 L1158.36 1552.07 Q1152.91 1550.92 1150.49 1548.56 Q1148.08 1546.18 1148.08 1542.04 Q1148.08 1537.01 1151.64 1534.27 Q1155.21 1531.54 1161.76 1531.54 Q1165.01 1531.54 1167.87 1532.01 Q1170.74 1532.49 1173.16 1533.45 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1190.18 1522.27 L1190.18 1532.4 L1202.25 1532.4 L1202.25 1536.95 L1190.18 1536.95 L1190.18 1556.3 Q1190.18 1560.66 1191.36 1561.9 Q1192.57 1563.14 1196.23 1563.14 L1202.25 1563.14 L1202.25 1568.04 L1196.23 1568.04 Q1189.45 1568.04 1186.87 1565.53 Q1184.3 1562.98 1184.3 1556.3 L1184.3 1536.95 L1180 1536.95 L1180 1532.4 L1184.3 1532.4 L1184.3 1522.27 L1190.18 1522.27 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1209.95 1532.4 L1215.81 1532.4 L1215.81 1568.04 L1209.95 1568.04 L1209.95 1532.4 M1209.95 1518.52 L1215.81 1518.52 L1215.81 1525.93 L1209.95 1525.93 L1209.95 1518.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1255.82 1539.24 Q1258.01 1535.29 1261.07 1533.41 Q1264.12 1531.54 1268.26 1531.54 Q1273.83 1531.54 1276.85 1535.45 Q1279.88 1539.33 1279.88 1546.53 L1279.88 1568.04 L1273.99 1568.04 L1273.99 1546.72 Q1273.99 1541.59 1272.17 1539.11 Q1270.36 1536.63 1266.64 1536.63 Q1262.09 1536.63 1259.44 1539.65 Q1256.8 1542.68 1256.8 1547.9 L1256.8 1568.04 L1250.91 1568.04 L1250.91 1546.72 Q1250.91 1541.56 1249.1 1539.11 Q1247.28 1536.63 1243.5 1536.63 Q1239.01 1536.63 1236.37 1539.68 Q1233.73 1542.71 1233.73 1547.9 L1233.73 1568.04 L1227.84 1568.04 L1227.84 1532.4 L1233.73 1532.4 L1233.73 1537.93 Q1235.73 1534.66 1238.53 1533.1 Q1241.33 1531.54 1245.18 1531.54 Q1249.07 1531.54 1251.77 1533.51 Q1254.51 1535.48 1255.82 1539.24 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1307.76 1550.12 Q1300.66 1550.12 1297.92 1551.75 Q1295.19 1553.37 1295.19 1557.29 Q1295.19 1560.4 1297.22 1562.25 Q1299.29 1564.07 1302.83 1564.07 Q1307.7 1564.07 1310.62 1560.63 Q1313.58 1557.16 1313.58 1551.43 L1313.58 1550.12 L1307.76 1550.12 M1319.44 1547.71 L1319.44 1568.04 L1313.58 1568.04 L1313.58 1562.63 Q1311.58 1565.88 1308.59 1567.44 Q1305.59 1568.97 1301.27 1568.97 Q1295.79 1568.97 1292.55 1565.91 Q1289.33 1562.82 1289.33 1557.67 Q1289.33 1551.65 1293.34 1548.6 Q1297.38 1545.54 1305.37 1545.54 L1313.58 1545.54 L1313.58 1544.97 Q1313.58 1540.93 1310.91 1538.73 Q1308.27 1536.5 1303.46 1536.5 Q1300.41 1536.5 1297.51 1537.23 Q1294.61 1537.97 1291.94 1539.43 L1291.94 1534.02 Q1295.16 1532.78 1298.18 1532.17 Q1301.2 1531.54 1304.07 1531.54 Q1311.8 1531.54 1315.62 1535.55 Q1319.44 1539.56 1319.44 1547.71 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1337.3 1522.27 L1337.3 1532.4 L1349.36 1532.4 L1349.36 1536.95 L1337.3 1536.95 L1337.3 1556.3 Q1337.3 1560.66 1338.47 1561.9 Q1339.68 1563.14 1343.34 1563.14 L1349.36 1563.14 L1349.36 1568.04 L1343.34 1568.04 Q1336.56 1568.04 1333.99 1565.53 Q1331.41 1562.98 1331.41 1556.3 L1331.41 1536.95 L1327.11 1536.95 L1327.11 1532.4 L1331.41 1532.4 L1331.41 1522.27 L1337.3 1522.27 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1370.88 1536.5 Q1366.16 1536.5 1363.43 1540.19 Q1360.69 1543.85 1360.69 1550.25 Q1360.69 1556.65 1363.4 1560.34 Q1366.13 1564 1370.88 1564 Q1375.55 1564 1378.29 1560.31 Q1381.03 1556.62 1381.03 1550.25 Q1381.03 1543.92 1378.29 1540.23 Q1375.55 1536.5 1370.88 1536.5 M1370.88 1531.54 Q1378.51 1531.54 1382.87 1536.5 Q1387.24 1541.47 1387.24 1550.25 Q1387.24 1559 1382.87 1564 Q1378.51 1568.97 1370.88 1568.97 Q1363.2 1568.97 1358.84 1564 Q1354.52 1559 1354.52 1550.25 Q1354.52 1541.47 1358.84 1536.5 Q1363.2 1531.54 1370.88 1531.54 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1417.6 1537.87 Q1416.61 1537.3 1415.44 1537.04 Q1414.29 1536.76 1412.89 1536.76 Q1407.92 1536.76 1405.25 1540 Q1402.61 1543.22 1402.61 1549.27 L1402.61 1568.04 L1396.72 1568.04 L1396.72 1532.4 L1402.61 1532.4 L1402.61 1537.93 Q1404.45 1534.69 1407.41 1533.13 Q1410.37 1531.54 1414.61 1531.54 Q1415.21 1531.54 1415.94 1531.63 Q1416.68 1531.7 1417.57 1531.85 L1417.6 1537.87 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1446.47 1533.45 L1446.47 1538.98 Q1443.99 1537.71 1441.31 1537.07 Q1438.64 1536.44 1435.77 1536.44 Q1431.41 1536.44 1429.22 1537.77 Q1427.05 1539.11 1427.05 1541.79 Q1427.05 1543.82 1428.61 1545 Q1430.17 1546.15 1434.88 1547.2 L1436.89 1547.64 Q1443.13 1548.98 1445.74 1551.43 Q1448.38 1553.85 1448.38 1558.21 Q1448.38 1563.17 1444.43 1566.07 Q1440.52 1568.97 1433.64 1568.97 Q1430.78 1568.97 1427.66 1568.39 Q1424.57 1567.85 1421.13 1566.74 L1421.13 1560.69 Q1424.38 1562.38 1427.53 1563.24 Q1430.68 1564.07 1433.77 1564.07 Q1437.91 1564.07 1440.13 1562.66 Q1442.36 1561.23 1442.36 1558.65 Q1442.36 1556.27 1440.74 1554.99 Q1439.15 1553.72 1433.7 1552.54 L1431.67 1552.07 Q1426.22 1550.92 1423.81 1548.56 Q1421.39 1546.18 1421.39 1542.04 Q1421.39 1537.01 1424.95 1534.27 Q1428.52 1531.54 1435.07 1531.54 Q1438.32 1531.54 1441.18 1532.01 Q1444.05 1532.49 1446.47 1533.45 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,1423.18 2352.76,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,1158.18 2352.76,1158.18 \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,893.189 2352.76,893.189 \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,628.194 2352.76,628.194 \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,363.198 2352.76,363.198 \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 205.121,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 224.019,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1158.18 224.019,1158.18 \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,893.189 224.019,893.189 \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,628.194 224.019,628.194 \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,363.198 224.019,363.198 \"/>\n",
       "<path clip-path=\"url(#clip490)\" d=\"M157.177 1408.98 Q153.566 1408.98 151.737 1412.54 Q149.931 1416.08 149.931 1423.21 Q149.931 1430.32 151.737 1433.89 Q153.566 1437.43 157.177 1437.43 Q160.811 1437.43 162.616 1433.89 Q164.445 1430.32 164.445 1423.21 Q164.445 1416.08 162.616 1412.54 Q160.811 1408.98 157.177 1408.98 M157.177 1405.27 Q162.987 1405.27 166.042 1409.88 Q169.121 1414.46 169.121 1423.21 Q169.121 1431.94 166.042 1436.55 Q162.987 1441.13 157.177 1441.13 Q151.366 1441.13 148.288 1436.55 Q145.232 1431.94 145.232 1423.21 Q145.232 1414.46 148.288 1409.88 Q151.366 1405.27 157.177 1405.27 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M121.043 1171.53 L137.362 1171.53 L137.362 1175.46 L115.418 1175.46 L115.418 1171.53 Q118.08 1168.77 122.663 1164.15 Q127.269 1159.49 128.45 1158.15 Q130.695 1155.63 131.575 1153.89 Q132.478 1152.13 132.478 1150.44 Q132.478 1147.69 130.533 1145.95 Q128.612 1144.21 125.51 1144.21 Q123.311 1144.21 120.857 1144.98 Q118.427 1145.74 115.649 1147.29 L115.649 1142.57 Q118.473 1141.44 120.927 1140.86 Q123.38 1140.28 125.418 1140.28 Q130.788 1140.28 133.982 1142.96 Q137.177 1145.65 137.177 1150.14 Q137.177 1152.27 136.367 1154.19 Q135.579 1156.09 133.473 1158.68 Q132.894 1159.35 129.792 1162.57 Q126.691 1165.77 121.043 1171.53 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M157.177 1143.98 Q153.566 1143.98 151.737 1147.55 Q149.931 1151.09 149.931 1158.22 Q149.931 1165.33 151.737 1168.89 Q153.566 1172.43 157.177 1172.43 Q160.811 1172.43 162.616 1168.89 Q164.445 1165.33 164.445 1158.22 Q164.445 1151.09 162.616 1147.55 Q160.811 1143.98 157.177 1143.98 M157.177 1140.28 Q162.987 1140.28 166.042 1144.89 Q169.121 1149.47 169.121 1158.22 Q169.121 1166.95 166.042 1171.55 Q162.987 1176.14 157.177 1176.14 Q151.366 1176.14 148.288 1171.55 Q145.232 1166.95 145.232 1158.22 Q145.232 1149.47 148.288 1144.89 Q151.366 1140.28 157.177 1140.28 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M129.862 879.983 L118.056 898.432 L129.862 898.432 L129.862 879.983 M128.635 875.909 L134.515 875.909 L134.515 898.432 L139.445 898.432 L139.445 902.321 L134.515 902.321 L134.515 910.469 L129.862 910.469 L129.862 902.321 L114.26 902.321 L114.26 897.807 L128.635 875.909 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M157.177 878.988 Q153.566 878.988 151.737 882.553 Q149.931 886.094 149.931 893.224 Q149.931 900.33 151.737 903.895 Q153.566 907.437 157.177 907.437 Q160.811 907.437 162.616 903.895 Q164.445 900.33 164.445 893.224 Q164.445 886.094 162.616 882.553 Q160.811 878.988 157.177 878.988 M157.177 875.284 Q162.987 875.284 166.042 879.891 Q169.121 884.474 169.121 893.224 Q169.121 901.951 166.042 906.557 Q162.987 911.14 157.177 911.14 Q151.366 911.14 148.288 906.557 Q145.232 901.951 145.232 893.224 Q145.232 884.474 148.288 879.891 Q151.366 875.284 157.177 875.284 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M127.593 626.33 Q124.445 626.33 122.593 628.483 Q120.765 630.636 120.765 634.386 Q120.765 638.113 122.593 640.289 Q124.445 642.441 127.593 642.441 Q130.742 642.441 132.57 640.289 Q134.422 638.113 134.422 634.386 Q134.422 630.636 132.57 628.483 Q130.742 626.33 127.593 626.33 M136.876 611.678 L136.876 615.937 Q135.117 615.103 133.311 614.664 Q131.529 614.224 129.769 614.224 Q125.14 614.224 122.686 617.349 Q120.255 620.474 119.908 626.793 Q121.274 624.779 123.334 623.715 Q125.394 622.627 127.871 622.627 Q133.08 622.627 136.089 625.798 Q139.121 628.946 139.121 634.386 Q139.121 639.71 135.973 642.927 Q132.825 646.145 127.593 646.145 Q121.598 646.145 118.427 641.562 Q115.256 636.955 115.256 628.228 Q115.256 620.034 119.144 615.173 Q123.033 610.289 129.584 610.289 Q131.343 610.289 133.126 610.636 Q134.931 610.983 136.876 611.678 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M157.177 613.992 Q153.566 613.992 151.737 617.557 Q149.931 621.099 149.931 628.228 Q149.931 635.335 151.737 638.9 Q153.566 642.441 157.177 642.441 Q160.811 642.441 162.616 638.9 Q164.445 635.335 164.445 628.228 Q164.445 621.099 162.616 617.557 Q160.811 613.992 157.177 613.992 M157.177 610.289 Q162.987 610.289 166.042 614.895 Q169.121 619.478 169.121 628.228 Q169.121 636.955 166.042 641.562 Q162.987 646.145 157.177 646.145 Q151.366 646.145 148.288 641.562 Q145.232 636.955 145.232 628.228 Q145.232 619.478 148.288 614.895 Q151.366 610.289 157.177 610.289 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M127.015 364.066 Q123.681 364.066 121.76 365.849 Q119.862 367.631 119.862 370.756 Q119.862 373.881 121.76 375.664 Q123.681 377.446 127.015 377.446 Q130.348 377.446 132.269 375.664 Q134.191 373.858 134.191 370.756 Q134.191 367.631 132.269 365.849 Q130.371 364.066 127.015 364.066 M122.339 362.076 Q119.33 361.335 117.64 359.275 Q115.973 357.215 115.973 354.252 Q115.973 350.108 118.913 347.701 Q121.876 345.293 127.015 345.293 Q132.177 345.293 135.117 347.701 Q138.056 350.108 138.056 354.252 Q138.056 357.215 136.367 359.275 Q134.7 361.335 131.714 362.076 Q135.093 362.863 136.968 365.154 Q138.867 367.446 138.867 370.756 Q138.867 375.779 135.788 378.464 Q132.732 381.15 127.015 381.15 Q121.297 381.15 118.218 378.464 Q115.163 375.779 115.163 370.756 Q115.163 367.446 117.061 365.154 Q118.959 362.863 122.339 362.076 M120.626 354.691 Q120.626 357.377 122.293 358.881 Q123.982 360.386 127.015 360.386 Q130.024 360.386 131.714 358.881 Q133.427 357.377 133.427 354.691 Q133.427 352.006 131.714 350.502 Q130.024 348.997 127.015 348.997 Q123.982 348.997 122.293 350.502 Q120.626 352.006 120.626 354.691 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M157.177 348.997 Q153.566 348.997 151.737 352.562 Q149.931 356.103 149.931 363.233 Q149.931 370.339 151.737 373.904 Q153.566 377.446 157.177 377.446 Q160.811 377.446 162.616 373.904 Q164.445 370.339 164.445 363.233 Q164.445 356.103 162.616 352.562 Q160.811 348.997 157.177 348.997 M157.177 345.293 Q162.987 345.293 166.042 349.9 Q169.121 354.483 169.121 363.233 Q169.121 371.96 166.042 376.566 Q162.987 381.15 157.177 381.15 Q151.366 381.15 148.288 376.566 Q145.232 371.96 145.232 363.233 Q145.232 354.483 148.288 349.9 Q151.366 345.293 157.177 345.293 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M21.7677 1011.01 L39.6235 1011.01 L39.6235 1002.92 Q39.6235 998.433 37.3 995.982 Q34.9765 993.532 30.6797 993.532 Q26.4147 993.532 24.0912 995.982 Q21.7677 998.433 21.7677 1002.92 L21.7677 1011.01 M16.4842 1017.43 L16.4842 1002.92 Q16.4842 994.932 20.1126 990.858 Q23.7092 986.752 30.6797 986.752 Q37.7138 986.752 41.3104 990.858 Q44.907 994.932 44.907 1002.92 L44.907 1011.01 L64.0042 1011.01 L64.0042 1017.43 L16.4842 1017.43 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M44.7161 950.213 L47.5806 950.213 L47.5806 977.14 Q53.6281 976.758 56.8109 973.512 Q59.9619 970.233 59.9619 964.409 Q59.9619 961.035 59.1344 957.884 Q58.3069 954.701 56.6518 951.582 L62.1899 951.582 Q63.5267 954.733 64.227 958.043 Q64.9272 961.353 64.9272 964.759 Q64.9272 973.289 59.9619 978.286 Q54.9967 983.251 46.5303 983.251 Q37.7774 983.251 32.6531 978.54 Q27.4968 973.798 27.4968 965.777 Q27.4968 958.584 32.1438 954.414 Q36.7589 950.213 44.7161 950.213 M42.9973 956.07 Q38.1912 956.133 35.3266 958.775 Q32.4621 961.385 32.4621 965.714 Q32.4621 970.615 35.2312 973.575 Q38.0002 976.503 43.0292 976.949 L42.9973 956.07 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M33.8307 919.944 Q33.2578 920.931 33.0032 922.108 Q32.7167 923.254 32.7167 924.655 Q32.7167 929.62 35.9632 932.294 Q39.1779 934.935 45.2253 934.935 L64.0042 934.935 L64.0042 940.824 L28.3562 940.824 L28.3562 934.935 L33.8944 934.935 Q30.6479 933.089 29.0883 930.129 Q27.4968 927.169 27.4968 922.936 Q27.4968 922.331 27.5923 921.599 Q27.656 920.867 27.8151 919.976 L33.8307 919.944 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M29.7248 889.58 L35.1993 889.58 Q33.8307 892.062 33.1623 894.577 Q32.4621 897.059 32.4621 899.606 Q32.4621 905.303 36.0905 908.454 Q39.6872 911.605 46.212 911.605 Q52.7369 911.605 56.3653 908.454 Q59.9619 905.303 59.9619 899.606 Q59.9619 897.059 59.2935 894.577 Q58.5933 892.062 57.2247 889.58 L62.6355 889.58 Q63.7814 892.03 64.3543 894.672 Q64.9272 897.282 64.9272 900.242 Q64.9272 908.295 59.8664 913.037 Q54.8057 917.78 46.212 917.78 Q37.491 917.78 32.4939 913.006 Q27.4968 908.199 27.4968 899.86 Q27.4968 897.155 28.0697 894.577 Q28.6108 891.999 29.7248 889.58 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M44.7161 848.903 L47.5806 848.903 L47.5806 875.83 Q53.6281 875.448 56.8109 872.201 Q59.9619 868.923 59.9619 863.098 Q59.9619 859.725 59.1344 856.574 Q58.3069 853.391 56.6518 850.271 L62.1899 850.271 Q63.5267 853.422 64.227 856.733 Q64.9272 860.043 64.9272 863.448 Q64.9272 871.979 59.9619 876.976 Q54.9967 881.941 46.5303 881.941 Q37.7774 881.941 32.6531 877.23 Q27.4968 872.488 27.4968 864.467 Q27.4968 857.274 32.1438 853.104 Q36.7589 848.903 44.7161 848.903 M42.9973 854.759 Q38.1912 854.823 35.3266 857.465 Q32.4621 860.075 32.4621 864.403 Q32.4621 869.305 35.2312 872.265 Q38.0002 875.193 43.0292 875.639 L42.9973 854.759 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M42.4881 809.658 L64.0042 809.658 L64.0042 815.515 L42.679 815.515 Q37.6183 815.515 35.1038 817.488 Q32.5894 819.461 32.5894 823.408 Q32.5894 828.151 35.6131 830.888 Q38.6368 833.625 43.8567 833.625 L64.0042 833.625 L64.0042 839.513 L28.3562 839.513 L28.3562 833.625 L33.8944 833.625 Q30.6797 831.524 29.0883 828.692 Q27.4968 825.827 27.4968 822.103 Q27.4968 815.96 31.3163 812.809 Q35.1038 809.658 42.4881 809.658 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M18.2347 792.184 L28.3562 792.184 L28.3562 780.121 L32.9077 780.121 L32.9077 792.184 L52.2594 792.184 Q56.6199 792.184 57.8613 791.007 Q59.1026 789.797 59.1026 786.137 L59.1026 780.121 L64.0042 780.121 L64.0042 786.137 Q64.0042 792.916 61.4897 795.495 Q58.9434 798.073 52.2594 798.073 L32.9077 798.073 L32.9077 802.37 L28.3562 802.37 L28.3562 798.073 L18.2347 798.073 L18.2347 792.184 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M46.0847 756.218 Q46.0847 763.316 47.7079 766.053 Q49.3312 768.79 53.2461 768.79 Q56.3653 768.79 58.2114 766.753 Q60.0256 764.685 60.0256 761.152 Q60.0256 756.282 56.5881 753.354 Q53.1188 750.394 47.3897 750.394 L46.0847 750.394 L46.0847 756.218 M43.6657 744.537 L64.0042 744.537 L64.0042 750.394 L58.5933 750.394 Q61.8398 752.399 63.3994 755.391 Q64.9272 758.382 64.9272 762.711 Q64.9272 768.186 61.8716 771.432 Q58.7843 774.647 53.6281 774.647 Q47.6125 774.647 44.5569 770.636 Q41.5014 766.594 41.5014 758.605 L41.5014 750.394 L40.9285 750.394 Q36.8862 750.394 34.6901 753.067 Q32.4621 755.709 32.4621 760.515 Q32.4621 763.571 33.1941 766.467 Q33.9262 769.363 35.3903 772.037 L29.9795 772.037 Q28.7381 768.822 28.1334 765.799 Q27.4968 762.775 27.4968 759.91 Q27.4968 752.176 31.5072 748.356 Q35.5176 744.537 43.6657 744.537 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M45.7664 709.016 Q39.4007 709.016 35.8996 711.658 Q32.3984 714.268 32.3984 719.011 Q32.3984 723.721 35.8996 726.363 Q39.4007 728.973 45.7664 728.973 Q52.1003 728.973 55.6014 726.363 Q59.1026 723.721 59.1026 719.011 Q59.1026 714.268 55.6014 711.658 Q52.1003 709.016 45.7664 709.016 M59.58 703.16 Q68.683 703.16 73.1071 707.202 Q77.5631 711.244 77.5631 719.584 Q77.5631 722.671 77.0857 725.408 Q76.6401 728.145 75.6852 730.723 L69.9879 730.723 Q71.3884 728.145 72.0568 725.631 Q72.7252 723.116 72.7252 720.507 Q72.7252 714.746 69.7015 711.881 Q66.7096 709.016 60.6303 709.016 L57.7339 709.016 Q60.885 710.831 62.4446 713.663 Q64.0042 716.496 64.0042 720.443 Q64.0042 727 59.0071 731.01 Q54.01 735.02 45.7664 735.02 Q37.491 735.02 32.4939 731.01 Q27.4968 727 27.4968 720.443 Q27.4968 716.496 29.0564 713.663 Q30.616 710.831 33.7671 709.016 L28.3562 709.016 L28.3562 703.16 L59.58 703.16 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M44.7161 660.605 L47.5806 660.605 L47.5806 687.532 Q53.6281 687.15 56.8109 683.904 Q59.9619 680.625 59.9619 674.801 Q59.9619 671.427 59.1344 668.276 Q58.3069 665.093 56.6518 661.974 L62.1899 661.974 Q63.5267 665.125 64.227 668.435 Q64.9272 671.745 64.9272 675.151 Q64.9272 683.681 59.9619 688.678 Q54.9967 693.643 46.5303 693.643 Q37.7774 693.643 32.6531 688.933 Q27.4968 684.19 27.4968 676.169 Q27.4968 668.976 32.1438 664.807 Q36.7589 660.605 44.7161 660.605 M42.9973 666.462 Q38.1912 666.525 35.3266 669.167 Q32.4621 671.777 32.4621 676.106 Q32.4621 681.007 35.2312 683.967 Q38.0002 686.896 43.0292 687.341 L42.9973 666.462 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M14.5426 616.204 Q21.8632 620.469 29.0246 622.538 Q36.186 624.607 43.5384 624.607 Q50.8908 624.607 58.1159 622.538 Q65.3091 620.438 72.5979 616.204 L72.5979 621.297 Q65.1182 626.071 57.8931 628.458 Q50.668 630.814 43.5384 630.814 Q36.4406 630.814 29.2474 628.458 Q22.0542 626.103 14.5426 621.297 L14.5426 616.204 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M43.0928 563.592 Q43.0928 566.361 45.4481 567.952 Q47.8034 569.512 52.0048 569.512 Q56.1425 569.512 58.5296 567.952 Q60.885 566.361 60.885 563.592 Q60.885 560.886 58.5296 559.327 Q56.1425 557.735 52.0048 557.735 Q47.8353 557.735 45.48 559.327 Q43.0928 560.886 43.0928 563.592 M39.0506 563.592 Q39.0506 558.563 42.5517 555.603 Q46.0529 552.643 52.0048 552.643 Q57.9567 552.643 61.4579 555.635 Q64.9272 558.595 64.9272 563.592 Q64.9272 568.684 61.4579 571.644 Q57.9567 574.605 52.0048 574.605 Q46.021 574.605 42.5517 571.644 Q39.0506 568.653 39.0506 563.592 M19.667 596.439 Q19.667 599.176 22.0542 600.768 Q24.4095 602.327 28.5472 602.327 Q32.7485 602.327 35.1038 600.768 Q37.4592 599.208 37.4592 596.439 Q37.4592 593.67 35.1038 592.11 Q32.7485 590.519 28.5472 590.519 Q24.4413 590.519 22.0542 592.11 Q19.667 593.702 19.667 596.439 M15.6248 567.698 L15.6248 562.605 L64.9272 592.333 L64.9272 597.426 L15.6248 567.698 M15.6248 596.439 Q15.6248 591.41 19.1259 588.418 Q22.5952 585.426 28.5472 585.426 Q34.5628 585.426 38.0321 588.418 Q41.5014 591.378 41.5014 596.439 Q41.5014 601.5 38.0321 604.46 Q34.5309 607.388 28.5472 607.388 Q22.6271 607.388 19.1259 604.428 Q15.6248 601.468 15.6248 596.439 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M14.5426 543.826 L14.5426 538.734 Q22.0542 533.96 29.2474 531.604 Q36.4406 529.217 43.5384 529.217 Q50.668 529.217 57.8931 531.604 Q65.1182 533.96 72.5979 538.734 L72.5979 543.826 Q65.3091 539.593 58.1159 537.524 Q50.8908 535.424 43.5384 535.424 Q36.186 535.424 29.0246 537.524 Q21.8632 539.593 14.5426 543.826 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M967.464 20.1573 L956.365 50.2555 L978.604 50.2555 L967.464 20.1573 M962.846 12.096 L972.123 12.096 L995.173 72.576 L986.666 72.576 L981.156 57.061 L953.894 57.061 L948.385 72.576 L939.756 72.576 L962.846 12.096 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1034.87 28.9478 L1034.87 35.9153 Q1031.71 34.1734 1028.51 33.3227 Q1025.35 32.4315 1022.11 32.4315 Q1014.86 32.4315 1010.85 37.0496 Q1006.84 41.6271 1006.84 49.9314 Q1006.84 58.2358 1010.85 62.8538 Q1014.86 67.4314 1022.11 67.4314 Q1025.35 67.4314 1028.51 66.5807 Q1031.71 65.6895 1034.87 63.9476 L1034.87 70.8341 Q1031.75 72.2924 1028.39 73.0216 Q1025.07 73.7508 1021.3 73.7508 Q1011.05 73.7508 1005.02 67.3098 Q998.98 60.8689 998.98 49.9314 Q998.98 38.832 1005.06 32.472 Q1011.17 26.1121 1021.79 26.1121 Q1025.23 26.1121 1028.51 26.8413 Q1031.79 27.5299 1034.87 28.9478 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1080.48 28.9478 L1080.48 35.9153 Q1077.32 34.1734 1074.12 33.3227 Q1070.97 32.4315 1067.72 32.4315 Q1060.47 32.4315 1056.46 37.0496 Q1052.45 41.6271 1052.45 49.9314 Q1052.45 58.2358 1056.46 62.8538 Q1060.47 67.4314 1067.72 67.4314 Q1070.97 67.4314 1074.12 66.5807 Q1077.32 65.6895 1080.48 63.9476 L1080.48 70.8341 Q1077.37 72.2924 1074 73.0216 Q1070.68 73.7508 1066.91 73.7508 Q1056.67 73.7508 1050.63 67.3098 Q1044.59 60.8689 1044.59 49.9314 Q1044.59 38.832 1050.67 32.472 Q1056.79 26.1121 1067.4 26.1121 Q1070.84 26.1121 1074.12 26.8413 Q1077.41 27.5299 1080.48 28.9478 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1092.68 54.671 L1092.68 27.2059 L1100.13 27.2059 L1100.13 54.3874 Q1100.13 60.8284 1102.64 64.0691 Q1105.15 67.2693 1110.18 67.2693 Q1116.21 67.2693 1119.7 63.421 Q1123.22 59.5726 1123.22 52.9291 L1123.22 27.2059 L1130.68 27.2059 L1130.68 72.576 L1123.22 72.576 L1123.22 65.6084 Q1120.51 69.7404 1116.9 71.7658 Q1113.34 73.7508 1108.6 73.7508 Q1100.78 73.7508 1096.73 68.8897 Q1092.68 64.0286 1092.68 54.671 M1111.43 26.1121 L1111.43 26.1121 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1172.32 34.1734 Q1171.06 33.4443 1169.56 33.1202 Q1168.11 32.7556 1166.32 32.7556 Q1160 32.7556 1156.6 36.8875 Q1153.24 40.9789 1153.24 48.6757 L1153.24 72.576 L1145.74 72.576 L1145.74 27.2059 L1153.24 27.2059 L1153.24 34.2544 Q1155.59 30.1225 1159.36 28.1376 Q1163.12 26.1121 1168.51 26.1121 Q1169.28 26.1121 1170.21 26.2337 Q1171.14 26.3147 1172.28 26.5172 L1172.32 34.1734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1200.76 49.7694 Q1191.72 49.7694 1188.24 51.8354 Q1184.75 53.9013 1184.75 58.8839 Q1184.75 62.8538 1187.35 65.2034 Q1189.98 67.5124 1194.48 67.5124 Q1200.67 67.5124 1204.4 63.1374 Q1208.17 58.7219 1208.17 51.4303 L1208.17 49.7694 L1200.76 49.7694 M1215.62 46.6907 L1215.62 72.576 L1208.17 72.576 L1208.17 65.6895 Q1205.62 69.8214 1201.81 71.8063 Q1198 73.7508 1192.49 73.7508 Q1185.52 73.7508 1181.39 69.8619 Q1177.3 65.9325 1177.3 59.3701 Q1177.3 51.7138 1182.41 47.825 Q1187.55 43.9361 1197.72 43.9361 L1208.17 43.9361 L1208.17 43.2069 Q1208.17 38.0623 1204.77 35.2672 Q1201.4 32.4315 1195.29 32.4315 Q1191.4 32.4315 1187.71 33.3632 Q1184.03 34.295 1180.62 36.1584 L1180.62 29.2718 Q1184.71 27.692 1188.56 26.9223 Q1192.41 26.1121 1196.06 26.1121 Q1205.9 26.1121 1210.76 31.2163 Q1215.62 36.3204 1215.62 46.6907 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1263.63 28.9478 L1263.63 35.9153 Q1260.47 34.1734 1257.27 33.3227 Q1254.11 32.4315 1250.87 32.4315 Q1243.61 32.4315 1239.6 37.0496 Q1235.59 41.6271 1235.59 49.9314 Q1235.59 58.2358 1239.6 62.8538 Q1243.61 67.4314 1250.87 67.4314 Q1254.11 67.4314 1257.27 66.5807 Q1260.47 65.6895 1263.63 63.9476 L1263.63 70.8341 Q1260.51 72.2924 1257.14 73.0216 Q1253.82 73.7508 1250.06 73.7508 Q1239.81 73.7508 1233.77 67.3098 Q1227.74 60.8689 1227.74 49.9314 Q1227.74 38.832 1233.81 32.472 Q1239.93 26.1121 1250.54 26.1121 Q1253.98 26.1121 1257.27 26.8413 Q1260.55 27.5299 1263.63 28.9478 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1295.47 76.7889 Q1292.31 84.8907 1289.31 87.3618 Q1286.31 89.8329 1281.29 89.8329 L1275.33 89.8329 L1275.33 83.5945 L1279.71 83.5945 Q1282.79 83.5945 1284.49 82.1361 Q1286.19 80.6778 1288.26 75.2496 L1289.59 71.8468 L1271.24 27.2059 L1279.14 27.2059 L1293.32 62.6918 L1307.5 27.2059 L1315.4 27.2059 L1295.47 76.7889 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1378.35 34.1734 Q1377.09 33.4443 1375.59 33.1202 Q1374.13 32.7556 1372.35 32.7556 Q1366.03 32.7556 1362.63 36.8875 Q1359.27 40.9789 1359.27 48.6757 L1359.27 72.576 L1351.77 72.576 L1351.77 27.2059 L1359.27 27.2059 L1359.27 34.2544 Q1361.62 30.1225 1365.38 28.1376 Q1369.15 26.1121 1374.54 26.1121 Q1375.31 26.1121 1376.24 26.2337 Q1377.17 26.3147 1378.31 26.5172 L1378.35 34.1734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1423.15 48.0275 L1423.15 51.6733 L1388.88 51.6733 Q1389.37 59.3701 1393.5 63.421 Q1397.67 67.4314 1405.08 67.4314 Q1409.38 67.4314 1413.39 66.3781 Q1417.44 65.3249 1421.41 63.2184 L1421.41 70.267 Q1417.4 71.9684 1413.19 72.8596 Q1408.97 73.7508 1404.64 73.7508 Q1393.78 73.7508 1387.42 67.4314 Q1381.1 61.1119 1381.1 50.3365 Q1381.1 39.1965 1387.1 32.6746 Q1393.13 26.1121 1403.34 26.1121 Q1412.5 26.1121 1417.8 32.0264 Q1423.15 37.9003 1423.15 48.0275 M1415.7 45.84 Q1415.62 39.7232 1412.25 36.0774 Q1408.93 32.4315 1403.42 32.4315 Q1397.18 32.4315 1393.42 35.9558 Q1389.69 39.4801 1389.12 45.8805 L1415.7 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1464.31 28.5427 L1464.31 35.5912 Q1461.15 33.9709 1457.75 33.1607 Q1454.34 32.3505 1450.7 32.3505 Q1445.15 32.3505 1442.35 34.0519 Q1439.6 35.7533 1439.6 39.156 Q1439.6 41.7486 1441.58 43.2475 Q1443.57 44.7058 1449.56 46.0426 L1452.11 46.6097 Q1460.05 48.3111 1463.38 51.4303 Q1466.74 54.509 1466.74 60.0587 Q1466.74 66.3781 1461.72 70.0644 Q1456.73 73.7508 1447.98 73.7508 Q1444.34 73.7508 1440.37 73.0216 Q1436.44 72.3329 1432.06 70.9151 L1432.06 63.2184 Q1436.19 65.3654 1440.2 66.4591 Q1444.22 67.5124 1448.14 67.5124 Q1453.41 67.5124 1456.25 65.73 Q1459.08 63.9071 1459.08 60.6258 Q1459.08 57.5877 1457.02 55.9673 Q1454.99 54.3469 1448.06 52.8481 L1445.47 52.2405 Q1438.54 50.7821 1435.47 47.7845 Q1432.39 44.7463 1432.39 39.4801 Q1432.39 33.0797 1436.92 29.5959 Q1441.46 26.1121 1449.81 26.1121 Q1453.94 26.1121 1457.58 26.7198 Q1461.23 27.3274 1464.31 28.5427 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1477.84 54.671 L1477.84 27.2059 L1485.29 27.2059 L1485.29 54.3874 Q1485.29 60.8284 1487.8 64.0691 Q1490.31 67.2693 1495.34 67.2693 Q1501.37 67.2693 1504.86 63.421 Q1508.38 59.5726 1508.38 52.9291 L1508.38 27.2059 L1515.84 27.2059 L1515.84 72.576 L1508.38 72.576 L1508.38 65.6084 Q1505.67 69.7404 1502.06 71.7658 Q1498.5 73.7508 1493.76 73.7508 Q1485.94 73.7508 1481.89 68.8897 Q1477.84 64.0286 1477.84 54.671 M1496.59 26.1121 L1496.59 26.1121 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1531.19 9.54393 L1538.64 9.54393 L1538.64 72.576 L1531.19 72.576 L1531.19 9.54393 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1561.61 14.324 L1561.61 27.2059 L1576.96 27.2059 L1576.96 32.9987 L1561.61 32.9987 L1561.61 57.6282 Q1561.61 63.1779 1563.11 64.7578 Q1564.65 66.3376 1569.31 66.3376 L1576.96 66.3376 L1576.96 72.576 L1569.31 72.576 Q1560.68 72.576 1557.4 69.3758 Q1554.12 66.1351 1554.12 57.6282 L1554.12 32.9987 L1548.65 32.9987 L1548.65 27.2059 L1554.12 27.2059 L1554.12 14.324 L1561.61 14.324 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1615.69 28.5427 L1615.69 35.5912 Q1612.53 33.9709 1609.13 33.1607 Q1605.72 32.3505 1602.08 32.3505 Q1596.53 32.3505 1593.73 34.0519 Q1590.98 35.7533 1590.98 39.156 Q1590.98 41.7486 1592.96 43.2475 Q1594.95 44.7058 1600.94 46.0426 L1603.5 46.6097 Q1611.44 48.3111 1614.76 51.4303 Q1618.12 54.509 1618.12 60.0587 Q1618.12 66.3781 1613.1 70.0644 Q1608.11 73.7508 1599.36 73.7508 Q1595.72 73.7508 1591.75 73.0216 Q1587.82 72.3329 1583.44 70.9151 L1583.44 63.2184 Q1587.58 65.3654 1591.59 66.4591 Q1595.6 67.5124 1599.53 67.5124 Q1604.79 67.5124 1607.63 65.73 Q1610.46 63.9071 1610.46 60.6258 Q1610.46 57.5877 1608.4 55.9673 Q1606.37 54.3469 1599.45 52.8481 L1596.85 52.2405 Q1589.93 50.7821 1586.85 47.7845 Q1583.77 44.7463 1583.77 39.4801 Q1583.77 33.0797 1588.31 29.5959 Q1592.84 26.1121 1601.19 26.1121 Q1605.32 26.1121 1608.97 26.7198 Q1612.61 27.3274 1615.69 28.5427 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip492)\" d=\"M323.245 295.774 L323.245 1423.18 L725.642 1423.18 L725.642 295.774 L323.245 295.774 L323.245 295.774  Z\" fill=\"#add8e6\" fill-rule=\"evenodd\" fill-opacity=\"0.5\"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"323.245,295.774 323.245,1423.18 725.642,1423.18 725.642,295.774 323.245,295.774 \"/>\n",
       "<path clip-path=\"url(#clip492)\" d=\"M826.241 277.132 L826.241 1423.18 L1228.64 1423.18 L1228.64 277.132 L826.241 277.132 L826.241 277.132  Z\" fill=\"#008000\" fill-rule=\"evenodd\" fill-opacity=\"0.5\"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"826.241,277.132 826.241,1423.18 1228.64,1423.18 1228.64,277.132 826.241,277.132 \"/>\n",
       "<path clip-path=\"url(#clip492)\" d=\"M1329.24 288.546 L1329.24 1423.18 L1731.64 1423.18 L1731.64 288.546 L1329.24 288.546 L1329.24 288.546  Z\" fill=\"#ff0000\" fill-rule=\"evenodd\" fill-opacity=\"0.5\"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"1329.24,288.546 1329.24,1423.18 1731.64,1423.18 1731.64,288.546 1329.24,288.546 \"/>\n",
       "<path clip-path=\"url(#clip492)\" d=\"M1832.23 315.304 L1832.23 1423.18 L2234.63 1423.18 L2234.63 315.304 L1832.23 315.304 L1832.23 315.304  Z\" fill=\"#0000ff\" fill-rule=\"evenodd\" fill-opacity=\"0.5\"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"1832.23,315.304 1832.23,1423.18 2234.63,1423.18 2234.63,315.304 1832.23,315.304 \"/>\n",
       "<circle clip-path=\"url(#clip492)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"524.443\" cy=\"295.774\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip492)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1027.44\" cy=\"277.132\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip492)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1530.44\" cy=\"288.546\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip492)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2033.43\" cy=\"315.304\" r=\"2\"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"524.443,468.075 524.443,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"1027.44,367.844 1027.44,186.421 \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"1530.44,399.274 1530.44,177.818 \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.5; fill:none\" points=\"2033.43,475.137 2033.43,155.471 \"/>\n",
       "<line clip-path=\"url(#clip492)\" x1=\"540.443\" y1=\"468.075\" x2=\"508.443\" y2=\"468.075\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip492)\" x1=\"540.443\" y1=\"123.472\" x2=\"508.443\" y2=\"123.472\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip492)\" x1=\"1043.44\" y1=\"367.844\" x2=\"1011.44\" y2=\"367.844\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip492)\" x1=\"1043.44\" y1=\"186.421\" x2=\"1011.44\" y2=\"186.421\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip492)\" x1=\"1546.44\" y1=\"399.274\" x2=\"1514.44\" y2=\"399.274\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip492)\" x1=\"1546.44\" y1=\"177.818\" x2=\"1514.44\" y2=\"177.818\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip492)\" x1=\"2049.43\" y1=\"475.137\" x2=\"2017.43\" y2=\"475.137\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "<line clip-path=\"url(#clip492)\" x1=\"2049.43\" y1=\"155.471\" x2=\"2017.43\" y2=\"155.471\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:0.5\"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mIndices Base.OneTo(4) of attribute `fillcolor` does not match data indices 2:12.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Plots ~/.julia/packages/Plots/sxUvK/src/utils.jl:141\u001b[39m\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mData contains NaNs or missing values, and indices of `fillcolor` vector do not match data indices.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mIf you intend elements of `fillcolor` to apply to individual NaN-separated segments in the data,\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mpass each segment in a separate vector instead, and use a row vector for `fillcolor`. Legend entries\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mmay be suppressed by passing an empty label.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mFor example,\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m    plot([1:2,1:3], [[4,5],[3,4,5]], label=[\"y\" \"\"], fillcolor=[1 2])\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mIndices Base.OneTo(4) of attribute `fillcolor` does not match data indices 2:12.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Plots ~/.julia/packages/Plots/sxUvK/src/utils.jl:141\u001b[39m\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mData contains NaNs or missing values, and indices of `fillcolor` vector do not match data indices.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mIf you intend elements of `fillcolor` to apply to individual NaN-separated segments in the data,\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mpass each segment in a separate vector instead, and use a row vector for `fillcolor`. Legend entries\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mmay be suppressed by passing an empty label.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mFor example,\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m    plot([1:2,1:3], [[4,5],[3,4,5]], label=[\"y\" \"\"], fillcolor=[1 2])\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mIndices Base.OneTo(4) of attribute `fillcolor` does not match data indices 2:12.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Plots ~/.julia/packages/Plots/sxUvK/src/utils.jl:141\u001b[39m\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mData contains NaNs or missing values, and indices of `fillcolor` vector do not match data indices.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mIf you intend elements of `fillcolor` to apply to individual NaN-separated segments in the data,\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mpass each segment in a separate vector instead, and use a row vector for `fillcolor`. Legend entries\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mmay be suppressed by passing an empty label.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mFor example,\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m    plot([1:2,1:3], [[4,5],[3,4,5]], label=[\"y\" \"\"], fillcolor=[1 2])\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mIndices Base.OneTo(4) of attribute `fillcolor` does not match data indices 2:12.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Plots ~/.julia/packages/Plots/sxUvK/src/utils.jl:141\u001b[39m\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mData contains NaNs or missing values, and indices of `fillcolor` vector do not match data indices.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mIf you intend elements of `fillcolor` to apply to individual NaN-separated segments in the data,\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mpass each segment in a separate vector instead, and use a row vector for `fillcolor`. Legend entries\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mmay be suppressed by passing an empty label.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mFor example,\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m    plot([1:2,1:3], [[4,5],[3,4,5]], label=[\"y\" \"\"], fillcolor=[1 2])\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mIndices Base.OneTo(4) of attribute `fillcolor` does not match data indices 2:12.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Plots ~/.julia/packages/Plots/sxUvK/src/utils.jl:141\u001b[39m\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mData contains NaNs or missing values, and indices of `fillcolor` vector do not match data indices.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mIf you intend elements of `fillcolor` to apply to individual NaN-separated segments in the data,\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mpass each segment in a separate vector instead, and use a row vector for `fillcolor`. Legend entries\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mmay be suppressed by passing an empty label.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mFor example,\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m    plot([1:2,1:3], [[4,5],[3,4,5]], label=[\"y\" \"\"], fillcolor=[1 2])\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mIndices Base.OneTo(4) of attribute `fillcolor` does not match data indices 2:12.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Plots ~/.julia/packages/Plots/sxUvK/src/utils.jl:141\u001b[39m\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mData contains NaNs or missing values, and indices of `fillcolor` vector do not match data indices.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mIf you intend elements of `fillcolor` to apply to individual NaN-separated segments in the data,\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mpass each segment in a separate vector instead, and use a row vector for `fillcolor`. Legend entries\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mmay be suppressed by passing an empty label.\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mFor example,\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m    plot([1:2,1:3], [[4,5],[3,4,5]], label=[\"y\" \"\"], fillcolor=[1 2])\n"
     ]
    }
   ],
   "source": [
    "plot_bars([85.08874458874459, 86.49567099567099, 85.63419913419914,83.61471861471863],[\"SVM\",\"KNN\", \"DT\",\"ANN\"],[13.004111119213235, 6.846281149915388, 8.356972212974133,12.06308211293468],[:lightblue, :green, :red,:blue])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8840ea28",
   "metadata": {},
   "source": [
    "## Techniques integrating the Ensemble approach\n",
    "\n",
    "Some of the best-known and currently used algorithms are based on this type of approach. Among these approaches, perhaps the most famous and widely used are those based on the generation of simple Decision Tress (DT). The reason for the use of the trees is their easy interpretation, as well as the speed of calculation and training. In the following we will see the two approaches known today in this sense, ***Random Forest*** and ***XGBoost***.\n",
    "\n",
    "\n",
    "### Random Forest\n",
    "This algorithm, proposed by Breitman and Cutler in 2006 on the basis of an earlier publication by Ho in 1995 (_Random Subspaces_), is the paradigm ensemble technique. The algorithm joins into an ensemble a set of simple classifiers that take the form of Decision Trees. These classifiers are trained following a **bagging** approach, and can therefore be trained in parallel. Combining the output of the algorithms is done for classification problems by means of the most voted option among the \"experts\" or, if it is a regression problem, by means of the arithmetic mean of the answers. \n",
    "\n",
    "It is an algorithm that needs the adjustment of very few hyperparameters to obtain very good results in almost any type of problem. In general, the most important value is the number of estimators and therefore the number of partitions to be made of the training set. Several authors point out that this number of estimators should be *$\\sqrt{\\textrm{#feature}}$* for classification problems, and *$\\frac{\\textrm{#feature}}{3}$* for regression problems. However, he also points out that the technique would saturate between 500 and 1000 trees and no matter how much it is increased it would not improve results. However, this last point has only been tested empirically on certain data sets and, therefore, should be taken with caution as it has no mathematical justification.\n",
    "\n",
    "In addition to the usual bagging process, the Random Forest also includes a second splitting mechanism. Once the patterns that will form part of the training set of the decision tree have been selected, only a subset of random features (*features*) are available for each node of the tree. This increases the diversity of the trees in the forest and focuses on the overall performance with a small variance in the results. This mechanism makes it possible to quantitatively assess the individual performance of each tree in the forest and its variables. Therefore, the importance of each variable can be measured. This measure that calibrates the participation of each variable in the nodes of the tree in decision-making is called impurity and measures the difference between the different branches of the tree when partitioning the examples. Sometimes, this same measure is used as a measure for the selection of variables by taking the measure in all the trees of the forest of the participation and importance by means of a filtering like those seen in the previous unit.\n",
    "\n",
    "For the calculation of this measure of impurity, there are different approaches. For example, `scikit learn` uses a measure it calls **Gini**. The latter is the probability of misclassifying a randomly chosen item in the dataset if it were randomly labelled according to the distribution of classes in the dataset. It is calculated as:\n",
    "$$G = \\sum_{i=1}^C p(i) * (1 - p(i))$$\n",
    "\n",
    "where $C$ is the number of classes and $p(i)$ is the probability of randomly selecting an element of class $i$. A good example of how to calculate the impurity of the branches can be seen in the following [link](https://victorzhou.com/blog/gini-impurity/)\n",
    "\n",
    "Next, on the example used in this unit, we will run a *Random Forest* model with the `scikit learn` implementation. The most important parameters of this implementation are:\n",
    "\n",
    "- ***n_estimator***, marking the number of trees to be generated or the number of *bagging* partitions.\n",
    "- ***criterion***, measure of node impurity. By default Gini is used, but it can be changed to gained entropy.\n",
    "- max_depth***, allows to limit the maximum depth of the trees in order to limit the number of variables to use.\n",
    "- ***min_sample_split***, for each decision tree, how many patterns are needed to perform an internal split in the *Decision Trees*.\n",
    "- bootstrap***, you can use the *bagging* or *bootstrap* approach to build the trees but if this property is false, then it uses the whole training set to generate the trees. In case of a True value, the following properties are taken into account:\n",
    "    + ***max_samples***, number of examples to extract from the original set to build the training set of the estimator, the default value is equal to the number of patterns but remember that the same can be extracted several times as it is a selection with replacement giving variability.\n",
    "    + ***oob_score***, *out of bag* measure for estimating generalisation. Those samples that have not been part of the training of an estimator can be used to calculate a validation measure, and averaged across all estimators to see how general the constructed forest is.\n",
    "    \n",
    "For example, the code below shows how to use the implementation in `scikit learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "668be4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB: 65.07936507936508 %\n",
      "DT: 73.01587301587301 %\n",
      "GTB: 85.71428571428571 %\n",
      "Ensemble (Stacking): 79.36507936507937 %\n",
      "SVM: 80.95238095238095 %\n",
      "LR: 77.77777777777779 %\n",
      "Ensemble (Hard Voting): 80.95238095238095 %\n",
      "Ensemble (Soft Voting): 80.95238095238095 %\n",
      "Bagging (SVC): 74.60317460317461 %\n",
      "Ada: 85.71428571428571 %\n",
      "RF: 80.95238095238095 %\n"
     ]
    }
   ],
   "source": [
    "@sk_import ensemble:RandomForestClassifier\n",
    "\n",
    "models[\"RF\"] = RandomForestClassifier(n_estimators=8, max_depth=nothing,\n",
    "                                    min_samples_split=2, n_jobs=-1)\n",
    "fit!(models[\"RF\"], train_input, train_output)\n",
    "    \n",
    "for key in keys(models)\n",
    "    model = models[key]\n",
    "    acc = score(model,test_input, test_output)\n",
    "    println(\"$key: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dbe372",
   "metadata": {},
   "source": [
    "In this approach, the number of estimators has been defined following the aforementioned rule of $\\sqrt{\\textrm{#features}}$. In this case, as there are few estimators and few patterns, the results may vary quite a lot depending on the type of partitions obtained.\n",
    "\n",
    "Then, once the model has been trained, the level of impurity obtained for each of the frequencies calculated with the Gini algorithm can be computed as an average of those obtained among the trees that make up the forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea85f5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip530\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip530)\" d=\"M0 1600 L2400 1600 L2400 0 L0 0  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip531\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip530)\" d=\"M205.121 1423.18 L2352.76 1423.18 L2352.76 123.472 L205.121 123.472  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip532\">\n",
       "    <rect x=\"205\" y=\"123\" width=\"2149\" height=\"1301\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"265.903,1423.18 265.903,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"603.582,1423.18 603.582,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"941.26,1423.18 941.26,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1278.94,1423.18 1278.94,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1616.62,1423.18 1616.62,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1954.3,1423.18 1954.3,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2291.97,1423.18 2291.97,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 2352.76,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.903,1423.18 265.903,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"603.582,1423.18 603.582,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"941.26,1423.18 941.26,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1278.94,1423.18 1278.94,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1616.62,1423.18 1616.62,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1954.3,1423.18 1954.3,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2291.97,1423.18 2291.97,1404.28 \"/>\n",
       "<path clip-path=\"url(#clip530)\" d=\"M265.903 1454.1 Q262.292 1454.1 260.463 1457.66 Q258.658 1461.2 258.658 1468.33 Q258.658 1475.44 260.463 1479.01 Q262.292 1482.55 265.903 1482.55 Q269.537 1482.55 271.343 1479.01 Q273.172 1475.44 273.172 1468.33 Q273.172 1461.2 271.343 1457.66 Q269.537 1454.1 265.903 1454.1 M265.903 1450.39 Q271.713 1450.39 274.769 1455 Q277.847 1459.58 277.847 1468.33 Q277.847 1477.06 274.769 1481.67 Q271.713 1486.25 265.903 1486.25 Q260.093 1486.25 257.014 1481.67 Q253.959 1477.06 253.959 1468.33 Q253.959 1459.58 257.014 1455 Q260.093 1450.39 265.903 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M578.269 1481.64 L585.908 1481.64 L585.908 1455.28 L577.598 1456.95 L577.598 1452.69 L585.862 1451.02 L590.538 1451.02 L590.538 1481.64 L598.176 1481.64 L598.176 1485.58 L578.269 1485.58 L578.269 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M617.621 1454.1 Q614.01 1454.1 612.181 1457.66 Q610.375 1461.2 610.375 1468.33 Q610.375 1475.44 612.181 1479.01 Q614.01 1482.55 617.621 1482.55 Q621.255 1482.55 623.061 1479.01 Q624.889 1475.44 624.889 1468.33 Q624.889 1461.2 623.061 1457.66 Q621.255 1454.1 617.621 1454.1 M617.621 1450.39 Q623.431 1450.39 626.487 1455 Q629.565 1459.58 629.565 1468.33 Q629.565 1477.06 626.487 1481.67 Q623.431 1486.25 617.621 1486.25 Q611.811 1486.25 608.732 1481.67 Q605.676 1477.06 605.676 1468.33 Q605.676 1459.58 608.732 1455 Q611.811 1450.39 617.621 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M920.033 1481.64 L936.353 1481.64 L936.353 1485.58 L914.408 1485.58 L914.408 1481.64 Q917.07 1478.89 921.654 1474.26 Q926.26 1469.61 927.441 1468.27 Q929.686 1465.74 930.566 1464.01 Q931.468 1462.25 931.468 1460.56 Q931.468 1457.8 929.524 1456.07 Q927.603 1454.33 924.501 1454.33 Q922.302 1454.33 919.848 1455.09 Q917.418 1455.86 914.64 1457.41 L914.64 1452.69 Q917.464 1451.55 919.918 1450.97 Q922.371 1450.39 924.408 1450.39 Q929.779 1450.39 932.973 1453.08 Q936.167 1455.77 936.167 1460.26 Q936.167 1462.39 935.357 1464.31 Q934.57 1466.2 932.464 1468.8 Q931.885 1469.47 928.783 1472.69 Q925.681 1475.88 920.033 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M956.167 1454.1 Q952.556 1454.1 950.728 1457.66 Q948.922 1461.2 948.922 1468.33 Q948.922 1475.44 950.728 1479.01 Q952.556 1482.55 956.167 1482.55 Q959.802 1482.55 961.607 1479.01 Q963.436 1475.44 963.436 1468.33 Q963.436 1461.2 961.607 1457.66 Q959.802 1454.1 956.167 1454.1 M956.167 1450.39 Q961.977 1450.39 965.033 1455 Q968.112 1459.58 968.112 1468.33 Q968.112 1477.06 965.033 1481.67 Q961.977 1486.25 956.167 1486.25 Q950.357 1486.25 947.278 1481.67 Q944.223 1477.06 944.223 1468.33 Q944.223 1459.58 947.278 1455 Q950.357 1450.39 956.167 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1267.78 1466.95 Q1271.14 1467.66 1273.01 1469.93 Q1274.91 1472.2 1274.91 1475.53 Q1274.91 1480.65 1271.39 1483.45 Q1267.87 1486.25 1261.39 1486.25 Q1259.22 1486.25 1256.9 1485.81 Q1254.61 1485.39 1252.16 1484.54 L1252.16 1480.02 Q1254.1 1481.16 1256.42 1481.74 Q1258.73 1482.32 1261.25 1482.32 Q1265.65 1482.32 1267.94 1480.58 Q1270.26 1478.84 1270.26 1475.53 Q1270.26 1472.48 1268.11 1470.77 Q1265.98 1469.03 1262.16 1469.03 L1258.13 1469.03 L1258.13 1465.19 L1262.34 1465.19 Q1265.79 1465.19 1267.62 1463.82 Q1269.45 1462.43 1269.45 1459.84 Q1269.45 1457.18 1267.55 1455.77 Q1265.67 1454.33 1262.16 1454.33 Q1260.23 1454.33 1258.04 1454.75 Q1255.84 1455.16 1253.2 1456.04 L1253.2 1451.88 Q1255.86 1451.14 1258.17 1450.77 Q1260.51 1450.39 1262.57 1450.39 Q1267.9 1450.39 1271 1452.83 Q1274.1 1455.23 1274.1 1459.35 Q1274.1 1462.22 1272.46 1464.21 Q1270.81 1466.18 1267.78 1466.95 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1293.78 1454.1 Q1290.17 1454.1 1288.34 1457.66 Q1286.53 1461.2 1286.53 1468.33 Q1286.53 1475.44 1288.34 1479.01 Q1290.17 1482.55 1293.78 1482.55 Q1297.41 1482.55 1299.22 1479.01 Q1301.04 1475.44 1301.04 1468.33 Q1301.04 1461.2 1299.22 1457.66 Q1297.41 1454.1 1293.78 1454.1 M1293.78 1450.39 Q1299.59 1450.39 1302.64 1455 Q1305.72 1459.58 1305.72 1468.33 Q1305.72 1477.06 1302.64 1481.67 Q1299.59 1486.25 1293.78 1486.25 Q1287.97 1486.25 1284.89 1481.67 Q1281.83 1477.06 1281.83 1468.33 Q1281.83 1459.58 1284.89 1455 Q1287.97 1450.39 1293.78 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1604.79 1455.09 L1592.98 1473.54 L1604.79 1473.54 L1604.79 1455.09 M1603.56 1451.02 L1609.44 1451.02 L1609.44 1473.54 L1614.37 1473.54 L1614.37 1477.43 L1609.44 1477.43 L1609.44 1485.58 L1604.79 1485.58 L1604.79 1477.43 L1589.19 1477.43 L1589.19 1472.92 L1603.56 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1632.1 1454.1 Q1628.49 1454.1 1626.66 1457.66 Q1624.86 1461.2 1624.86 1468.33 Q1624.86 1475.44 1626.66 1479.01 Q1628.49 1482.55 1632.1 1482.55 Q1635.74 1482.55 1637.54 1479.01 Q1639.37 1475.44 1639.37 1468.33 Q1639.37 1461.2 1637.54 1457.66 Q1635.74 1454.1 1632.1 1454.1 M1632.1 1450.39 Q1637.91 1450.39 1640.97 1455 Q1644.05 1459.58 1644.05 1468.33 Q1644.05 1477.06 1640.97 1481.67 Q1637.91 1486.25 1632.1 1486.25 Q1626.29 1486.25 1623.21 1481.67 Q1620.16 1477.06 1620.16 1468.33 Q1620.16 1459.58 1623.21 1455 Q1626.29 1450.39 1632.1 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1928.99 1451.02 L1947.35 1451.02 L1947.35 1454.96 L1933.28 1454.96 L1933.28 1463.43 Q1934.3 1463.08 1935.31 1462.92 Q1936.33 1462.73 1937.35 1462.73 Q1943.14 1462.73 1946.52 1465.9 Q1949.9 1469.08 1949.9 1474.49 Q1949.9 1480.07 1946.43 1483.17 Q1942.95 1486.25 1936.63 1486.25 Q1934.46 1486.25 1932.19 1485.88 Q1929.94 1485.51 1927.54 1484.77 L1927.54 1480.07 Q1929.62 1481.2 1931.84 1481.76 Q1934.06 1482.32 1936.54 1482.32 Q1940.55 1482.32 1942.88 1480.21 Q1945.22 1478.1 1945.22 1474.49 Q1945.22 1470.88 1942.88 1468.77 Q1940.55 1466.67 1936.54 1466.67 Q1934.67 1466.67 1932.79 1467.08 Q1930.94 1467.5 1928.99 1468.38 L1928.99 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1969.11 1454.1 Q1965.5 1454.1 1963.67 1457.66 Q1961.86 1461.2 1961.86 1468.33 Q1961.86 1475.44 1963.67 1479.01 Q1965.5 1482.55 1969.11 1482.55 Q1972.74 1482.55 1974.55 1479.01 Q1976.38 1475.44 1976.38 1468.33 Q1976.38 1461.2 1974.55 1457.66 Q1972.74 1454.1 1969.11 1454.1 M1969.11 1450.39 Q1974.92 1450.39 1977.98 1455 Q1981.05 1459.58 1981.05 1468.33 Q1981.05 1477.06 1977.98 1481.67 Q1974.92 1486.25 1969.11 1486.25 Q1963.3 1486.25 1960.22 1481.67 Q1957.17 1477.06 1957.17 1468.33 Q1957.17 1459.58 1960.22 1455 Q1963.3 1450.39 1969.11 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M2277.38 1466.44 Q2274.23 1466.44 2272.38 1468.59 Q2270.55 1470.74 2270.55 1474.49 Q2270.55 1478.22 2272.38 1480.39 Q2274.23 1482.55 2277.38 1482.55 Q2280.53 1482.55 2282.36 1480.39 Q2284.21 1478.22 2284.21 1474.49 Q2284.21 1470.74 2282.36 1468.59 Q2280.53 1466.44 2277.38 1466.44 M2286.66 1451.78 L2286.66 1456.04 Q2284.9 1455.21 2283.1 1454.77 Q2281.31 1454.33 2279.55 1454.33 Q2274.93 1454.33 2272.47 1457.45 Q2270.04 1460.58 2269.69 1466.9 Q2271.06 1464.89 2273.12 1463.82 Q2275.18 1462.73 2277.66 1462.73 Q2282.87 1462.73 2285.87 1465.9 Q2288.91 1469.05 2288.91 1474.49 Q2288.91 1479.82 2285.76 1483.03 Q2282.61 1486.25 2277.38 1486.25 Q2271.38 1486.25 2268.21 1481.67 Q2265.04 1477.06 2265.04 1468.33 Q2265.04 1460.14 2268.93 1455.28 Q2272.82 1450.39 2279.37 1450.39 Q2281.13 1450.39 2282.91 1450.74 Q2284.72 1451.09 2286.66 1451.78 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M2306.96 1454.1 Q2303.35 1454.1 2301.52 1457.66 Q2299.72 1461.2 2299.72 1468.33 Q2299.72 1475.44 2301.52 1479.01 Q2303.35 1482.55 2306.96 1482.55 Q2310.6 1482.55 2312.4 1479.01 Q2314.23 1475.44 2314.23 1468.33 Q2314.23 1461.2 2312.4 1457.66 Q2310.6 1454.1 2306.96 1454.1 M2306.96 1450.39 Q2312.77 1450.39 2315.83 1455 Q2318.91 1459.58 2318.91 1468.33 Q2318.91 1477.06 2315.83 1481.67 Q2312.77 1486.25 2306.96 1486.25 Q2301.15 1486.25 2298.07 1481.67 Q2295.02 1477.06 2295.02 1468.33 Q2295.02 1459.58 2298.07 1455 Q2301.15 1450.39 2306.96 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1169.35 1561.26 L1169.35 1548.5 L1158.85 1548.5 L1158.85 1543.22 L1175.72 1543.22 L1175.72 1563.62 Q1171.99 1566.26 1167.51 1567.63 Q1163.02 1568.97 1157.93 1568.97 Q1146.79 1568.97 1140.48 1562.47 Q1134.21 1555.95 1134.21 1544.33 Q1134.21 1532.68 1140.48 1526.19 Q1146.79 1519.66 1157.93 1519.66 Q1162.57 1519.66 1166.74 1520.81 Q1170.94 1521.96 1174.48 1524.18 L1174.48 1531.03 Q1170.91 1528 1166.9 1526.48 Q1162.89 1524.95 1158.47 1524.95 Q1149.75 1524.95 1145.35 1529.82 Q1140.99 1534.69 1140.99 1544.33 Q1140.99 1553.94 1145.35 1558.81 Q1149.75 1563.68 1158.47 1563.68 Q1161.87 1563.68 1164.55 1563.11 Q1167.22 1562.51 1169.35 1561.26 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1187.21 1532.4 L1193.07 1532.4 L1193.07 1568.04 L1187.21 1568.04 L1187.21 1532.4 M1187.21 1518.52 L1193.07 1518.52 L1193.07 1525.93 L1187.21 1525.93 L1187.21 1518.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1234.95 1546.53 L1234.95 1568.04 L1229.09 1568.04 L1229.09 1546.72 Q1229.09 1541.66 1227.12 1539.14 Q1225.15 1536.63 1221.2 1536.63 Q1216.46 1536.63 1213.72 1539.65 Q1210.98 1542.68 1210.98 1547.9 L1210.98 1568.04 L1205.1 1568.04 L1205.1 1532.4 L1210.98 1532.4 L1210.98 1537.93 Q1213.09 1534.72 1215.92 1533.13 Q1218.78 1531.54 1222.51 1531.54 Q1228.65 1531.54 1231.8 1535.36 Q1234.95 1539.14 1234.95 1546.53 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1246.63 1532.4 L1252.49 1532.4 L1252.49 1568.04 L1246.63 1568.04 L1246.63 1532.4 M1246.63 1518.52 L1252.49 1518.52 L1252.49 1525.93 L1246.63 1525.93 L1246.63 1518.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1318.12 1561.26 L1318.12 1548.5 L1307.62 1548.5 L1307.62 1543.22 L1324.49 1543.22 L1324.49 1563.62 Q1320.76 1566.26 1316.27 1567.63 Q1311.79 1568.97 1306.69 1568.97 Q1295.55 1568.97 1289.25 1562.47 Q1282.98 1555.95 1282.98 1544.33 Q1282.98 1532.68 1289.25 1526.19 Q1295.55 1519.66 1306.69 1519.66 Q1311.34 1519.66 1315.51 1520.81 Q1319.71 1521.96 1323.24 1524.18 L1323.24 1531.03 Q1319.68 1528 1315.67 1526.48 Q1311.66 1524.95 1307.23 1524.95 Q1298.51 1524.95 1294.12 1529.82 Q1289.76 1534.69 1289.76 1544.33 Q1289.76 1553.94 1294.12 1558.81 Q1298.51 1563.68 1307.23 1563.68 Q1310.64 1563.68 1313.31 1563.11 Q1315.99 1562.51 1318.12 1561.26 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1352.18 1550.12 Q1345.08 1550.12 1342.34 1551.75 Q1339.6 1553.37 1339.6 1557.29 Q1339.6 1560.4 1341.64 1562.25 Q1343.71 1564.07 1347.24 1564.07 Q1352.11 1564.07 1355.04 1560.63 Q1358 1557.16 1358 1551.43 L1358 1550.12 L1352.18 1550.12 M1363.86 1547.71 L1363.86 1568.04 L1358 1568.04 L1358 1562.63 Q1356 1565.88 1353 1567.44 Q1350.01 1568.97 1345.68 1568.97 Q1340.21 1568.97 1336.96 1565.91 Q1333.75 1562.82 1333.75 1557.67 Q1333.75 1551.65 1337.76 1548.6 Q1341.8 1545.54 1349.79 1545.54 L1358 1545.54 L1358 1544.97 Q1358 1540.93 1355.33 1538.73 Q1352.69 1536.5 1347.88 1536.5 Q1344.82 1536.5 1341.93 1537.23 Q1339.03 1537.97 1336.36 1539.43 L1336.36 1534.02 Q1339.57 1532.78 1342.6 1532.17 Q1345.62 1531.54 1348.48 1531.54 Q1356.22 1531.54 1360.04 1535.55 Q1363.86 1539.56 1363.86 1547.71 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1375.92 1532.4 L1381.78 1532.4 L1381.78 1568.04 L1375.92 1568.04 L1375.92 1532.4 M1375.92 1518.52 L1381.78 1518.52 L1381.78 1525.93 L1375.92 1525.93 L1375.92 1518.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1423.66 1546.53 L1423.66 1568.04 L1417.81 1568.04 L1417.81 1546.72 Q1417.81 1541.66 1415.83 1539.14 Q1413.86 1536.63 1409.91 1536.63 Q1405.17 1536.63 1402.43 1539.65 Q1399.7 1542.68 1399.7 1547.9 L1399.7 1568.04 L1393.81 1568.04 L1393.81 1532.4 L1399.7 1532.4 L1399.7 1537.93 Q1401.8 1534.72 1404.63 1533.13 Q1407.49 1531.54 1411.22 1531.54 Q1417.36 1531.54 1420.51 1535.36 Q1423.66 1539.14 1423.66 1546.53 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,1398.7 2352.76,1398.7 \"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,1193.66 2352.76,1193.66 \"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,988.618 2352.76,988.618 \"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,783.578 2352.76,783.578 \"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,578.538 2352.76,578.538 \"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,373.498 2352.76,373.498 \"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,168.458 2352.76,168.458 \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 205.121,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1398.7 224.019,1398.7 \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1193.66 224.019,1193.66 \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,988.618 224.019,988.618 \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,783.578 224.019,783.578 \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,578.538 224.019,578.538 \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,373.498 224.019,373.498 \"/>\n",
       "<polyline clip-path=\"url(#clip530)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,168.458 224.019,168.458 \"/>\n",
       "<path clip-path=\"url(#clip530)\" d=\"M157.177 1384.5 Q153.566 1384.5 151.737 1388.06 Q149.931 1391.6 149.931 1398.73 Q149.931 1405.84 151.737 1409.4 Q153.566 1412.95 157.177 1412.95 Q160.811 1412.95 162.616 1409.4 Q164.445 1405.84 164.445 1398.73 Q164.445 1391.6 162.616 1388.06 Q160.811 1384.5 157.177 1384.5 M157.177 1380.79 Q162.987 1380.79 166.042 1385.4 Q169.121 1389.98 169.121 1398.73 Q169.121 1407.46 166.042 1412.07 Q162.987 1416.65 157.177 1416.65 Q151.366 1416.65 148.288 1412.07 Q145.232 1407.46 145.232 1398.73 Q145.232 1389.98 148.288 1385.4 Q151.366 1380.79 157.177 1380.79 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M117.825 1207 L125.464 1207 L125.464 1180.64 L117.154 1182.3 L117.154 1178.04 L125.418 1176.38 L130.093 1176.38 L130.093 1207 L137.732 1207 L137.732 1210.94 L117.825 1210.94 L117.825 1207 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M157.177 1179.46 Q153.566 1179.46 151.737 1183.02 Q149.931 1186.56 149.931 1193.69 Q149.931 1200.8 151.737 1204.36 Q153.566 1207.91 157.177 1207.91 Q160.811 1207.91 162.616 1204.36 Q164.445 1200.8 164.445 1193.69 Q164.445 1186.56 162.616 1183.02 Q160.811 1179.46 157.177 1179.46 M157.177 1175.75 Q162.987 1175.75 166.042 1180.36 Q169.121 1184.94 169.121 1193.69 Q169.121 1202.42 166.042 1207.03 Q162.987 1211.61 157.177 1211.61 Q151.366 1211.61 148.288 1207.03 Q145.232 1202.42 145.232 1193.69 Q145.232 1184.94 148.288 1180.36 Q151.366 1175.75 157.177 1175.75 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M121.043 1001.96 L137.362 1001.96 L137.362 1005.9 L115.418 1005.9 L115.418 1001.96 Q118.08 999.208 122.663 994.579 Q127.269 989.926 128.45 988.583 Q130.695 986.06 131.575 984.324 Q132.478 982.565 132.478 980.875 Q132.478 978.12 130.533 976.384 Q128.612 974.648 125.51 974.648 Q123.311 974.648 120.857 975.412 Q118.427 976.176 115.649 977.727 L115.649 973.005 Q118.473 971.87 120.927 971.292 Q123.38 970.713 125.418 970.713 Q130.788 970.713 133.982 973.398 Q137.177 976.083 137.177 980.574 Q137.177 982.704 136.367 984.625 Q135.579 986.523 133.473 989.116 Q132.894 989.787 129.792 993.005 Q126.691 996.199 121.043 1001.96 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M157.177 974.417 Q153.566 974.417 151.737 977.981 Q149.931 981.523 149.931 988.653 Q149.931 995.759 151.737 999.324 Q153.566 1002.87 157.177 1002.87 Q160.811 1002.87 162.616 999.324 Q164.445 995.759 164.445 988.653 Q164.445 981.523 162.616 977.981 Q160.811 974.417 157.177 974.417 M157.177 970.713 Q162.987 970.713 166.042 975.319 Q169.121 979.903 169.121 988.653 Q169.121 997.38 166.042 1001.99 Q162.987 1006.57 157.177 1006.57 Q151.366 1006.57 148.288 1001.99 Q145.232 997.38 145.232 988.653 Q145.232 979.903 148.288 975.319 Q151.366 970.713 157.177 970.713 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M131.181 782.224 Q134.538 782.941 136.413 785.21 Q138.311 787.478 138.311 790.812 Q138.311 795.927 134.792 798.728 Q131.274 801.529 124.793 801.529 Q122.617 801.529 120.302 801.089 Q118.01 800.673 115.556 799.816 L115.556 795.302 Q117.501 796.437 119.816 797.015 Q122.13 797.594 124.654 797.594 Q129.052 797.594 131.343 795.858 Q133.658 794.122 133.658 790.812 Q133.658 787.756 131.505 786.043 Q129.376 784.307 125.556 784.307 L121.529 784.307 L121.529 780.465 L125.742 780.465 Q129.191 780.465 131.019 779.099 Q132.848 777.71 132.848 775.117 Q132.848 772.455 130.95 771.043 Q129.075 769.608 125.556 769.608 Q123.635 769.608 121.436 770.025 Q119.237 770.441 116.598 771.321 L116.598 767.154 Q119.26 766.414 121.575 766.043 Q123.913 765.673 125.973 765.673 Q131.297 765.673 134.399 768.104 Q137.501 770.511 137.501 774.631 Q137.501 777.502 135.857 779.492 Q134.214 781.46 131.181 782.224 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M157.177 769.377 Q153.566 769.377 151.737 772.941 Q149.931 776.483 149.931 783.613 Q149.931 790.719 151.737 794.284 Q153.566 797.826 157.177 797.826 Q160.811 797.826 162.616 794.284 Q164.445 790.719 164.445 783.613 Q164.445 776.483 162.616 772.941 Q160.811 769.377 157.177 769.377 M157.177 765.673 Q162.987 765.673 166.042 770.279 Q169.121 774.863 169.121 783.613 Q169.121 792.34 166.042 796.946 Q162.987 801.529 157.177 801.529 Q151.366 801.529 148.288 796.946 Q145.232 792.34 145.232 783.613 Q145.232 774.863 148.288 770.279 Q151.366 765.673 157.177 765.673 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M129.862 565.332 L118.056 583.781 L129.862 583.781 L129.862 565.332 M128.635 561.258 L134.515 561.258 L134.515 583.781 L139.445 583.781 L139.445 587.67 L134.515 587.67 L134.515 595.818 L129.862 595.818 L129.862 587.67 L114.26 587.67 L114.26 583.156 L128.635 561.258 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M157.177 564.337 Q153.566 564.337 151.737 567.901 Q149.931 571.443 149.931 578.573 Q149.931 585.679 151.737 589.244 Q153.566 592.786 157.177 592.786 Q160.811 592.786 162.616 589.244 Q164.445 585.679 164.445 578.573 Q164.445 571.443 162.616 567.901 Q160.811 564.337 157.177 564.337 M157.177 560.633 Q162.987 560.633 166.042 565.239 Q169.121 569.823 169.121 578.573 Q169.121 587.299 166.042 591.906 Q162.987 596.489 157.177 596.489 Q151.366 596.489 148.288 591.906 Q145.232 587.299 145.232 578.573 Q145.232 569.823 148.288 565.239 Q151.366 560.633 157.177 560.633 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M117.061 356.218 L135.417 356.218 L135.417 360.153 L121.343 360.153 L121.343 368.625 Q122.362 368.278 123.38 368.116 Q124.399 367.931 125.418 367.931 Q131.205 367.931 134.584 371.102 Q137.964 374.273 137.964 379.69 Q137.964 385.269 134.492 388.371 Q131.019 391.449 124.7 391.449 Q122.524 391.449 120.255 391.079 Q118.01 390.708 115.603 389.968 L115.603 385.269 Q117.686 386.403 119.908 386.959 Q122.13 387.514 124.607 387.514 Q128.612 387.514 130.95 385.408 Q133.288 383.301 133.288 379.69 Q133.288 376.079 130.95 373.972 Q128.612 371.866 124.607 371.866 Q122.732 371.866 120.857 372.283 Q119.006 372.699 117.061 373.579 L117.061 356.218 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M157.177 359.297 Q153.566 359.297 151.737 362.861 Q149.931 366.403 149.931 373.533 Q149.931 380.639 151.737 384.204 Q153.566 387.746 157.177 387.746 Q160.811 387.746 162.616 384.204 Q164.445 380.639 164.445 373.533 Q164.445 366.403 162.616 362.861 Q160.811 359.297 157.177 359.297 M157.177 355.593 Q162.987 355.593 166.042 360.199 Q169.121 364.783 169.121 373.533 Q169.121 382.259 166.042 386.866 Q162.987 391.449 157.177 391.449 Q151.366 391.449 148.288 386.866 Q145.232 382.259 145.232 373.533 Q145.232 364.783 148.288 360.199 Q151.366 355.593 157.177 355.593 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M127.593 166.594 Q124.445 166.594 122.593 168.747 Q120.765 170.9 120.765 174.65 Q120.765 178.377 122.593 180.553 Q124.445 182.706 127.593 182.706 Q130.742 182.706 132.57 180.553 Q134.422 178.377 134.422 174.65 Q134.422 170.9 132.57 168.747 Q130.742 166.594 127.593 166.594 M136.876 151.942 L136.876 156.201 Q135.117 155.368 133.311 154.928 Q131.529 154.488 129.769 154.488 Q125.14 154.488 122.686 157.613 Q120.255 160.738 119.908 167.057 Q121.274 165.044 123.334 163.979 Q125.394 162.891 127.871 162.891 Q133.08 162.891 136.089 166.062 Q139.121 169.21 139.121 174.65 Q139.121 179.974 135.973 183.192 Q132.825 186.409 127.593 186.409 Q121.598 186.409 118.427 181.826 Q115.256 177.219 115.256 168.493 Q115.256 160.298 119.144 155.437 Q123.033 150.553 129.584 150.553 Q131.343 150.553 133.126 150.9 Q134.931 151.247 136.876 151.942 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M157.177 154.257 Q153.566 154.257 151.737 157.821 Q149.931 161.363 149.931 168.493 Q149.931 175.599 151.737 179.164 Q153.566 182.706 157.177 182.706 Q160.811 182.706 162.616 179.164 Q164.445 175.599 164.445 168.493 Q164.445 161.363 162.616 157.821 Q160.811 154.257 157.177 154.257 M157.177 150.553 Q162.987 150.553 166.042 155.159 Q169.121 159.743 169.121 168.493 Q169.121 177.219 166.042 181.826 Q162.987 186.409 157.177 186.409 Q151.366 186.409 148.288 181.826 Q145.232 177.219 145.232 168.493 Q145.232 159.743 148.288 155.159 Q151.366 150.553 157.177 150.553 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M16.4842 892.174 L16.4842 864.865 L21.895 864.865 L21.895 885.744 L35.8996 885.744 L35.8996 866.902 L41.3104 866.902 L41.3104 885.744 L64.0042 885.744 L64.0042 892.174 L16.4842 892.174 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M44.7161 828.007 L47.5806 828.007 L47.5806 854.934 Q53.6281 854.552 56.8109 851.306 Q59.9619 848.028 59.9619 842.203 Q59.9619 838.829 59.1344 835.678 Q58.3069 832.495 56.6518 829.376 L62.1899 829.376 Q63.5267 832.527 64.227 835.837 Q64.9272 839.147 64.9272 842.553 Q64.9272 851.083 59.9619 856.08 Q54.9967 861.045 46.5303 861.045 Q37.7774 861.045 32.6531 856.335 Q27.4968 851.592 27.4968 843.572 Q27.4968 836.378 32.1438 832.209 Q36.7589 828.007 44.7161 828.007 M42.9973 833.864 Q38.1912 833.928 35.3266 836.569 Q32.4621 839.179 32.4621 843.508 Q32.4621 848.409 35.2312 851.37 Q38.0002 854.298 43.0292 854.743 L42.9973 833.864 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M46.0847 802.194 Q46.0847 809.292 47.7079 812.029 Q49.3312 814.767 53.2461 814.767 Q56.3653 814.767 58.2114 812.73 Q60.0256 810.661 60.0256 807.128 Q60.0256 802.258 56.5881 799.33 Q53.1188 796.37 47.3897 796.37 L46.0847 796.37 L46.0847 802.194 M43.6657 790.513 L64.0042 790.513 L64.0042 796.37 L58.5933 796.37 Q61.8398 798.375 63.3994 801.367 Q64.9272 804.359 64.9272 808.687 Q64.9272 814.162 61.8716 817.409 Q58.7843 820.623 53.6281 820.623 Q47.6125 820.623 44.5569 816.613 Q41.5014 812.571 41.5014 804.582 L41.5014 796.37 L40.9285 796.37 Q36.8862 796.37 34.6901 799.043 Q32.4621 801.685 32.4621 806.491 Q32.4621 809.547 33.1941 812.443 Q33.9262 815.34 35.3903 818.013 L29.9795 818.013 Q28.7381 814.799 28.1334 811.775 Q27.4968 808.751 27.4968 805.887 Q27.4968 798.152 31.5072 794.333 Q35.5176 790.513 43.6657 790.513 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M33.8307 757.794 Q33.2578 758.78 33.0032 759.958 Q32.7167 761.104 32.7167 762.504 Q32.7167 767.47 35.9632 770.143 Q39.1779 772.785 45.2253 772.785 L64.0042 772.785 L64.0042 778.673 L28.3562 778.673 L28.3562 772.785 L33.8944 772.785 Q30.6479 770.939 29.0883 767.979 Q27.4968 765.019 27.4968 760.786 Q27.4968 760.181 27.5923 759.449 Q27.656 758.717 27.8151 757.825 L33.8307 757.794 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M49.9359 752.255 L28.3562 752.255 L28.3562 746.399 L49.7131 746.399 Q54.7739 746.399 57.3202 744.426 Q59.8346 742.452 59.8346 738.506 Q59.8346 733.763 56.8109 731.026 Q53.7872 728.257 48.5673 728.257 L28.3562 728.257 L28.3562 722.4 L64.0042 722.4 L64.0042 728.257 L58.5296 728.257 Q61.7762 730.389 63.3676 733.222 Q64.9272 736.023 64.9272 739.747 Q64.9272 745.89 61.1078 749.073 Q57.2883 752.255 49.9359 752.255 M27.4968 737.519 L27.4968 737.519 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M33.8307 689.681 Q33.2578 690.667 33.0032 691.845 Q32.7167 692.991 32.7167 694.391 Q32.7167 699.356 35.9632 702.03 Q39.1779 704.672 45.2253 704.672 L64.0042 704.672 L64.0042 710.56 L28.3562 710.56 L28.3562 704.672 L33.8944 704.672 Q30.6479 702.826 29.0883 699.866 Q27.4968 696.906 27.4968 692.672 Q27.4968 692.068 27.5923 691.336 Q27.656 690.604 27.8151 689.712 L33.8307 689.681 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M44.7161 654.478 L47.5806 654.478 L47.5806 681.405 Q53.6281 681.023 56.8109 677.777 Q59.9619 674.498 59.9619 668.674 Q59.9619 665.3 59.1344 662.149 Q58.3069 658.966 56.6518 655.847 L62.1899 655.847 Q63.5267 658.998 64.227 662.308 Q64.9272 665.618 64.9272 669.024 Q64.9272 677.554 59.9619 682.551 Q54.9967 687.516 46.5303 687.516 Q37.7774 687.516 32.6531 682.806 Q27.4968 678.063 27.4968 670.042 Q27.4968 662.849 32.1438 658.68 Q36.7589 654.478 44.7161 654.478 M42.9973 660.335 Q38.1912 660.398 35.3266 663.04 Q32.4621 665.65 32.4621 669.979 Q32.4621 674.88 35.2312 677.84 Q38.0002 680.769 43.0292 681.214 L42.9973 660.335 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M877.575 12.096 L912.332 12.096 L912.332 18.9825 L885.758 18.9825 L885.758 36.8065 L909.739 36.8065 L909.739 43.6931 L885.758 43.6931 L885.758 72.576 L877.575 72.576 L877.575 12.096 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M959.241 48.0275 L959.241 51.6733 L924.97 51.6733 Q925.457 59.3701 929.588 63.421 Q933.761 67.4314 941.174 67.4314 Q945.468 67.4314 949.478 66.3781 Q953.529 65.3249 957.499 63.2184 L957.499 70.267 Q953.489 71.9684 949.276 72.8596 Q945.063 73.7508 940.728 73.7508 Q929.872 73.7508 923.512 67.4314 Q917.193 61.1119 917.193 50.3365 Q917.193 39.1965 923.188 32.6746 Q929.224 26.1121 939.432 26.1121 Q948.587 26.1121 953.894 32.0264 Q959.241 37.9003 959.241 48.0275 M951.787 45.84 Q951.706 39.7232 948.344 36.0774 Q945.022 32.4315 939.513 32.4315 Q933.275 32.4315 929.507 35.9558 Q925.781 39.4801 925.213 45.8805 L951.787 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M992.094 49.7694 Q983.06 49.7694 979.577 51.8354 Q976.093 53.9013 976.093 58.8839 Q976.093 62.8538 978.685 65.2034 Q981.319 67.5124 985.815 67.5124 Q992.013 67.5124 995.74 63.1374 Q999.507 58.7219 999.507 51.4303 L999.507 49.7694 L992.094 49.7694 M1006.96 46.6907 L1006.96 72.576 L999.507 72.576 L999.507 65.6895 Q996.955 69.8214 993.147 71.8063 Q989.339 73.7508 983.83 73.7508 Q976.863 73.7508 972.731 69.8619 Q968.639 65.9325 968.639 59.3701 Q968.639 51.7138 973.743 47.825 Q978.888 43.9361 989.056 43.9361 L999.507 43.9361 L999.507 43.2069 Q999.507 38.0623 996.104 35.2672 Q992.742 32.4315 986.625 32.4315 Q982.736 32.4315 979.05 33.3632 Q975.364 34.295 971.961 36.1584 L971.961 29.2718 Q976.052 27.692 979.901 26.9223 Q983.749 26.1121 987.395 26.1121 Q997.239 26.1121 1002.1 31.2163 Q1006.96 36.3204 1006.96 46.6907 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1029.69 14.324 L1029.69 27.2059 L1045.04 27.2059 L1045.04 32.9987 L1029.69 32.9987 L1029.69 57.6282 Q1029.69 63.1779 1031.19 64.7578 Q1032.72 66.3376 1037.38 66.3376 L1045.04 66.3376 L1045.04 72.576 L1037.38 72.576 Q1028.75 72.576 1025.47 69.3758 Q1022.19 66.1351 1022.19 57.6282 L1022.19 32.9987 L1016.72 32.9987 L1016.72 27.2059 L1022.19 27.2059 L1022.19 14.324 L1029.69 14.324 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1054.07 54.671 L1054.07 27.2059 L1061.53 27.2059 L1061.53 54.3874 Q1061.53 60.8284 1064.04 64.0691 Q1066.55 67.2693 1071.57 67.2693 Q1077.61 67.2693 1081.09 63.421 Q1084.62 59.5726 1084.62 52.9291 L1084.62 27.2059 L1092.07 27.2059 L1092.07 72.576 L1084.62 72.576 L1084.62 65.6084 Q1081.9 69.7404 1078.3 71.7658 Q1074.73 73.7508 1069.99 73.7508 Q1062.17 73.7508 1058.12 68.8897 Q1054.07 64.0286 1054.07 54.671 M1072.83 26.1121 L1072.83 26.1121 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1133.71 34.1734 Q1132.46 33.4443 1130.96 33.1202 Q1129.5 32.7556 1127.72 32.7556 Q1121.4 32.7556 1118 36.8875 Q1114.63 40.9789 1114.63 48.6757 L1114.63 72.576 L1107.14 72.576 L1107.14 27.2059 L1114.63 27.2059 L1114.63 34.2544 Q1116.98 30.1225 1120.75 28.1376 Q1124.52 26.1121 1129.91 26.1121 Q1130.68 26.1121 1131.61 26.2337 Q1132.54 26.3147 1133.67 26.5172 L1133.71 34.1734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1178.52 48.0275 L1178.52 51.6733 L1144.25 51.6733 Q1144.73 59.3701 1148.86 63.421 Q1153.04 67.4314 1160.45 67.4314 Q1164.74 67.4314 1168.75 66.3781 Q1172.8 65.3249 1176.77 63.2184 L1176.77 70.267 Q1172.76 71.9684 1168.55 72.8596 Q1164.34 73.7508 1160 73.7508 Q1149.15 73.7508 1142.79 67.4314 Q1136.47 61.1119 1136.47 50.3365 Q1136.47 39.1965 1142.46 32.6746 Q1148.5 26.1121 1158.71 26.1121 Q1167.86 26.1121 1173.17 32.0264 Q1178.52 37.9003 1178.52 48.0275 M1171.06 45.84 Q1170.98 39.7232 1167.62 36.0774 Q1164.3 32.4315 1158.79 32.4315 Q1152.55 32.4315 1148.78 35.9558 Q1145.06 39.4801 1144.49 45.8805 L1171.06 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1217.45 12.096 L1225.63 12.096 L1225.63 72.576 L1217.45 72.576 L1217.45 12.096 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1276.91 35.9153 Q1279.71 30.8922 1283.6 28.5022 Q1287.49 26.1121 1292.75 26.1121 Q1299.84 26.1121 1303.69 31.0947 Q1307.54 36.0368 1307.54 45.1919 L1307.54 72.576 L1300.04 72.576 L1300.04 45.4349 Q1300.04 38.913 1297.73 35.7533 Q1295.43 32.5936 1290.69 32.5936 Q1284.89 32.5936 1281.53 36.4419 Q1278.17 40.2903 1278.17 46.9338 L1278.17 72.576 L1270.67 72.576 L1270.67 45.4349 Q1270.67 38.8725 1268.37 35.7533 Q1266.06 32.5936 1261.24 32.5936 Q1255.52 32.5936 1252.16 36.4824 Q1248.8 40.3308 1248.8 46.9338 L1248.8 72.576 L1241.31 72.576 L1241.31 27.2059 L1248.8 27.2059 L1248.8 34.2544 Q1251.35 30.082 1254.92 28.0971 Q1258.48 26.1121 1263.38 26.1121 Q1268.33 26.1121 1271.77 28.6237 Q1275.25 31.1352 1276.91 35.9153 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1329.62 65.7705 L1329.62 89.8329 L1322.12 89.8329 L1322.12 27.2059 L1329.62 27.2059 L1329.62 34.0924 Q1331.96 30.0415 1335.53 28.0971 Q1339.13 26.1121 1344.12 26.1121 Q1352.38 26.1121 1357.53 32.6746 Q1362.71 39.2371 1362.71 49.9314 Q1362.71 60.6258 1357.53 67.1883 Q1352.38 73.7508 1344.12 73.7508 Q1339.13 73.7508 1335.53 71.8063 Q1331.96 69.8214 1329.62 65.7705 M1354.97 49.9314 Q1354.97 41.7081 1351.57 37.0496 Q1348.21 32.3505 1342.29 32.3505 Q1336.38 32.3505 1332.98 37.0496 Q1329.62 41.7081 1329.62 49.9314 Q1329.62 58.1548 1332.98 62.8538 Q1336.38 67.5124 1342.29 67.5124 Q1348.21 67.5124 1351.57 62.8538 Q1354.97 58.1548 1354.97 49.9314 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1392.65 32.4315 Q1386.65 32.4315 1383.17 37.1306 Q1379.68 41.7891 1379.68 49.9314 Q1379.68 58.0738 1383.13 62.7728 Q1386.61 67.4314 1392.65 67.4314 Q1398.6 67.4314 1402.09 62.7323 Q1405.57 58.0333 1405.57 49.9314 Q1405.57 41.8701 1402.09 37.1711 Q1398.6 32.4315 1392.65 32.4315 M1392.65 26.1121 Q1402.37 26.1121 1407.92 32.4315 Q1413.47 38.7509 1413.47 49.9314 Q1413.47 61.0714 1407.92 67.4314 Q1402.37 73.7508 1392.65 73.7508 Q1382.88 73.7508 1377.33 67.4314 Q1371.83 61.0714 1371.83 49.9314 Q1371.83 38.7509 1377.33 32.4315 Q1382.88 26.1121 1392.65 26.1121 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1452.11 34.1734 Q1450.86 33.4443 1449.36 33.1202 Q1447.9 32.7556 1446.12 32.7556 Q1439.8 32.7556 1436.4 36.8875 Q1433.03 40.9789 1433.03 48.6757 L1433.03 72.576 L1425.54 72.576 L1425.54 27.2059 L1433.03 27.2059 L1433.03 34.2544 Q1435.38 30.1225 1439.15 28.1376 Q1442.92 26.1121 1448.31 26.1121 Q1449.08 26.1121 1450.01 26.2337 Q1450.94 26.3147 1452.07 26.5172 L1452.11 34.1734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1467.31 14.324 L1467.31 27.2059 L1482.66 27.2059 L1482.66 32.9987 L1467.31 32.9987 L1467.31 57.6282 Q1467.31 63.1779 1468.8 64.7578 Q1470.34 66.3376 1475 66.3376 L1482.66 66.3376 L1482.66 72.576 L1475 72.576 Q1466.37 72.576 1463.09 69.3758 Q1459.81 66.1351 1459.81 57.6282 L1459.81 32.9987 L1454.34 32.9987 L1454.34 27.2059 L1459.81 27.2059 L1459.81 14.324 L1467.31 14.324 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1513.08 49.7694 Q1504.05 49.7694 1500.56 51.8354 Q1497.08 53.9013 1497.08 58.8839 Q1497.08 62.8538 1499.67 65.2034 Q1502.31 67.5124 1506.8 67.5124 Q1513 67.5124 1516.73 63.1374 Q1520.49 58.7219 1520.49 51.4303 L1520.49 49.7694 L1513.08 49.7694 M1527.95 46.6907 L1527.95 72.576 L1520.49 72.576 L1520.49 65.6895 Q1517.94 69.8214 1514.13 71.8063 Q1510.33 73.7508 1504.82 73.7508 Q1497.85 73.7508 1493.72 69.8619 Q1489.63 65.9325 1489.63 59.3701 Q1489.63 51.7138 1494.73 47.825 Q1499.87 43.9361 1510.04 43.9361 L1520.49 43.9361 L1520.49 43.2069 Q1520.49 38.0623 1517.09 35.2672 Q1513.73 32.4315 1507.61 32.4315 Q1503.72 32.4315 1500.04 33.3632 Q1496.35 34.295 1492.95 36.1584 L1492.95 29.2718 Q1497.04 27.692 1500.89 26.9223 Q1504.74 26.1121 1508.38 26.1121 Q1518.23 26.1121 1523.09 31.2163 Q1527.95 36.3204 1527.95 46.6907 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1581.01 45.1919 L1581.01 72.576 L1573.56 72.576 L1573.56 45.4349 Q1573.56 38.994 1571.05 35.7938 Q1568.54 32.5936 1563.51 32.5936 Q1557.48 32.5936 1553.99 36.4419 Q1550.51 40.2903 1550.51 46.9338 L1550.51 72.576 L1543.02 72.576 L1543.02 27.2059 L1550.51 27.2059 L1550.51 34.2544 Q1553.18 30.163 1556.79 28.1376 Q1560.44 26.1121 1565.18 26.1121 Q1572.99 26.1121 1577 30.9732 Q1581.01 35.7938 1581.01 45.1919 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1628.53 28.9478 L1628.53 35.9153 Q1625.37 34.1734 1622.17 33.3227 Q1619.01 32.4315 1615.77 32.4315 Q1608.52 32.4315 1604.51 37.0496 Q1600.5 41.6271 1600.5 49.9314 Q1600.5 58.2358 1604.51 62.8538 Q1608.52 67.4314 1615.77 67.4314 Q1619.01 67.4314 1622.17 66.5807 Q1625.37 65.6895 1628.53 63.9476 L1628.53 70.8341 Q1625.41 72.2924 1622.05 73.0216 Q1618.73 73.7508 1614.96 73.7508 Q1604.71 73.7508 1598.68 67.3098 Q1592.64 60.8689 1592.64 49.9314 Q1592.64 38.832 1598.72 32.472 Q1604.83 26.1121 1615.45 26.1121 Q1618.89 26.1121 1622.17 26.8413 Q1625.45 27.5299 1628.53 28.9478 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip530)\" d=\"M1680.3 48.0275 L1680.3 51.6733 L1646.03 51.6733 Q1646.52 59.3701 1650.65 63.421 Q1654.82 67.4314 1662.23 67.4314 Q1666.53 67.4314 1670.54 66.3781 Q1674.59 65.3249 1678.56 63.2184 L1678.56 70.267 Q1674.55 71.9684 1670.34 72.8596 Q1666.12 73.7508 1661.79 73.7508 Q1650.93 73.7508 1644.57 67.4314 Q1638.25 61.1119 1638.25 50.3365 Q1638.25 39.1965 1644.25 32.6746 Q1650.28 26.1121 1660.49 26.1121 Q1669.65 26.1121 1674.95 32.0264 Q1680.3 37.9003 1680.3 48.0275 M1672.85 45.84 Q1672.77 39.7232 1669.41 36.0774 Q1666.08 32.4315 1660.57 32.4315 Q1654.34 32.4315 1650.57 35.9558 Q1646.84 39.4801 1646.27 45.8805 L1672.85 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip532)\" d=\"M266.01 1386.4 L265.903 1386.4 L265.903 1369.99 L266.01 1369.99 L266.01 1386.4 L266.01 1386.4  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.01,1386.4 265.903,1386.4 265.903,1369.99 266.01,1369.99 266.01,1386.4 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.24 1365.89 L265.903 1365.89 L265.903 1349.49 L266.24 1349.49 L266.24 1365.89 L266.24 1365.89  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.24,1365.89 265.903,1365.89 265.903,1349.49 266.24,1349.49 266.24,1365.89 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.644 1345.39 L265.903 1345.39 L265.903 1328.98 L266.644 1328.98 L266.644 1345.39 L266.644 1345.39  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.644,1345.39 265.903,1345.39 265.903,1328.98 266.644,1328.98 266.644,1345.39 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M265.903 1324.88 L265.903 1324.88 L265.903 1308.48 L265.903 1308.48 L265.903 1324.88 L265.903 1324.88  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.903,1324.88 265.903,1324.88 265.903,1308.48 265.903,1324.88 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.2 1304.38 L265.903 1304.38 L265.903 1287.98 L266.2 1287.98 L266.2 1304.38 L266.2 1304.38  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.2,1304.38 265.903,1304.38 265.903,1287.98 266.2,1287.98 266.2,1304.38 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.219 1283.88 L265.903 1283.88 L265.903 1267.47 L266.219 1267.47 L266.219 1283.88 L266.219 1283.88  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.219,1283.88 265.903,1283.88 265.903,1267.47 266.219,1267.47 266.219,1283.88 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.371 1263.37 L265.903 1263.37 L265.903 1246.97 L266.371 1246.97 L266.371 1263.37 L266.371 1263.37  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.371,1263.37 265.903,1263.37 265.903,1246.97 266.371,1246.97 266.371,1263.37 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.161 1242.87 L265.903 1242.87 L265.903 1226.46 L266.161 1226.46 L266.161 1242.87 L266.161 1242.87  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.161,1242.87 265.903,1242.87 265.903,1226.46 266.161,1226.46 266.161,1242.87 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.361 1222.36 L265.903 1222.36 L265.903 1205.96 L266.361 1205.96 L266.361 1222.36 L266.361 1222.36  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.361,1222.36 265.903,1222.36 265.903,1205.96 266.361,1205.96 266.361,1222.36 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.993 1201.86 L265.903 1201.86 L265.903 1185.46 L266.993 1185.46 L266.993 1201.86 L266.993 1201.86  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.993,1201.86 265.903,1201.86 265.903,1185.46 266.993,1185.46 266.993,1201.86 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M268.737 1181.36 L265.903 1181.36 L265.903 1164.95 L268.737 1164.95 L268.737 1181.36 L268.737 1181.36  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"268.737,1181.36 265.903,1181.36 265.903,1164.95 268.737,1164.95 268.737,1181.36 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.957 1160.85 L265.903 1160.85 L265.903 1144.45 L266.957 1144.45 L266.957 1160.85 L266.957 1160.85  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.957,1160.85 265.903,1160.85 265.903,1144.45 266.957,1144.45 266.957,1160.85 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.68 1140.35 L265.903 1140.35 L265.903 1123.94 L266.68 1123.94 L266.68 1140.35 L266.68 1140.35  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.68,1140.35 265.903,1140.35 265.903,1123.94 266.68,1123.94 266.68,1140.35 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.252 1119.84 L265.903 1119.84 L265.903 1103.44 L266.252 1103.44 L266.252 1119.84 L266.252 1119.84  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.252,1119.84 265.903,1119.84 265.903,1103.44 266.252,1103.44 266.252,1119.84 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.628 1099.34 L265.903 1099.34 L265.903 1082.94 L266.628 1082.94 L266.628 1099.34 L266.628 1099.34  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.628,1099.34 265.903,1099.34 265.903,1082.94 266.628,1082.94 266.628,1099.34 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.153 1078.84 L265.903 1078.84 L265.903 1062.43 L266.153 1062.43 L266.153 1078.84 L266.153 1078.84  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.153,1078.84 265.903,1078.84 265.903,1062.43 266.153,1062.43 266.153,1078.84 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.513 1058.33 L265.903 1058.33 L265.903 1041.93 L266.513 1041.93 L266.513 1058.33 L266.513 1058.33  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.513,1058.33 265.903,1058.33 265.903,1041.93 266.513,1041.93 266.513,1058.33 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.354 1037.83 L265.903 1037.83 L265.903 1021.42 L266.354 1021.42 L266.354 1037.83 L266.354 1037.83  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.354,1037.83 265.903,1037.83 265.903,1021.42 266.354,1021.42 266.354,1037.83 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M265.903 1017.32 L265.903 1017.32 L265.903 1000.92 L265.903 1000.92 L265.903 1017.32 L265.903 1017.32  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.903,1017.32 265.903,1017.32 265.903,1000.92 265.903,1017.32 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.215 996.82 L265.903 996.82 L265.903 980.416 L266.215 980.416 L266.215 996.82 L266.215 996.82  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.215,996.82 265.903,996.82 265.903,980.416 266.215,980.416 266.215,996.82 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.346 976.316 L265.903 976.316 L265.903 959.912 L266.346 959.912 L266.346 976.316 L266.346 976.316  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.346,976.316 265.903,976.316 265.903,959.912 266.346,959.912 266.346,976.316 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.053 955.812 L265.903 955.812 L265.903 939.408 L266.053 939.408 L266.053 955.812 L266.053 955.812  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.053,955.812 265.903,955.812 265.903,939.408 266.053,939.408 266.053,955.812 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.4 935.308 L265.903 935.308 L265.903 918.904 L266.4 918.904 L266.4 935.308 L266.4 935.308  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.4,935.308 265.903,935.308 265.903,918.904 266.4,918.904 266.4,935.308 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.113 914.804 L265.903 914.804 L265.903 898.4 L266.113 898.4 L266.113 914.804 L266.113 914.804  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.113,914.804 265.903,914.804 265.903,898.4 266.113,898.4 266.113,914.804 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M265.923 894.3 L265.903 894.3 L265.903 877.896 L265.923 877.896 L265.923 894.3 L265.923 894.3  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.923,894.3 265.903,894.3 265.903,877.896 265.923,877.896 265.923,894.3 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M265.903 873.796 L265.903 873.796 L265.903 857.392 L265.903 857.392 L265.903 873.796 L265.903 873.796  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.903,873.796 265.903,873.796 265.903,857.392 265.903,873.796 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M265.903 853.292 L265.903 853.292 L265.903 836.888 L265.903 836.888 L265.903 853.292 L265.903 853.292  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.903,853.292 265.903,853.292 265.903,836.888 265.903,853.292 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.644 832.788 L265.903 832.788 L265.903 816.384 L266.644 816.384 L266.644 832.788 L266.644 832.788  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.644,832.788 265.903,832.788 265.903,816.384 266.644,816.384 266.644,832.788 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.551 812.284 L265.903 812.284 L265.903 795.88 L266.551 795.88 L266.551 812.284 L266.551 812.284  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.551,812.284 265.903,812.284 265.903,795.88 266.551,795.88 266.551,812.284 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.003 791.78 L265.903 791.78 L265.903 775.376 L266.003 775.376 L266.003 791.78 L266.003 791.78  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.003,791.78 265.903,791.78 265.903,775.376 266.003,775.376 266.003,791.78 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.874 771.276 L265.903 771.276 L265.903 754.872 L266.874 754.872 L266.874 771.276 L266.874 771.276  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.874,771.276 265.903,771.276 265.903,754.872 266.874,754.872 266.874,771.276 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.271 750.772 L265.903 750.772 L265.903 734.368 L266.271 734.368 L266.271 750.772 L266.271 750.772  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.271,750.772 265.903,750.772 265.903,734.368 266.271,734.368 266.271,750.772 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.27 730.268 L265.903 730.268 L265.903 713.864 L266.27 713.864 L266.27 730.268 L266.27 730.268  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.27,730.268 265.903,730.268 265.903,713.864 266.27,713.864 266.27,730.268 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.829 709.764 L265.903 709.764 L265.903 693.36 L266.829 693.36 L266.829 709.764 L266.829 709.764  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.829,709.764 265.903,709.764 265.903,693.36 266.829,693.36 266.829,709.764 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.103 689.26 L265.903 689.26 L265.903 672.856 L266.103 672.856 L266.103 689.26 L266.103 689.26  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.103,689.26 265.903,689.26 265.903,672.856 266.103,672.856 266.103,689.26 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M267.405 668.756 L265.903 668.756 L265.903 652.352 L267.405 652.352 L267.405 668.756 L267.405 668.756  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"267.405,668.756 265.903,668.756 265.903,652.352 267.405,652.352 267.405,668.756 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.074 648.252 L265.903 648.252 L265.903 631.848 L266.074 631.848 L266.074 648.252 L266.074 648.252  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.074,648.252 265.903,648.252 265.903,631.848 266.074,631.848 266.074,648.252 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.824 627.748 L265.903 627.748 L265.903 611.344 L266.824 611.344 L266.824 627.748 L266.824 627.748  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.824,627.748 265.903,627.748 265.903,611.344 266.824,611.344 266.824,627.748 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.364 607.244 L265.903 607.244 L265.903 590.84 L266.364 590.84 L266.364 607.244 L266.364 607.244  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.364,607.244 265.903,607.244 265.903,590.84 266.364,590.84 266.364,607.244 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.557 586.74 L265.903 586.74 L265.903 570.336 L266.557 570.336 L266.557 586.74 L266.557 586.74  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.557,586.74 265.903,586.74 265.903,570.336 266.557,570.336 266.557,586.74 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M265.903 566.236 L265.903 566.236 L265.903 549.832 L265.903 549.832 L265.903 566.236 L265.903 566.236  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.903,566.236 265.903,566.236 265.903,549.832 265.903,566.236 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M265.903 545.732 L265.903 545.732 L265.903 529.328 L265.903 529.328 L265.903 545.732 L265.903 545.732  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.903,545.732 265.903,545.732 265.903,529.328 265.903,545.732 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.284 525.228 L265.903 525.228 L265.903 508.824 L266.284 508.824 L266.284 525.228 L266.284 525.228  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.284,525.228 265.903,525.228 265.903,508.824 266.284,508.824 266.284,525.228 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M267.018 504.724 L265.903 504.724 L265.903 488.32 L267.018 488.32 L267.018 504.724 L267.018 504.724  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"267.018,504.724 265.903,504.724 265.903,488.32 267.018,488.32 267.018,504.724 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M268.055 484.22 L265.903 484.22 L265.903 467.816 L268.055 467.816 L268.055 484.22 L268.055 484.22  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"268.055,484.22 265.903,484.22 265.903,467.816 268.055,467.816 268.055,484.22 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.09 463.716 L265.903 463.716 L265.903 447.312 L266.09 447.312 L266.09 463.716 L266.09 463.716  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.09,463.716 265.903,463.716 265.903,447.312 266.09,447.312 266.09,463.716 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.724 443.212 L265.903 443.212 L265.903 426.808 L266.724 426.808 L266.724 443.212 L266.724 443.212  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.724,443.212 265.903,443.212 265.903,426.808 266.724,426.808 266.724,443.212 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M267.114 422.708 L265.903 422.708 L265.903 406.304 L267.114 406.304 L267.114 422.708 L267.114 422.708  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"267.114,422.708 265.903,422.708 265.903,406.304 267.114,406.304 267.114,422.708 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.581 402.204 L265.903 402.204 L265.903 385.8 L266.581 385.8 L266.581 402.204 L266.581 402.204  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.581,402.204 265.903,402.204 265.903,385.8 266.581,385.8 266.581,402.204 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M267.159 381.7 L265.903 381.7 L265.903 365.296 L267.159 365.296 L267.159 381.7 L267.159 381.7  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"267.159,381.7 265.903,381.7 265.903,365.296 267.159,365.296 267.159,381.7 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.011 361.196 L265.903 361.196 L265.903 344.792 L266.011 344.792 L266.011 361.196 L266.011 361.196  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.011,361.196 265.903,361.196 265.903,344.792 266.011,344.792 266.011,361.196 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.552 340.692 L265.903 340.692 L265.903 324.288 L266.552 324.288 L266.552 340.692 L266.552 340.692  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.552,340.692 265.903,340.692 265.903,324.288 266.552,324.288 266.552,340.692 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.507 320.188 L265.903 320.188 L265.903 303.784 L266.507 303.784 L266.507 320.188 L266.507 320.188  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.507,320.188 265.903,320.188 265.903,303.784 266.507,303.784 266.507,320.188 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.97 299.684 L265.903 299.684 L265.903 283.28 L266.97 283.28 L266.97 299.684 L266.97 299.684  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.97,299.684 265.903,299.684 265.903,283.28 266.97,283.28 266.97,299.684 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.633 279.18 L265.903 279.18 L265.903 262.776 L266.633 262.776 L266.633 279.18 L266.633 279.18  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.633,279.18 265.903,279.18 265.903,262.776 266.633,262.776 266.633,279.18 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.101 258.676 L265.903 258.676 L265.903 242.272 L266.101 242.272 L266.101 258.676 L266.101 258.676  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.101,258.676 265.903,258.676 265.903,242.272 266.101,242.272 266.101,258.676 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.184 238.172 L265.903 238.172 L265.903 221.768 L266.184 221.768 L266.184 238.172 L266.184 238.172  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.184,238.172 265.903,238.172 265.903,221.768 266.184,221.768 266.184,238.172 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M266.012 217.668 L265.903 217.668 L265.903 201.264 L266.012 201.264 L266.012 217.668 L266.012 217.668  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.012,217.668 265.903,217.668 265.903,201.264 266.012,201.264 266.012,217.668 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M267.266 197.164 L265.903 197.164 L265.903 180.76 L267.266 180.76 L267.266 197.164 L267.266 197.164  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"267.266,197.164 265.903,197.164 265.903,180.76 267.266,180.76 267.266,197.164 \"/>\n",
       "<path clip-path=\"url(#clip532)\" d=\"M265.983 176.66 L265.903 176.66 L265.903 160.256 L265.983 160.256 L265.983 176.66 L265.983 176.66  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip532)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.983,176.66 265.903,176.66 265.903,160.256 265.983,160.256 265.983,176.66 \"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"299.671\" cy=\"1398.63\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"333.439\" cy=\"1398.49\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"367.207\" cy=\"1398.25\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"400.974\" cy=\"1398.7\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"434.742\" cy=\"1398.52\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"468.51\" cy=\"1398.51\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"502.278\" cy=\"1398.41\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"536.046\" cy=\"1398.54\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"569.814\" cy=\"1398.42\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"603.582\" cy=\"1398.04\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"637.349\" cy=\"1396.98\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"671.117\" cy=\"1398.06\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"704.885\" cy=\"1398.23\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"738.653\" cy=\"1398.49\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"772.421\" cy=\"1398.26\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"806.189\" cy=\"1398.55\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"839.956\" cy=\"1398.33\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"873.724\" cy=\"1398.42\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"907.492\" cy=\"1398.7\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"941.26\" cy=\"1398.51\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"975.028\" cy=\"1398.43\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1008.8\" cy=\"1398.61\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1042.56\" cy=\"1398.4\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1076.33\" cy=\"1398.57\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1110.1\" cy=\"1398.69\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1143.87\" cy=\"1398.7\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1177.63\" cy=\"1398.7\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1211.4\" cy=\"1398.25\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1245.17\" cy=\"1398.3\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1278.94\" cy=\"1398.64\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1312.71\" cy=\"1398.11\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1346.47\" cy=\"1398.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1380.24\" cy=\"1398.48\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1414.01\" cy=\"1398.14\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1447.78\" cy=\"1398.58\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1481.55\" cy=\"1397.79\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1515.31\" cy=\"1398.59\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1549.08\" cy=\"1398.14\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1582.85\" cy=\"1398.42\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1616.62\" cy=\"1398.3\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1650.38\" cy=\"1398.7\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1684.15\" cy=\"1398.7\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1717.92\" cy=\"1398.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1751.69\" cy=\"1398.02\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1785.46\" cy=\"1397.39\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1819.22\" cy=\"1398.58\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1852.99\" cy=\"1398.2\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1886.76\" cy=\"1397.96\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1920.53\" cy=\"1398.29\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1954.3\" cy=\"1397.94\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1988.06\" cy=\"1398.63\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2021.83\" cy=\"1398.3\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2055.6\" cy=\"1398.33\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2089.37\" cy=\"1398.05\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2123.13\" cy=\"1398.25\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2156.9\" cy=\"1398.58\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2190.67\" cy=\"1398.53\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2224.44\" cy=\"1398.63\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2258.21\" cy=\"1397.87\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip532)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2291.97\" cy=\"1398.65\" r=\"2\"/>\n",
       "</svg>\n"
      ],
      "text/html": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip580\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip580)\" d=\"M0 1600 L2400 1600 L2400 0 L0 0  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip581\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip580)\" d=\"M205.121 1423.18 L2352.76 1423.18 L2352.76 123.472 L205.121 123.472  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip582\">\n",
       "    <rect x=\"205\" y=\"123\" width=\"2149\" height=\"1301\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"265.903,1423.18 265.903,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"603.582,1423.18 603.582,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"941.26,1423.18 941.26,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1278.94,1423.18 1278.94,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1616.62,1423.18 1616.62,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1954.3,1423.18 1954.3,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2291.97,1423.18 2291.97,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 2352.76,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.903,1423.18 265.903,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"603.582,1423.18 603.582,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"941.26,1423.18 941.26,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1278.94,1423.18 1278.94,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1616.62,1423.18 1616.62,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1954.3,1423.18 1954.3,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2291.97,1423.18 2291.97,1404.28 \"/>\n",
       "<path clip-path=\"url(#clip580)\" d=\"M265.903 1454.1 Q262.292 1454.1 260.463 1457.66 Q258.658 1461.2 258.658 1468.33 Q258.658 1475.44 260.463 1479.01 Q262.292 1482.55 265.903 1482.55 Q269.537 1482.55 271.343 1479.01 Q273.172 1475.44 273.172 1468.33 Q273.172 1461.2 271.343 1457.66 Q269.537 1454.1 265.903 1454.1 M265.903 1450.39 Q271.713 1450.39 274.769 1455 Q277.847 1459.58 277.847 1468.33 Q277.847 1477.06 274.769 1481.67 Q271.713 1486.25 265.903 1486.25 Q260.093 1486.25 257.014 1481.67 Q253.959 1477.06 253.959 1468.33 Q253.959 1459.58 257.014 1455 Q260.093 1450.39 265.903 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M578.269 1481.64 L585.908 1481.64 L585.908 1455.28 L577.598 1456.95 L577.598 1452.69 L585.862 1451.02 L590.538 1451.02 L590.538 1481.64 L598.176 1481.64 L598.176 1485.58 L578.269 1485.58 L578.269 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M617.621 1454.1 Q614.01 1454.1 612.181 1457.66 Q610.375 1461.2 610.375 1468.33 Q610.375 1475.44 612.181 1479.01 Q614.01 1482.55 617.621 1482.55 Q621.255 1482.55 623.061 1479.01 Q624.889 1475.44 624.889 1468.33 Q624.889 1461.2 623.061 1457.66 Q621.255 1454.1 617.621 1454.1 M617.621 1450.39 Q623.431 1450.39 626.487 1455 Q629.565 1459.58 629.565 1468.33 Q629.565 1477.06 626.487 1481.67 Q623.431 1486.25 617.621 1486.25 Q611.811 1486.25 608.732 1481.67 Q605.676 1477.06 605.676 1468.33 Q605.676 1459.58 608.732 1455 Q611.811 1450.39 617.621 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M920.033 1481.64 L936.353 1481.64 L936.353 1485.58 L914.408 1485.58 L914.408 1481.64 Q917.07 1478.89 921.654 1474.26 Q926.26 1469.61 927.441 1468.27 Q929.686 1465.74 930.566 1464.01 Q931.468 1462.25 931.468 1460.56 Q931.468 1457.8 929.524 1456.07 Q927.603 1454.33 924.501 1454.33 Q922.302 1454.33 919.848 1455.09 Q917.418 1455.86 914.64 1457.41 L914.64 1452.69 Q917.464 1451.55 919.918 1450.97 Q922.371 1450.39 924.408 1450.39 Q929.779 1450.39 932.973 1453.08 Q936.167 1455.77 936.167 1460.26 Q936.167 1462.39 935.357 1464.31 Q934.57 1466.2 932.464 1468.8 Q931.885 1469.47 928.783 1472.69 Q925.681 1475.88 920.033 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M956.167 1454.1 Q952.556 1454.1 950.728 1457.66 Q948.922 1461.2 948.922 1468.33 Q948.922 1475.44 950.728 1479.01 Q952.556 1482.55 956.167 1482.55 Q959.802 1482.55 961.607 1479.01 Q963.436 1475.44 963.436 1468.33 Q963.436 1461.2 961.607 1457.66 Q959.802 1454.1 956.167 1454.1 M956.167 1450.39 Q961.977 1450.39 965.033 1455 Q968.112 1459.58 968.112 1468.33 Q968.112 1477.06 965.033 1481.67 Q961.977 1486.25 956.167 1486.25 Q950.357 1486.25 947.278 1481.67 Q944.223 1477.06 944.223 1468.33 Q944.223 1459.58 947.278 1455 Q950.357 1450.39 956.167 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1267.78 1466.95 Q1271.14 1467.66 1273.01 1469.93 Q1274.91 1472.2 1274.91 1475.53 Q1274.91 1480.65 1271.39 1483.45 Q1267.87 1486.25 1261.39 1486.25 Q1259.22 1486.25 1256.9 1485.81 Q1254.61 1485.39 1252.16 1484.54 L1252.16 1480.02 Q1254.1 1481.16 1256.42 1481.74 Q1258.73 1482.32 1261.25 1482.32 Q1265.65 1482.32 1267.94 1480.58 Q1270.26 1478.84 1270.26 1475.53 Q1270.26 1472.48 1268.11 1470.77 Q1265.98 1469.03 1262.16 1469.03 L1258.13 1469.03 L1258.13 1465.19 L1262.34 1465.19 Q1265.79 1465.19 1267.62 1463.82 Q1269.45 1462.43 1269.45 1459.84 Q1269.45 1457.18 1267.55 1455.77 Q1265.67 1454.33 1262.16 1454.33 Q1260.23 1454.33 1258.04 1454.75 Q1255.84 1455.16 1253.2 1456.04 L1253.2 1451.88 Q1255.86 1451.14 1258.17 1450.77 Q1260.51 1450.39 1262.57 1450.39 Q1267.9 1450.39 1271 1452.83 Q1274.1 1455.23 1274.1 1459.35 Q1274.1 1462.22 1272.46 1464.21 Q1270.81 1466.18 1267.78 1466.95 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1293.78 1454.1 Q1290.17 1454.1 1288.34 1457.66 Q1286.53 1461.2 1286.53 1468.33 Q1286.53 1475.44 1288.34 1479.01 Q1290.17 1482.55 1293.78 1482.55 Q1297.41 1482.55 1299.22 1479.01 Q1301.04 1475.44 1301.04 1468.33 Q1301.04 1461.2 1299.22 1457.66 Q1297.41 1454.1 1293.78 1454.1 M1293.78 1450.39 Q1299.59 1450.39 1302.64 1455 Q1305.72 1459.58 1305.72 1468.33 Q1305.72 1477.06 1302.64 1481.67 Q1299.59 1486.25 1293.78 1486.25 Q1287.97 1486.25 1284.89 1481.67 Q1281.83 1477.06 1281.83 1468.33 Q1281.83 1459.58 1284.89 1455 Q1287.97 1450.39 1293.78 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1604.79 1455.09 L1592.98 1473.54 L1604.79 1473.54 L1604.79 1455.09 M1603.56 1451.02 L1609.44 1451.02 L1609.44 1473.54 L1614.37 1473.54 L1614.37 1477.43 L1609.44 1477.43 L1609.44 1485.58 L1604.79 1485.58 L1604.79 1477.43 L1589.19 1477.43 L1589.19 1472.92 L1603.56 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1632.1 1454.1 Q1628.49 1454.1 1626.66 1457.66 Q1624.86 1461.2 1624.86 1468.33 Q1624.86 1475.44 1626.66 1479.01 Q1628.49 1482.55 1632.1 1482.55 Q1635.74 1482.55 1637.54 1479.01 Q1639.37 1475.44 1639.37 1468.33 Q1639.37 1461.2 1637.54 1457.66 Q1635.74 1454.1 1632.1 1454.1 M1632.1 1450.39 Q1637.91 1450.39 1640.97 1455 Q1644.05 1459.58 1644.05 1468.33 Q1644.05 1477.06 1640.97 1481.67 Q1637.91 1486.25 1632.1 1486.25 Q1626.29 1486.25 1623.21 1481.67 Q1620.16 1477.06 1620.16 1468.33 Q1620.16 1459.58 1623.21 1455 Q1626.29 1450.39 1632.1 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1928.99 1451.02 L1947.35 1451.02 L1947.35 1454.96 L1933.28 1454.96 L1933.28 1463.43 Q1934.3 1463.08 1935.31 1462.92 Q1936.33 1462.73 1937.35 1462.73 Q1943.14 1462.73 1946.52 1465.9 Q1949.9 1469.08 1949.9 1474.49 Q1949.9 1480.07 1946.43 1483.17 Q1942.95 1486.25 1936.63 1486.25 Q1934.46 1486.25 1932.19 1485.88 Q1929.94 1485.51 1927.54 1484.77 L1927.54 1480.07 Q1929.62 1481.2 1931.84 1481.76 Q1934.06 1482.32 1936.54 1482.32 Q1940.55 1482.32 1942.88 1480.21 Q1945.22 1478.1 1945.22 1474.49 Q1945.22 1470.88 1942.88 1468.77 Q1940.55 1466.67 1936.54 1466.67 Q1934.67 1466.67 1932.79 1467.08 Q1930.94 1467.5 1928.99 1468.38 L1928.99 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1969.11 1454.1 Q1965.5 1454.1 1963.67 1457.66 Q1961.86 1461.2 1961.86 1468.33 Q1961.86 1475.44 1963.67 1479.01 Q1965.5 1482.55 1969.11 1482.55 Q1972.74 1482.55 1974.55 1479.01 Q1976.38 1475.44 1976.38 1468.33 Q1976.38 1461.2 1974.55 1457.66 Q1972.74 1454.1 1969.11 1454.1 M1969.11 1450.39 Q1974.92 1450.39 1977.98 1455 Q1981.05 1459.58 1981.05 1468.33 Q1981.05 1477.06 1977.98 1481.67 Q1974.92 1486.25 1969.11 1486.25 Q1963.3 1486.25 1960.22 1481.67 Q1957.17 1477.06 1957.17 1468.33 Q1957.17 1459.58 1960.22 1455 Q1963.3 1450.39 1969.11 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M2277.38 1466.44 Q2274.23 1466.44 2272.38 1468.59 Q2270.55 1470.74 2270.55 1474.49 Q2270.55 1478.22 2272.38 1480.39 Q2274.23 1482.55 2277.38 1482.55 Q2280.53 1482.55 2282.36 1480.39 Q2284.21 1478.22 2284.21 1474.49 Q2284.21 1470.74 2282.36 1468.59 Q2280.53 1466.44 2277.38 1466.44 M2286.66 1451.78 L2286.66 1456.04 Q2284.9 1455.21 2283.1 1454.77 Q2281.31 1454.33 2279.55 1454.33 Q2274.93 1454.33 2272.47 1457.45 Q2270.04 1460.58 2269.69 1466.9 Q2271.06 1464.89 2273.12 1463.82 Q2275.18 1462.73 2277.66 1462.73 Q2282.87 1462.73 2285.87 1465.9 Q2288.91 1469.05 2288.91 1474.49 Q2288.91 1479.82 2285.76 1483.03 Q2282.61 1486.25 2277.38 1486.25 Q2271.38 1486.25 2268.21 1481.67 Q2265.04 1477.06 2265.04 1468.33 Q2265.04 1460.14 2268.93 1455.28 Q2272.82 1450.39 2279.37 1450.39 Q2281.13 1450.39 2282.91 1450.74 Q2284.72 1451.09 2286.66 1451.78 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M2306.96 1454.1 Q2303.35 1454.1 2301.52 1457.66 Q2299.72 1461.2 2299.72 1468.33 Q2299.72 1475.44 2301.52 1479.01 Q2303.35 1482.55 2306.96 1482.55 Q2310.6 1482.55 2312.4 1479.01 Q2314.23 1475.44 2314.23 1468.33 Q2314.23 1461.2 2312.4 1457.66 Q2310.6 1454.1 2306.96 1454.1 M2306.96 1450.39 Q2312.77 1450.39 2315.83 1455 Q2318.91 1459.58 2318.91 1468.33 Q2318.91 1477.06 2315.83 1481.67 Q2312.77 1486.25 2306.96 1486.25 Q2301.15 1486.25 2298.07 1481.67 Q2295.02 1477.06 2295.02 1468.33 Q2295.02 1459.58 2298.07 1455 Q2301.15 1450.39 2306.96 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1169.35 1561.26 L1169.35 1548.5 L1158.85 1548.5 L1158.85 1543.22 L1175.72 1543.22 L1175.72 1563.62 Q1171.99 1566.26 1167.51 1567.63 Q1163.02 1568.97 1157.93 1568.97 Q1146.79 1568.97 1140.48 1562.47 Q1134.21 1555.95 1134.21 1544.33 Q1134.21 1532.68 1140.48 1526.19 Q1146.79 1519.66 1157.93 1519.66 Q1162.57 1519.66 1166.74 1520.81 Q1170.94 1521.96 1174.48 1524.18 L1174.48 1531.03 Q1170.91 1528 1166.9 1526.48 Q1162.89 1524.95 1158.47 1524.95 Q1149.75 1524.95 1145.35 1529.82 Q1140.99 1534.69 1140.99 1544.33 Q1140.99 1553.94 1145.35 1558.81 Q1149.75 1563.68 1158.47 1563.68 Q1161.87 1563.68 1164.55 1563.11 Q1167.22 1562.51 1169.35 1561.26 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1187.21 1532.4 L1193.07 1532.4 L1193.07 1568.04 L1187.21 1568.04 L1187.21 1532.4 M1187.21 1518.52 L1193.07 1518.52 L1193.07 1525.93 L1187.21 1525.93 L1187.21 1518.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1234.95 1546.53 L1234.95 1568.04 L1229.09 1568.04 L1229.09 1546.72 Q1229.09 1541.66 1227.12 1539.14 Q1225.15 1536.63 1221.2 1536.63 Q1216.46 1536.63 1213.72 1539.65 Q1210.98 1542.68 1210.98 1547.9 L1210.98 1568.04 L1205.1 1568.04 L1205.1 1532.4 L1210.98 1532.4 L1210.98 1537.93 Q1213.09 1534.72 1215.92 1533.13 Q1218.78 1531.54 1222.51 1531.54 Q1228.65 1531.54 1231.8 1535.36 Q1234.95 1539.14 1234.95 1546.53 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1246.63 1532.4 L1252.49 1532.4 L1252.49 1568.04 L1246.63 1568.04 L1246.63 1532.4 M1246.63 1518.52 L1252.49 1518.52 L1252.49 1525.93 L1246.63 1525.93 L1246.63 1518.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1318.12 1561.26 L1318.12 1548.5 L1307.62 1548.5 L1307.62 1543.22 L1324.49 1543.22 L1324.49 1563.62 Q1320.76 1566.26 1316.27 1567.63 Q1311.79 1568.97 1306.69 1568.97 Q1295.55 1568.97 1289.25 1562.47 Q1282.98 1555.95 1282.98 1544.33 Q1282.98 1532.68 1289.25 1526.19 Q1295.55 1519.66 1306.69 1519.66 Q1311.34 1519.66 1315.51 1520.81 Q1319.71 1521.96 1323.24 1524.18 L1323.24 1531.03 Q1319.68 1528 1315.67 1526.48 Q1311.66 1524.95 1307.23 1524.95 Q1298.51 1524.95 1294.12 1529.82 Q1289.76 1534.69 1289.76 1544.33 Q1289.76 1553.94 1294.12 1558.81 Q1298.51 1563.68 1307.23 1563.68 Q1310.64 1563.68 1313.31 1563.11 Q1315.99 1562.51 1318.12 1561.26 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1352.18 1550.12 Q1345.08 1550.12 1342.34 1551.75 Q1339.6 1553.37 1339.6 1557.29 Q1339.6 1560.4 1341.64 1562.25 Q1343.71 1564.07 1347.24 1564.07 Q1352.11 1564.07 1355.04 1560.63 Q1358 1557.16 1358 1551.43 L1358 1550.12 L1352.18 1550.12 M1363.86 1547.71 L1363.86 1568.04 L1358 1568.04 L1358 1562.63 Q1356 1565.88 1353 1567.44 Q1350.01 1568.97 1345.68 1568.97 Q1340.21 1568.97 1336.96 1565.91 Q1333.75 1562.82 1333.75 1557.67 Q1333.75 1551.65 1337.76 1548.6 Q1341.8 1545.54 1349.79 1545.54 L1358 1545.54 L1358 1544.97 Q1358 1540.93 1355.33 1538.73 Q1352.69 1536.5 1347.88 1536.5 Q1344.82 1536.5 1341.93 1537.23 Q1339.03 1537.97 1336.36 1539.43 L1336.36 1534.02 Q1339.57 1532.78 1342.6 1532.17 Q1345.62 1531.54 1348.48 1531.54 Q1356.22 1531.54 1360.04 1535.55 Q1363.86 1539.56 1363.86 1547.71 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1375.92 1532.4 L1381.78 1532.4 L1381.78 1568.04 L1375.92 1568.04 L1375.92 1532.4 M1375.92 1518.52 L1381.78 1518.52 L1381.78 1525.93 L1375.92 1525.93 L1375.92 1518.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1423.66 1546.53 L1423.66 1568.04 L1417.81 1568.04 L1417.81 1546.72 Q1417.81 1541.66 1415.83 1539.14 Q1413.86 1536.63 1409.91 1536.63 Q1405.17 1536.63 1402.43 1539.65 Q1399.7 1542.68 1399.7 1547.9 L1399.7 1568.04 L1393.81 1568.04 L1393.81 1532.4 L1399.7 1532.4 L1399.7 1537.93 Q1401.8 1534.72 1404.63 1533.13 Q1407.49 1531.54 1411.22 1531.54 Q1417.36 1531.54 1420.51 1535.36 Q1423.66 1539.14 1423.66 1546.53 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,1398.7 2352.76,1398.7 \"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,1193.66 2352.76,1193.66 \"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,988.618 2352.76,988.618 \"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,783.578 2352.76,783.578 \"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,578.538 2352.76,578.538 \"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,373.498 2352.76,373.498 \"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,168.458 2352.76,168.458 \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 205.121,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1398.7 224.019,1398.7 \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1193.66 224.019,1193.66 \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,988.618 224.019,988.618 \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,783.578 224.019,783.578 \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,578.538 224.019,578.538 \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,373.498 224.019,373.498 \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,168.458 224.019,168.458 \"/>\n",
       "<path clip-path=\"url(#clip580)\" d=\"M157.177 1384.5 Q153.566 1384.5 151.737 1388.06 Q149.931 1391.6 149.931 1398.73 Q149.931 1405.84 151.737 1409.4 Q153.566 1412.95 157.177 1412.95 Q160.811 1412.95 162.616 1409.4 Q164.445 1405.84 164.445 1398.73 Q164.445 1391.6 162.616 1388.06 Q160.811 1384.5 157.177 1384.5 M157.177 1380.79 Q162.987 1380.79 166.042 1385.4 Q169.121 1389.98 169.121 1398.73 Q169.121 1407.46 166.042 1412.07 Q162.987 1416.65 157.177 1416.65 Q151.366 1416.65 148.288 1412.07 Q145.232 1407.46 145.232 1398.73 Q145.232 1389.98 148.288 1385.4 Q151.366 1380.79 157.177 1380.79 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M117.825 1207 L125.464 1207 L125.464 1180.64 L117.154 1182.3 L117.154 1178.04 L125.418 1176.38 L130.093 1176.38 L130.093 1207 L137.732 1207 L137.732 1210.94 L117.825 1210.94 L117.825 1207 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M157.177 1179.46 Q153.566 1179.46 151.737 1183.02 Q149.931 1186.56 149.931 1193.69 Q149.931 1200.8 151.737 1204.36 Q153.566 1207.91 157.177 1207.91 Q160.811 1207.91 162.616 1204.36 Q164.445 1200.8 164.445 1193.69 Q164.445 1186.56 162.616 1183.02 Q160.811 1179.46 157.177 1179.46 M157.177 1175.75 Q162.987 1175.75 166.042 1180.36 Q169.121 1184.94 169.121 1193.69 Q169.121 1202.42 166.042 1207.03 Q162.987 1211.61 157.177 1211.61 Q151.366 1211.61 148.288 1207.03 Q145.232 1202.42 145.232 1193.69 Q145.232 1184.94 148.288 1180.36 Q151.366 1175.75 157.177 1175.75 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M121.043 1001.96 L137.362 1001.96 L137.362 1005.9 L115.418 1005.9 L115.418 1001.96 Q118.08 999.208 122.663 994.579 Q127.269 989.926 128.45 988.583 Q130.695 986.06 131.575 984.324 Q132.478 982.565 132.478 980.875 Q132.478 978.12 130.533 976.384 Q128.612 974.648 125.51 974.648 Q123.311 974.648 120.857 975.412 Q118.427 976.176 115.649 977.727 L115.649 973.005 Q118.473 971.87 120.927 971.292 Q123.38 970.713 125.418 970.713 Q130.788 970.713 133.982 973.398 Q137.177 976.083 137.177 980.574 Q137.177 982.704 136.367 984.625 Q135.579 986.523 133.473 989.116 Q132.894 989.787 129.792 993.005 Q126.691 996.199 121.043 1001.96 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M157.177 974.417 Q153.566 974.417 151.737 977.981 Q149.931 981.523 149.931 988.653 Q149.931 995.759 151.737 999.324 Q153.566 1002.87 157.177 1002.87 Q160.811 1002.87 162.616 999.324 Q164.445 995.759 164.445 988.653 Q164.445 981.523 162.616 977.981 Q160.811 974.417 157.177 974.417 M157.177 970.713 Q162.987 970.713 166.042 975.319 Q169.121 979.903 169.121 988.653 Q169.121 997.38 166.042 1001.99 Q162.987 1006.57 157.177 1006.57 Q151.366 1006.57 148.288 1001.99 Q145.232 997.38 145.232 988.653 Q145.232 979.903 148.288 975.319 Q151.366 970.713 157.177 970.713 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M131.181 782.224 Q134.538 782.941 136.413 785.21 Q138.311 787.478 138.311 790.812 Q138.311 795.927 134.792 798.728 Q131.274 801.529 124.793 801.529 Q122.617 801.529 120.302 801.089 Q118.01 800.673 115.556 799.816 L115.556 795.302 Q117.501 796.437 119.816 797.015 Q122.13 797.594 124.654 797.594 Q129.052 797.594 131.343 795.858 Q133.658 794.122 133.658 790.812 Q133.658 787.756 131.505 786.043 Q129.376 784.307 125.556 784.307 L121.529 784.307 L121.529 780.465 L125.742 780.465 Q129.191 780.465 131.019 779.099 Q132.848 777.71 132.848 775.117 Q132.848 772.455 130.95 771.043 Q129.075 769.608 125.556 769.608 Q123.635 769.608 121.436 770.025 Q119.237 770.441 116.598 771.321 L116.598 767.154 Q119.26 766.414 121.575 766.043 Q123.913 765.673 125.973 765.673 Q131.297 765.673 134.399 768.104 Q137.501 770.511 137.501 774.631 Q137.501 777.502 135.857 779.492 Q134.214 781.46 131.181 782.224 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M157.177 769.377 Q153.566 769.377 151.737 772.941 Q149.931 776.483 149.931 783.613 Q149.931 790.719 151.737 794.284 Q153.566 797.826 157.177 797.826 Q160.811 797.826 162.616 794.284 Q164.445 790.719 164.445 783.613 Q164.445 776.483 162.616 772.941 Q160.811 769.377 157.177 769.377 M157.177 765.673 Q162.987 765.673 166.042 770.279 Q169.121 774.863 169.121 783.613 Q169.121 792.34 166.042 796.946 Q162.987 801.529 157.177 801.529 Q151.366 801.529 148.288 796.946 Q145.232 792.34 145.232 783.613 Q145.232 774.863 148.288 770.279 Q151.366 765.673 157.177 765.673 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M129.862 565.332 L118.056 583.781 L129.862 583.781 L129.862 565.332 M128.635 561.258 L134.515 561.258 L134.515 583.781 L139.445 583.781 L139.445 587.67 L134.515 587.67 L134.515 595.818 L129.862 595.818 L129.862 587.67 L114.26 587.67 L114.26 583.156 L128.635 561.258 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M157.177 564.337 Q153.566 564.337 151.737 567.901 Q149.931 571.443 149.931 578.573 Q149.931 585.679 151.737 589.244 Q153.566 592.786 157.177 592.786 Q160.811 592.786 162.616 589.244 Q164.445 585.679 164.445 578.573 Q164.445 571.443 162.616 567.901 Q160.811 564.337 157.177 564.337 M157.177 560.633 Q162.987 560.633 166.042 565.239 Q169.121 569.823 169.121 578.573 Q169.121 587.299 166.042 591.906 Q162.987 596.489 157.177 596.489 Q151.366 596.489 148.288 591.906 Q145.232 587.299 145.232 578.573 Q145.232 569.823 148.288 565.239 Q151.366 560.633 157.177 560.633 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M117.061 356.218 L135.417 356.218 L135.417 360.153 L121.343 360.153 L121.343 368.625 Q122.362 368.278 123.38 368.116 Q124.399 367.931 125.418 367.931 Q131.205 367.931 134.584 371.102 Q137.964 374.273 137.964 379.69 Q137.964 385.269 134.492 388.371 Q131.019 391.449 124.7 391.449 Q122.524 391.449 120.255 391.079 Q118.01 390.708 115.603 389.968 L115.603 385.269 Q117.686 386.403 119.908 386.959 Q122.13 387.514 124.607 387.514 Q128.612 387.514 130.95 385.408 Q133.288 383.301 133.288 379.69 Q133.288 376.079 130.95 373.972 Q128.612 371.866 124.607 371.866 Q122.732 371.866 120.857 372.283 Q119.006 372.699 117.061 373.579 L117.061 356.218 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M157.177 359.297 Q153.566 359.297 151.737 362.861 Q149.931 366.403 149.931 373.533 Q149.931 380.639 151.737 384.204 Q153.566 387.746 157.177 387.746 Q160.811 387.746 162.616 384.204 Q164.445 380.639 164.445 373.533 Q164.445 366.403 162.616 362.861 Q160.811 359.297 157.177 359.297 M157.177 355.593 Q162.987 355.593 166.042 360.199 Q169.121 364.783 169.121 373.533 Q169.121 382.259 166.042 386.866 Q162.987 391.449 157.177 391.449 Q151.366 391.449 148.288 386.866 Q145.232 382.259 145.232 373.533 Q145.232 364.783 148.288 360.199 Q151.366 355.593 157.177 355.593 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M127.593 166.594 Q124.445 166.594 122.593 168.747 Q120.765 170.9 120.765 174.65 Q120.765 178.377 122.593 180.553 Q124.445 182.706 127.593 182.706 Q130.742 182.706 132.57 180.553 Q134.422 178.377 134.422 174.65 Q134.422 170.9 132.57 168.747 Q130.742 166.594 127.593 166.594 M136.876 151.942 L136.876 156.201 Q135.117 155.368 133.311 154.928 Q131.529 154.488 129.769 154.488 Q125.14 154.488 122.686 157.613 Q120.255 160.738 119.908 167.057 Q121.274 165.044 123.334 163.979 Q125.394 162.891 127.871 162.891 Q133.08 162.891 136.089 166.062 Q139.121 169.21 139.121 174.65 Q139.121 179.974 135.973 183.192 Q132.825 186.409 127.593 186.409 Q121.598 186.409 118.427 181.826 Q115.256 177.219 115.256 168.493 Q115.256 160.298 119.144 155.437 Q123.033 150.553 129.584 150.553 Q131.343 150.553 133.126 150.9 Q134.931 151.247 136.876 151.942 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M157.177 154.257 Q153.566 154.257 151.737 157.821 Q149.931 161.363 149.931 168.493 Q149.931 175.599 151.737 179.164 Q153.566 182.706 157.177 182.706 Q160.811 182.706 162.616 179.164 Q164.445 175.599 164.445 168.493 Q164.445 161.363 162.616 157.821 Q160.811 154.257 157.177 154.257 M157.177 150.553 Q162.987 150.553 166.042 155.159 Q169.121 159.743 169.121 168.493 Q169.121 177.219 166.042 181.826 Q162.987 186.409 157.177 186.409 Q151.366 186.409 148.288 181.826 Q145.232 177.219 145.232 168.493 Q145.232 159.743 148.288 155.159 Q151.366 150.553 157.177 150.553 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M16.4842 892.174 L16.4842 864.865 L21.895 864.865 L21.895 885.744 L35.8996 885.744 L35.8996 866.902 L41.3104 866.902 L41.3104 885.744 L64.0042 885.744 L64.0042 892.174 L16.4842 892.174 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M44.7161 828.007 L47.5806 828.007 L47.5806 854.934 Q53.6281 854.552 56.8109 851.306 Q59.9619 848.028 59.9619 842.203 Q59.9619 838.829 59.1344 835.678 Q58.3069 832.495 56.6518 829.376 L62.1899 829.376 Q63.5267 832.527 64.227 835.837 Q64.9272 839.147 64.9272 842.553 Q64.9272 851.083 59.9619 856.08 Q54.9967 861.045 46.5303 861.045 Q37.7774 861.045 32.6531 856.335 Q27.4968 851.592 27.4968 843.572 Q27.4968 836.378 32.1438 832.209 Q36.7589 828.007 44.7161 828.007 M42.9973 833.864 Q38.1912 833.928 35.3266 836.569 Q32.4621 839.179 32.4621 843.508 Q32.4621 848.409 35.2312 851.37 Q38.0002 854.298 43.0292 854.743 L42.9973 833.864 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M46.0847 802.194 Q46.0847 809.292 47.7079 812.029 Q49.3312 814.767 53.2461 814.767 Q56.3653 814.767 58.2114 812.73 Q60.0256 810.661 60.0256 807.128 Q60.0256 802.258 56.5881 799.33 Q53.1188 796.37 47.3897 796.37 L46.0847 796.37 L46.0847 802.194 M43.6657 790.513 L64.0042 790.513 L64.0042 796.37 L58.5933 796.37 Q61.8398 798.375 63.3994 801.367 Q64.9272 804.359 64.9272 808.687 Q64.9272 814.162 61.8716 817.409 Q58.7843 820.623 53.6281 820.623 Q47.6125 820.623 44.5569 816.613 Q41.5014 812.571 41.5014 804.582 L41.5014 796.37 L40.9285 796.37 Q36.8862 796.37 34.6901 799.043 Q32.4621 801.685 32.4621 806.491 Q32.4621 809.547 33.1941 812.443 Q33.9262 815.34 35.3903 818.013 L29.9795 818.013 Q28.7381 814.799 28.1334 811.775 Q27.4968 808.751 27.4968 805.887 Q27.4968 798.152 31.5072 794.333 Q35.5176 790.513 43.6657 790.513 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M33.8307 757.794 Q33.2578 758.78 33.0032 759.958 Q32.7167 761.104 32.7167 762.504 Q32.7167 767.47 35.9632 770.143 Q39.1779 772.785 45.2253 772.785 L64.0042 772.785 L64.0042 778.673 L28.3562 778.673 L28.3562 772.785 L33.8944 772.785 Q30.6479 770.939 29.0883 767.979 Q27.4968 765.019 27.4968 760.786 Q27.4968 760.181 27.5923 759.449 Q27.656 758.717 27.8151 757.825 L33.8307 757.794 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M49.9359 752.255 L28.3562 752.255 L28.3562 746.399 L49.7131 746.399 Q54.7739 746.399 57.3202 744.426 Q59.8346 742.452 59.8346 738.506 Q59.8346 733.763 56.8109 731.026 Q53.7872 728.257 48.5673 728.257 L28.3562 728.257 L28.3562 722.4 L64.0042 722.4 L64.0042 728.257 L58.5296 728.257 Q61.7762 730.389 63.3676 733.222 Q64.9272 736.023 64.9272 739.747 Q64.9272 745.89 61.1078 749.073 Q57.2883 752.255 49.9359 752.255 M27.4968 737.519 L27.4968 737.519 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M33.8307 689.681 Q33.2578 690.667 33.0032 691.845 Q32.7167 692.991 32.7167 694.391 Q32.7167 699.356 35.9632 702.03 Q39.1779 704.672 45.2253 704.672 L64.0042 704.672 L64.0042 710.56 L28.3562 710.56 L28.3562 704.672 L33.8944 704.672 Q30.6479 702.826 29.0883 699.866 Q27.4968 696.906 27.4968 692.672 Q27.4968 692.068 27.5923 691.336 Q27.656 690.604 27.8151 689.712 L33.8307 689.681 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M44.7161 654.478 L47.5806 654.478 L47.5806 681.405 Q53.6281 681.023 56.8109 677.777 Q59.9619 674.498 59.9619 668.674 Q59.9619 665.3 59.1344 662.149 Q58.3069 658.966 56.6518 655.847 L62.1899 655.847 Q63.5267 658.998 64.227 662.308 Q64.9272 665.618 64.9272 669.024 Q64.9272 677.554 59.9619 682.551 Q54.9967 687.516 46.5303 687.516 Q37.7774 687.516 32.6531 682.806 Q27.4968 678.063 27.4968 670.042 Q27.4968 662.849 32.1438 658.68 Q36.7589 654.478 44.7161 654.478 M42.9973 660.335 Q38.1912 660.398 35.3266 663.04 Q32.4621 665.65 32.4621 669.979 Q32.4621 674.88 35.2312 677.84 Q38.0002 680.769 43.0292 681.214 L42.9973 660.335 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M877.575 12.096 L912.332 12.096 L912.332 18.9825 L885.758 18.9825 L885.758 36.8065 L909.739 36.8065 L909.739 43.6931 L885.758 43.6931 L885.758 72.576 L877.575 72.576 L877.575 12.096 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M959.241 48.0275 L959.241 51.6733 L924.97 51.6733 Q925.457 59.3701 929.588 63.421 Q933.761 67.4314 941.174 67.4314 Q945.468 67.4314 949.478 66.3781 Q953.529 65.3249 957.499 63.2184 L957.499 70.267 Q953.489 71.9684 949.276 72.8596 Q945.063 73.7508 940.728 73.7508 Q929.872 73.7508 923.512 67.4314 Q917.193 61.1119 917.193 50.3365 Q917.193 39.1965 923.188 32.6746 Q929.224 26.1121 939.432 26.1121 Q948.587 26.1121 953.894 32.0264 Q959.241 37.9003 959.241 48.0275 M951.787 45.84 Q951.706 39.7232 948.344 36.0774 Q945.022 32.4315 939.513 32.4315 Q933.275 32.4315 929.507 35.9558 Q925.781 39.4801 925.213 45.8805 L951.787 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M992.094 49.7694 Q983.06 49.7694 979.577 51.8354 Q976.093 53.9013 976.093 58.8839 Q976.093 62.8538 978.685 65.2034 Q981.319 67.5124 985.815 67.5124 Q992.013 67.5124 995.74 63.1374 Q999.507 58.7219 999.507 51.4303 L999.507 49.7694 L992.094 49.7694 M1006.96 46.6907 L1006.96 72.576 L999.507 72.576 L999.507 65.6895 Q996.955 69.8214 993.147 71.8063 Q989.339 73.7508 983.83 73.7508 Q976.863 73.7508 972.731 69.8619 Q968.639 65.9325 968.639 59.3701 Q968.639 51.7138 973.743 47.825 Q978.888 43.9361 989.056 43.9361 L999.507 43.9361 L999.507 43.2069 Q999.507 38.0623 996.104 35.2672 Q992.742 32.4315 986.625 32.4315 Q982.736 32.4315 979.05 33.3632 Q975.364 34.295 971.961 36.1584 L971.961 29.2718 Q976.052 27.692 979.901 26.9223 Q983.749 26.1121 987.395 26.1121 Q997.239 26.1121 1002.1 31.2163 Q1006.96 36.3204 1006.96 46.6907 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1029.69 14.324 L1029.69 27.2059 L1045.04 27.2059 L1045.04 32.9987 L1029.69 32.9987 L1029.69 57.6282 Q1029.69 63.1779 1031.19 64.7578 Q1032.72 66.3376 1037.38 66.3376 L1045.04 66.3376 L1045.04 72.576 L1037.38 72.576 Q1028.75 72.576 1025.47 69.3758 Q1022.19 66.1351 1022.19 57.6282 L1022.19 32.9987 L1016.72 32.9987 L1016.72 27.2059 L1022.19 27.2059 L1022.19 14.324 L1029.69 14.324 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1054.07 54.671 L1054.07 27.2059 L1061.53 27.2059 L1061.53 54.3874 Q1061.53 60.8284 1064.04 64.0691 Q1066.55 67.2693 1071.57 67.2693 Q1077.61 67.2693 1081.09 63.421 Q1084.62 59.5726 1084.62 52.9291 L1084.62 27.2059 L1092.07 27.2059 L1092.07 72.576 L1084.62 72.576 L1084.62 65.6084 Q1081.9 69.7404 1078.3 71.7658 Q1074.73 73.7508 1069.99 73.7508 Q1062.17 73.7508 1058.12 68.8897 Q1054.07 64.0286 1054.07 54.671 M1072.83 26.1121 L1072.83 26.1121 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1133.71 34.1734 Q1132.46 33.4443 1130.96 33.1202 Q1129.5 32.7556 1127.72 32.7556 Q1121.4 32.7556 1118 36.8875 Q1114.63 40.9789 1114.63 48.6757 L1114.63 72.576 L1107.14 72.576 L1107.14 27.2059 L1114.63 27.2059 L1114.63 34.2544 Q1116.98 30.1225 1120.75 28.1376 Q1124.52 26.1121 1129.91 26.1121 Q1130.68 26.1121 1131.61 26.2337 Q1132.54 26.3147 1133.67 26.5172 L1133.71 34.1734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1178.52 48.0275 L1178.52 51.6733 L1144.25 51.6733 Q1144.73 59.3701 1148.86 63.421 Q1153.04 67.4314 1160.45 67.4314 Q1164.74 67.4314 1168.75 66.3781 Q1172.8 65.3249 1176.77 63.2184 L1176.77 70.267 Q1172.76 71.9684 1168.55 72.8596 Q1164.34 73.7508 1160 73.7508 Q1149.15 73.7508 1142.79 67.4314 Q1136.47 61.1119 1136.47 50.3365 Q1136.47 39.1965 1142.46 32.6746 Q1148.5 26.1121 1158.71 26.1121 Q1167.86 26.1121 1173.17 32.0264 Q1178.52 37.9003 1178.52 48.0275 M1171.06 45.84 Q1170.98 39.7232 1167.62 36.0774 Q1164.3 32.4315 1158.79 32.4315 Q1152.55 32.4315 1148.78 35.9558 Q1145.06 39.4801 1144.49 45.8805 L1171.06 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1217.45 12.096 L1225.63 12.096 L1225.63 72.576 L1217.45 72.576 L1217.45 12.096 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1276.91 35.9153 Q1279.71 30.8922 1283.6 28.5022 Q1287.49 26.1121 1292.75 26.1121 Q1299.84 26.1121 1303.69 31.0947 Q1307.54 36.0368 1307.54 45.1919 L1307.54 72.576 L1300.04 72.576 L1300.04 45.4349 Q1300.04 38.913 1297.73 35.7533 Q1295.43 32.5936 1290.69 32.5936 Q1284.89 32.5936 1281.53 36.4419 Q1278.17 40.2903 1278.17 46.9338 L1278.17 72.576 L1270.67 72.576 L1270.67 45.4349 Q1270.67 38.8725 1268.37 35.7533 Q1266.06 32.5936 1261.24 32.5936 Q1255.52 32.5936 1252.16 36.4824 Q1248.8 40.3308 1248.8 46.9338 L1248.8 72.576 L1241.31 72.576 L1241.31 27.2059 L1248.8 27.2059 L1248.8 34.2544 Q1251.35 30.082 1254.92 28.0971 Q1258.48 26.1121 1263.38 26.1121 Q1268.33 26.1121 1271.77 28.6237 Q1275.25 31.1352 1276.91 35.9153 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1329.62 65.7705 L1329.62 89.8329 L1322.12 89.8329 L1322.12 27.2059 L1329.62 27.2059 L1329.62 34.0924 Q1331.96 30.0415 1335.53 28.0971 Q1339.13 26.1121 1344.12 26.1121 Q1352.38 26.1121 1357.53 32.6746 Q1362.71 39.2371 1362.71 49.9314 Q1362.71 60.6258 1357.53 67.1883 Q1352.38 73.7508 1344.12 73.7508 Q1339.13 73.7508 1335.53 71.8063 Q1331.96 69.8214 1329.62 65.7705 M1354.97 49.9314 Q1354.97 41.7081 1351.57 37.0496 Q1348.21 32.3505 1342.29 32.3505 Q1336.38 32.3505 1332.98 37.0496 Q1329.62 41.7081 1329.62 49.9314 Q1329.62 58.1548 1332.98 62.8538 Q1336.38 67.5124 1342.29 67.5124 Q1348.21 67.5124 1351.57 62.8538 Q1354.97 58.1548 1354.97 49.9314 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1392.65 32.4315 Q1386.65 32.4315 1383.17 37.1306 Q1379.68 41.7891 1379.68 49.9314 Q1379.68 58.0738 1383.13 62.7728 Q1386.61 67.4314 1392.65 67.4314 Q1398.6 67.4314 1402.09 62.7323 Q1405.57 58.0333 1405.57 49.9314 Q1405.57 41.8701 1402.09 37.1711 Q1398.6 32.4315 1392.65 32.4315 M1392.65 26.1121 Q1402.37 26.1121 1407.92 32.4315 Q1413.47 38.7509 1413.47 49.9314 Q1413.47 61.0714 1407.92 67.4314 Q1402.37 73.7508 1392.65 73.7508 Q1382.88 73.7508 1377.33 67.4314 Q1371.83 61.0714 1371.83 49.9314 Q1371.83 38.7509 1377.33 32.4315 Q1382.88 26.1121 1392.65 26.1121 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1452.11 34.1734 Q1450.86 33.4443 1449.36 33.1202 Q1447.9 32.7556 1446.12 32.7556 Q1439.8 32.7556 1436.4 36.8875 Q1433.03 40.9789 1433.03 48.6757 L1433.03 72.576 L1425.54 72.576 L1425.54 27.2059 L1433.03 27.2059 L1433.03 34.2544 Q1435.38 30.1225 1439.15 28.1376 Q1442.92 26.1121 1448.31 26.1121 Q1449.08 26.1121 1450.01 26.2337 Q1450.94 26.3147 1452.07 26.5172 L1452.11 34.1734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1467.31 14.324 L1467.31 27.2059 L1482.66 27.2059 L1482.66 32.9987 L1467.31 32.9987 L1467.31 57.6282 Q1467.31 63.1779 1468.8 64.7578 Q1470.34 66.3376 1475 66.3376 L1482.66 66.3376 L1482.66 72.576 L1475 72.576 Q1466.37 72.576 1463.09 69.3758 Q1459.81 66.1351 1459.81 57.6282 L1459.81 32.9987 L1454.34 32.9987 L1454.34 27.2059 L1459.81 27.2059 L1459.81 14.324 L1467.31 14.324 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1513.08 49.7694 Q1504.05 49.7694 1500.56 51.8354 Q1497.08 53.9013 1497.08 58.8839 Q1497.08 62.8538 1499.67 65.2034 Q1502.31 67.5124 1506.8 67.5124 Q1513 67.5124 1516.73 63.1374 Q1520.49 58.7219 1520.49 51.4303 L1520.49 49.7694 L1513.08 49.7694 M1527.95 46.6907 L1527.95 72.576 L1520.49 72.576 L1520.49 65.6895 Q1517.94 69.8214 1514.13 71.8063 Q1510.33 73.7508 1504.82 73.7508 Q1497.85 73.7508 1493.72 69.8619 Q1489.63 65.9325 1489.63 59.3701 Q1489.63 51.7138 1494.73 47.825 Q1499.87 43.9361 1510.04 43.9361 L1520.49 43.9361 L1520.49 43.2069 Q1520.49 38.0623 1517.09 35.2672 Q1513.73 32.4315 1507.61 32.4315 Q1503.72 32.4315 1500.04 33.3632 Q1496.35 34.295 1492.95 36.1584 L1492.95 29.2718 Q1497.04 27.692 1500.89 26.9223 Q1504.74 26.1121 1508.38 26.1121 Q1518.23 26.1121 1523.09 31.2163 Q1527.95 36.3204 1527.95 46.6907 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1581.01 45.1919 L1581.01 72.576 L1573.56 72.576 L1573.56 45.4349 Q1573.56 38.994 1571.05 35.7938 Q1568.54 32.5936 1563.51 32.5936 Q1557.48 32.5936 1553.99 36.4419 Q1550.51 40.2903 1550.51 46.9338 L1550.51 72.576 L1543.02 72.576 L1543.02 27.2059 L1550.51 27.2059 L1550.51 34.2544 Q1553.18 30.163 1556.79 28.1376 Q1560.44 26.1121 1565.18 26.1121 Q1572.99 26.1121 1577 30.9732 Q1581.01 35.7938 1581.01 45.1919 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1628.53 28.9478 L1628.53 35.9153 Q1625.37 34.1734 1622.17 33.3227 Q1619.01 32.4315 1615.77 32.4315 Q1608.52 32.4315 1604.51 37.0496 Q1600.5 41.6271 1600.5 49.9314 Q1600.5 58.2358 1604.51 62.8538 Q1608.52 67.4314 1615.77 67.4314 Q1619.01 67.4314 1622.17 66.5807 Q1625.37 65.6895 1628.53 63.9476 L1628.53 70.8341 Q1625.41 72.2924 1622.05 73.0216 Q1618.73 73.7508 1614.96 73.7508 Q1604.71 73.7508 1598.68 67.3098 Q1592.64 60.8689 1592.64 49.9314 Q1592.64 38.832 1598.72 32.472 Q1604.83 26.1121 1615.45 26.1121 Q1618.89 26.1121 1622.17 26.8413 Q1625.45 27.5299 1628.53 28.9478 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1680.3 48.0275 L1680.3 51.6733 L1646.03 51.6733 Q1646.52 59.3701 1650.65 63.421 Q1654.82 67.4314 1662.23 67.4314 Q1666.53 67.4314 1670.54 66.3781 Q1674.59 65.3249 1678.56 63.2184 L1678.56 70.267 Q1674.55 71.9684 1670.34 72.8596 Q1666.12 73.7508 1661.79 73.7508 Q1650.93 73.7508 1644.57 67.4314 Q1638.25 61.1119 1638.25 50.3365 Q1638.25 39.1965 1644.25 32.6746 Q1650.28 26.1121 1660.49 26.1121 Q1669.65 26.1121 1674.95 32.0264 Q1680.3 37.9003 1680.3 48.0275 M1672.85 45.84 Q1672.77 39.7232 1669.41 36.0774 Q1666.08 32.4315 1660.57 32.4315 Q1654.34 32.4315 1650.57 35.9558 Q1646.84 39.4801 1646.27 45.8805 L1672.85 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip582)\" d=\"M266.01 1386.4 L265.903 1386.4 L265.903 1369.99 L266.01 1369.99 L266.01 1386.4 L266.01 1386.4  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.01,1386.4 265.903,1386.4 265.903,1369.99 266.01,1369.99 266.01,1386.4 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.24 1365.89 L265.903 1365.89 L265.903 1349.49 L266.24 1349.49 L266.24 1365.89 L266.24 1365.89  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.24,1365.89 265.903,1365.89 265.903,1349.49 266.24,1349.49 266.24,1365.89 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.644 1345.39 L265.903 1345.39 L265.903 1328.98 L266.644 1328.98 L266.644 1345.39 L266.644 1345.39  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.644,1345.39 265.903,1345.39 265.903,1328.98 266.644,1328.98 266.644,1345.39 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M265.903 1324.88 L265.903 1324.88 L265.903 1308.48 L265.903 1308.48 L265.903 1324.88 L265.903 1324.88  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.903,1324.88 265.903,1324.88 265.903,1308.48 265.903,1324.88 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.2 1304.38 L265.903 1304.38 L265.903 1287.98 L266.2 1287.98 L266.2 1304.38 L266.2 1304.38  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.2,1304.38 265.903,1304.38 265.903,1287.98 266.2,1287.98 266.2,1304.38 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.219 1283.88 L265.903 1283.88 L265.903 1267.47 L266.219 1267.47 L266.219 1283.88 L266.219 1283.88  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.219,1283.88 265.903,1283.88 265.903,1267.47 266.219,1267.47 266.219,1283.88 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.371 1263.37 L265.903 1263.37 L265.903 1246.97 L266.371 1246.97 L266.371 1263.37 L266.371 1263.37  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.371,1263.37 265.903,1263.37 265.903,1246.97 266.371,1246.97 266.371,1263.37 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.161 1242.87 L265.903 1242.87 L265.903 1226.46 L266.161 1226.46 L266.161 1242.87 L266.161 1242.87  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.161,1242.87 265.903,1242.87 265.903,1226.46 266.161,1226.46 266.161,1242.87 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.361 1222.36 L265.903 1222.36 L265.903 1205.96 L266.361 1205.96 L266.361 1222.36 L266.361 1222.36  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.361,1222.36 265.903,1222.36 265.903,1205.96 266.361,1205.96 266.361,1222.36 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.993 1201.86 L265.903 1201.86 L265.903 1185.46 L266.993 1185.46 L266.993 1201.86 L266.993 1201.86  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.993,1201.86 265.903,1201.86 265.903,1185.46 266.993,1185.46 266.993,1201.86 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M268.737 1181.36 L265.903 1181.36 L265.903 1164.95 L268.737 1164.95 L268.737 1181.36 L268.737 1181.36  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"268.737,1181.36 265.903,1181.36 265.903,1164.95 268.737,1164.95 268.737,1181.36 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.957 1160.85 L265.903 1160.85 L265.903 1144.45 L266.957 1144.45 L266.957 1160.85 L266.957 1160.85  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.957,1160.85 265.903,1160.85 265.903,1144.45 266.957,1144.45 266.957,1160.85 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.68 1140.35 L265.903 1140.35 L265.903 1123.94 L266.68 1123.94 L266.68 1140.35 L266.68 1140.35  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.68,1140.35 265.903,1140.35 265.903,1123.94 266.68,1123.94 266.68,1140.35 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.252 1119.84 L265.903 1119.84 L265.903 1103.44 L266.252 1103.44 L266.252 1119.84 L266.252 1119.84  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.252,1119.84 265.903,1119.84 265.903,1103.44 266.252,1103.44 266.252,1119.84 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.628 1099.34 L265.903 1099.34 L265.903 1082.94 L266.628 1082.94 L266.628 1099.34 L266.628 1099.34  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.628,1099.34 265.903,1099.34 265.903,1082.94 266.628,1082.94 266.628,1099.34 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.153 1078.84 L265.903 1078.84 L265.903 1062.43 L266.153 1062.43 L266.153 1078.84 L266.153 1078.84  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.153,1078.84 265.903,1078.84 265.903,1062.43 266.153,1062.43 266.153,1078.84 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.513 1058.33 L265.903 1058.33 L265.903 1041.93 L266.513 1041.93 L266.513 1058.33 L266.513 1058.33  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.513,1058.33 265.903,1058.33 265.903,1041.93 266.513,1041.93 266.513,1058.33 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.354 1037.83 L265.903 1037.83 L265.903 1021.42 L266.354 1021.42 L266.354 1037.83 L266.354 1037.83  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.354,1037.83 265.903,1037.83 265.903,1021.42 266.354,1021.42 266.354,1037.83 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M265.903 1017.32 L265.903 1017.32 L265.903 1000.92 L265.903 1000.92 L265.903 1017.32 L265.903 1017.32  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.903,1017.32 265.903,1017.32 265.903,1000.92 265.903,1017.32 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.215 996.82 L265.903 996.82 L265.903 980.416 L266.215 980.416 L266.215 996.82 L266.215 996.82  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.215,996.82 265.903,996.82 265.903,980.416 266.215,980.416 266.215,996.82 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.346 976.316 L265.903 976.316 L265.903 959.912 L266.346 959.912 L266.346 976.316 L266.346 976.316  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.346,976.316 265.903,976.316 265.903,959.912 266.346,959.912 266.346,976.316 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.053 955.812 L265.903 955.812 L265.903 939.408 L266.053 939.408 L266.053 955.812 L266.053 955.812  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.053,955.812 265.903,955.812 265.903,939.408 266.053,939.408 266.053,955.812 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.4 935.308 L265.903 935.308 L265.903 918.904 L266.4 918.904 L266.4 935.308 L266.4 935.308  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.4,935.308 265.903,935.308 265.903,918.904 266.4,918.904 266.4,935.308 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.113 914.804 L265.903 914.804 L265.903 898.4 L266.113 898.4 L266.113 914.804 L266.113 914.804  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.113,914.804 265.903,914.804 265.903,898.4 266.113,898.4 266.113,914.804 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M265.923 894.3 L265.903 894.3 L265.903 877.896 L265.923 877.896 L265.923 894.3 L265.923 894.3  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.923,894.3 265.903,894.3 265.903,877.896 265.923,877.896 265.923,894.3 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M265.903 873.796 L265.903 873.796 L265.903 857.392 L265.903 857.392 L265.903 873.796 L265.903 873.796  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.903,873.796 265.903,873.796 265.903,857.392 265.903,873.796 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M265.903 853.292 L265.903 853.292 L265.903 836.888 L265.903 836.888 L265.903 853.292 L265.903 853.292  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.903,853.292 265.903,853.292 265.903,836.888 265.903,853.292 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.644 832.788 L265.903 832.788 L265.903 816.384 L266.644 816.384 L266.644 832.788 L266.644 832.788  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.644,832.788 265.903,832.788 265.903,816.384 266.644,816.384 266.644,832.788 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.551 812.284 L265.903 812.284 L265.903 795.88 L266.551 795.88 L266.551 812.284 L266.551 812.284  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.551,812.284 265.903,812.284 265.903,795.88 266.551,795.88 266.551,812.284 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.003 791.78 L265.903 791.78 L265.903 775.376 L266.003 775.376 L266.003 791.78 L266.003 791.78  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.003,791.78 265.903,791.78 265.903,775.376 266.003,775.376 266.003,791.78 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.874 771.276 L265.903 771.276 L265.903 754.872 L266.874 754.872 L266.874 771.276 L266.874 771.276  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.874,771.276 265.903,771.276 265.903,754.872 266.874,754.872 266.874,771.276 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.271 750.772 L265.903 750.772 L265.903 734.368 L266.271 734.368 L266.271 750.772 L266.271 750.772  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.271,750.772 265.903,750.772 265.903,734.368 266.271,734.368 266.271,750.772 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.27 730.268 L265.903 730.268 L265.903 713.864 L266.27 713.864 L266.27 730.268 L266.27 730.268  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.27,730.268 265.903,730.268 265.903,713.864 266.27,713.864 266.27,730.268 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.829 709.764 L265.903 709.764 L265.903 693.36 L266.829 693.36 L266.829 709.764 L266.829 709.764  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.829,709.764 265.903,709.764 265.903,693.36 266.829,693.36 266.829,709.764 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.103 689.26 L265.903 689.26 L265.903 672.856 L266.103 672.856 L266.103 689.26 L266.103 689.26  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.103,689.26 265.903,689.26 265.903,672.856 266.103,672.856 266.103,689.26 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M267.405 668.756 L265.903 668.756 L265.903 652.352 L267.405 652.352 L267.405 668.756 L267.405 668.756  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"267.405,668.756 265.903,668.756 265.903,652.352 267.405,652.352 267.405,668.756 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.074 648.252 L265.903 648.252 L265.903 631.848 L266.074 631.848 L266.074 648.252 L266.074 648.252  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.074,648.252 265.903,648.252 265.903,631.848 266.074,631.848 266.074,648.252 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.824 627.748 L265.903 627.748 L265.903 611.344 L266.824 611.344 L266.824 627.748 L266.824 627.748  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.824,627.748 265.903,627.748 265.903,611.344 266.824,611.344 266.824,627.748 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.364 607.244 L265.903 607.244 L265.903 590.84 L266.364 590.84 L266.364 607.244 L266.364 607.244  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.364,607.244 265.903,607.244 265.903,590.84 266.364,590.84 266.364,607.244 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.557 586.74 L265.903 586.74 L265.903 570.336 L266.557 570.336 L266.557 586.74 L266.557 586.74  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.557,586.74 265.903,586.74 265.903,570.336 266.557,570.336 266.557,586.74 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M265.903 566.236 L265.903 566.236 L265.903 549.832 L265.903 549.832 L265.903 566.236 L265.903 566.236  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.903,566.236 265.903,566.236 265.903,549.832 265.903,566.236 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M265.903 545.732 L265.903 545.732 L265.903 529.328 L265.903 529.328 L265.903 545.732 L265.903 545.732  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.903,545.732 265.903,545.732 265.903,529.328 265.903,545.732 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.284 525.228 L265.903 525.228 L265.903 508.824 L266.284 508.824 L266.284 525.228 L266.284 525.228  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.284,525.228 265.903,525.228 265.903,508.824 266.284,508.824 266.284,525.228 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M267.018 504.724 L265.903 504.724 L265.903 488.32 L267.018 488.32 L267.018 504.724 L267.018 504.724  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"267.018,504.724 265.903,504.724 265.903,488.32 267.018,488.32 267.018,504.724 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M268.055 484.22 L265.903 484.22 L265.903 467.816 L268.055 467.816 L268.055 484.22 L268.055 484.22  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"268.055,484.22 265.903,484.22 265.903,467.816 268.055,467.816 268.055,484.22 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.09 463.716 L265.903 463.716 L265.903 447.312 L266.09 447.312 L266.09 463.716 L266.09 463.716  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.09,463.716 265.903,463.716 265.903,447.312 266.09,447.312 266.09,463.716 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.724 443.212 L265.903 443.212 L265.903 426.808 L266.724 426.808 L266.724 443.212 L266.724 443.212  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.724,443.212 265.903,443.212 265.903,426.808 266.724,426.808 266.724,443.212 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M267.114 422.708 L265.903 422.708 L265.903 406.304 L267.114 406.304 L267.114 422.708 L267.114 422.708  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"267.114,422.708 265.903,422.708 265.903,406.304 267.114,406.304 267.114,422.708 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.581 402.204 L265.903 402.204 L265.903 385.8 L266.581 385.8 L266.581 402.204 L266.581 402.204  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.581,402.204 265.903,402.204 265.903,385.8 266.581,385.8 266.581,402.204 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M267.159 381.7 L265.903 381.7 L265.903 365.296 L267.159 365.296 L267.159 381.7 L267.159 381.7  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"267.159,381.7 265.903,381.7 265.903,365.296 267.159,365.296 267.159,381.7 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.011 361.196 L265.903 361.196 L265.903 344.792 L266.011 344.792 L266.011 361.196 L266.011 361.196  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.011,361.196 265.903,361.196 265.903,344.792 266.011,344.792 266.011,361.196 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.552 340.692 L265.903 340.692 L265.903 324.288 L266.552 324.288 L266.552 340.692 L266.552 340.692  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.552,340.692 265.903,340.692 265.903,324.288 266.552,324.288 266.552,340.692 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.507 320.188 L265.903 320.188 L265.903 303.784 L266.507 303.784 L266.507 320.188 L266.507 320.188  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.507,320.188 265.903,320.188 265.903,303.784 266.507,303.784 266.507,320.188 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.97 299.684 L265.903 299.684 L265.903 283.28 L266.97 283.28 L266.97 299.684 L266.97 299.684  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.97,299.684 265.903,299.684 265.903,283.28 266.97,283.28 266.97,299.684 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.633 279.18 L265.903 279.18 L265.903 262.776 L266.633 262.776 L266.633 279.18 L266.633 279.18  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.633,279.18 265.903,279.18 265.903,262.776 266.633,262.776 266.633,279.18 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.101 258.676 L265.903 258.676 L265.903 242.272 L266.101 242.272 L266.101 258.676 L266.101 258.676  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.101,258.676 265.903,258.676 265.903,242.272 266.101,242.272 266.101,258.676 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.184 238.172 L265.903 238.172 L265.903 221.768 L266.184 221.768 L266.184 238.172 L266.184 238.172  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.184,238.172 265.903,238.172 265.903,221.768 266.184,221.768 266.184,238.172 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M266.012 217.668 L265.903 217.668 L265.903 201.264 L266.012 201.264 L266.012 217.668 L266.012 217.668  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"266.012,217.668 265.903,217.668 265.903,201.264 266.012,201.264 266.012,217.668 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M267.266 197.164 L265.903 197.164 L265.903 180.76 L267.266 180.76 L267.266 197.164 L267.266 197.164  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"267.266,197.164 265.903,197.164 265.903,180.76 267.266,180.76 267.266,197.164 \"/>\n",
       "<path clip-path=\"url(#clip582)\" d=\"M265.983 176.66 L265.903 176.66 L265.903 160.256 L265.983 160.256 L265.983 176.66 L265.983 176.66  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.983,176.66 265.903,176.66 265.903,160.256 265.983,160.256 265.983,176.66 \"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"299.671\" cy=\"1398.63\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"333.439\" cy=\"1398.49\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"367.207\" cy=\"1398.25\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"400.974\" cy=\"1398.7\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"434.742\" cy=\"1398.52\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"468.51\" cy=\"1398.51\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"502.278\" cy=\"1398.41\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"536.046\" cy=\"1398.54\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"569.814\" cy=\"1398.42\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"603.582\" cy=\"1398.04\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"637.349\" cy=\"1396.98\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"671.117\" cy=\"1398.06\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"704.885\" cy=\"1398.23\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"738.653\" cy=\"1398.49\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"772.421\" cy=\"1398.26\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"806.189\" cy=\"1398.55\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"839.956\" cy=\"1398.33\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"873.724\" cy=\"1398.42\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"907.492\" cy=\"1398.7\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"941.26\" cy=\"1398.51\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"975.028\" cy=\"1398.43\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1008.8\" cy=\"1398.61\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1042.56\" cy=\"1398.4\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1076.33\" cy=\"1398.57\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1110.1\" cy=\"1398.69\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1143.87\" cy=\"1398.7\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1177.63\" cy=\"1398.7\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1211.4\" cy=\"1398.25\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1245.17\" cy=\"1398.3\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1278.94\" cy=\"1398.64\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1312.71\" cy=\"1398.11\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1346.47\" cy=\"1398.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1380.24\" cy=\"1398.48\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1414.01\" cy=\"1398.14\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1447.78\" cy=\"1398.58\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1481.55\" cy=\"1397.79\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1515.31\" cy=\"1398.59\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1549.08\" cy=\"1398.14\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1582.85\" cy=\"1398.42\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1616.62\" cy=\"1398.3\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1650.38\" cy=\"1398.7\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1684.15\" cy=\"1398.7\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1717.92\" cy=\"1398.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1751.69\" cy=\"1398.02\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1785.46\" cy=\"1397.39\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1819.22\" cy=\"1398.58\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1852.99\" cy=\"1398.2\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1886.76\" cy=\"1397.96\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1920.53\" cy=\"1398.29\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1954.3\" cy=\"1397.94\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1988.06\" cy=\"1398.63\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2021.83\" cy=\"1398.3\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2055.6\" cy=\"1398.33\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2089.37\" cy=\"1398.05\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2123.13\" cy=\"1398.25\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2156.9\" cy=\"1398.58\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2190.67\" cy=\"1398.53\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2224.44\" cy=\"1398.63\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2258.21\" cy=\"1397.87\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip582)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2291.97\" cy=\"1398.65\" r=\"2\"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = bar(y=1:60,models[\"RF\"].feature_importances_, orientation=:horizontal, legend = false)\n",
    "xlabel!(p,\"Gini Gain\")\n",
    "ylabel!(p,\"Fearure\")\n",
    "title!(\"Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4feb1ec",
   "metadata": {},
   "source": [
    "It should be pointed out that, as can be seen in the graph, this value determines that most of the information is concentrated in some of the frequencies used. This is why a filtering of the information such as the ones we will see in the next session could be carried out based on this value. \n",
    "\n",
    "### XGBoost (eXtreme Gradient Boosting)\n",
    "\n",
    "Finally, in this last section, Gradient Boosting should be mentioned again, specifically an implementation that in recent years has become very famous for its versatility and speed. This implementation is known as ***XGBoost (eXtreme Gradient Boosting)*** , which has stood out especially in competitions such as the Kaggle platform for its speed in obtaining results and robustness. \n",
    "\n",
    "The ***XGBoost*** will be a similar ensemble to Random Forest but uses a different base classifier known as CART (classification and regression trees) instead of *Decision Trees*. This change comes from the need for the algorithm to obtain the probability of the decisions, as was the case with *Gradient Tree Boosting*. The other fundamental change in this algotimo, since it is based on *Gradient Tree Boosting*, is the change from *bagging* to *boosting* strategy for the creation of the classifier training sets.\n",
    "\n",
    "Subsequently, this technique performs an additive training approach whose weights are adjusted based on a **Declining Gradient** on a *loss* function to be defined. By adding the *loss* function with the regularisation term, the second derivative of the functions can be calculated in order to update the classification weights of the different trees. The calculation of this gradient thus allows the adjustment of the values of the classifiers that are generated following a given one in order to allow the weights to focus attention on the patterns that are incorrectly classified. The mathematical details of the implementation can be found in this [link](https://xgboost.readthedocs.io/en/stable/tutorials/model.html).\n",
    "\n",
    "Unlike the other approaches we have seen, the `xgboost` is not currently implemented in `scikit learn`. For this reason, the reference version must be installed if it is not already present on the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a03972c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg;\n",
    "Pkg.add(\"XGBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499e0c0f",
   "metadata": {},
   "source": [
    "After that installation, the library could be used as shown in the following example. Unlike other implementations, the Julia implementation supports Julia Array, SparseMatrixCSC, libSVM format text and XGBoost binary file as input.  Althouugh the vastly options given by Julia libraría in deep to change internaly to the format [LIBSVM](https://xgboost.readthedocs.io/en/stable/tutorials/input_format.html) as any other library. This library has not all the posibilities and, more especificl, the BitVector is not supported nowadays in their function `DMatrix`. So, an small change in the format is required in order to use the library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7fd5e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using XGBoost;\n",
    "\n",
    "train_input = input_data\n",
    "train_output = output_data\n",
    "\n",
    "test_input = input_data\n",
    "test_output = output_data\n",
    "\n",
    "train_output_asNumber= Vector{Number}(train_output);\n",
    "\n",
    "@assert train_output_asNumber isa Vector{Number}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad57fb4",
   "metadata": {},
   "source": [
    "Once this data adaptation is done, you can proceed with the training of a model from the `xgboost` library. To do so, it is only necessary to call the function train with the corresponding parameters. Among these parameters, the most important are:\n",
    "\n",
    "- **eta**, term that will determine the compression of the weights after each new stage of *boosting*. It takes values between 0 and 1.\n",
    "- **max_depth**, maximum depth of the trees has by default a value of 6, increasing it will allow more complex models.\n",
    "- **gamma**, parameter that controls the minimum loss reduction necessary to perform a new partition on a leaf node of the tree. The higher it is, the more conservative it will be\n",
    "- **alpha** and **lambda**, are the parameters that control the L1 and L2 regulation respectively.\n",
    "- **objective**, sets the loss function to be used which can be one of the predefined ones, which can be consulted in this [link](https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster)\n",
    "\n",
    "Further it is only necessary to set the maximum number of iterations of the boosting process as shown in the following example with 20 rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eec905d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mXGBoost: starting training.\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39m[10:41:54] WARNING: /workspace/srcdir/xgboost/src/learner.cc:742: \n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mParameters: { \"rounds\" } are not used.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ XGBoost ~/.julia/packages/XGBoost/pHRDD/src/XGBoost.jl:35\u001b[39m\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[1]\ttrain-rmse:0.06685667952816027\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[2]\ttrain-rmse:0.03784067051373981\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[3]\ttrain-rmse:0.02057971291063566\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[4]\ttrain-rmse:0.01269548720252575\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[5]\ttrain-rmse:0.00709807877639992\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[6]\ttrain-rmse:0.00428344147805759\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[7]\ttrain-rmse:0.00305694459598080\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[8]\ttrain-rmse:0.00142192344718740\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[9]\ttrain-rmse:0.00098684141249462\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[10]\ttrain-rmse:0.00068744187963404\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining rounds complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Booster()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_data = DMatrix(train_input, label=train_output_asNumber)\n",
    "\n",
    "model = xgboost(svm_data, rounds=20, eta = 1, max_depth = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c275ed94",
   "metadata": {},
   "source": [
    "On the folllowing piece of code, several parameter as pass as a dictionary and two differnt metrics are calculated. First, error refers to the incorrect classified ones over the total amount and the second one is the Area Under Curbe ROC (AUC).\n",
    "\n",
    "### Question\n",
    "Which is the canonical name of the first measure that is been monitorized?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f5411",
   "metadata": {},
   "source": [
    "`Answer here` That measureis commonly known as classification error rate. It can be calculated as the number of incorrect classified predictions divided by the total of predictions and also, it can be calculated following this expresion: Error rate = 1 - Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40ab769b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mXGBoost: starting training.\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39m[10:41:54] WARNING: /workspace/srcdir/xgboost/src/learner.cc:742: \n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mParameters: { \"metrics\", \"param\", \"rounds\" } are not used.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ XGBoost ~/.julia/packages/XGBoost/pHRDD/src/XGBoost.jl:35\u001b[39m\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[1]\ttrain-rmse:0.36104272805218679\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[2]\ttrain-rmse:0.26349314167054072\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[3]\ttrain-rmse:0.19403141904608778\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[4]\ttrain-rmse:0.14254849879038894\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[5]\ttrain-rmse:0.10912439741860382\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[6]\ttrain-rmse:0.08405248067916692\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[7]\ttrain-rmse:0.06422803732657284\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[8]\ttrain-rmse:0.05041922616002128\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[9]\ttrain-rmse:0.04010784619540812\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[10]\ttrain-rmse:0.03205229098624832\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining rounds complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "208-element Vector{Float32}:\n",
       " 0.01804065\n",
       " 0.031605005\n",
       " 0.028208893\n",
       " 0.01256614\n",
       " 0.08909455\n",
       " 0.016385488\n",
       " 0.048917405\n",
       " 0.073377535\n",
       " 0.03411845\n",
       " 0.014371509\n",
       " 0.010147011\n",
       " 0.02828527\n",
       " 0.013967805\n",
       " ⋮\n",
       " 0.9912259\n",
       " 0.9912259\n",
       " 0.9912259\n",
       " 0.9773253\n",
       " 0.9912259\n",
       " 0.9912259\n",
       " 1.0010829\n",
       " 0.9912259\n",
       " 0.9906951\n",
       " 0.9912259\n",
       " 0.9912259\n",
       " 0.98940575"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using XGBoost: predict\n",
    "param = [\"max_depth\" => 2,\n",
    "         \"eta\" => 1,\n",
    "         \"objective\" => \"binary:logistic\"]\n",
    "metrics = metrics = [\"error\", \"auc\"]\n",
    "model = xgboost(DMatrix(train_input, label=train_output_asNumber), rounds=20, param=param, metrics=metrics)\n",
    "\n",
    "pred = predict(model, train_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6120deba",
   "metadata": {},
   "source": [
    "***Important***.\n",
    "\n",
    "In case a validation set is used, this must be passed in the *evals* parameter of the training function. In addition, and only when the mentioned *evals* parameter is defined, you can set the rounds for the pre-stop with the *early_stopping_rounds* parameter of the training function. The code would be similar to:\n",
    "``` julia\n",
    "    evals = DMatrix(val_input, label=val_output)\n",
    "    xgb_model = xgb.train(param, train_input, num_round,label = train_output_asNumber, evals=evals,\n",
    "                    early_stopping_rounds=10)\n",
    "```\n",
    "\n",
    "The value provided in the output corresponds to the sum of the outputs of the trees, being between 0 and 1 for membership of a given class. Since this is a binary class, simply set a limit of 0.5 to the output to determine what the answer is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a468354c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error of XGboost= 0.0\n"
     ]
    }
   ],
   "source": [
    "using XGBoost: predict as predict_xgb\n",
    "\n",
    "pred = predict_xgb(model, test_input)\n",
    "print(\"Error of XGboost= \", sum((pred .> 0.5) .!= test_output) / float(size(pred)[1]), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569d26f2",
   "metadata": {},
   "source": [
    "Finally, as in the case of the Random Forest it is possible to identify the importance and paint it for each of the variables in the ranking. With the following code it is possible to see such a marker ordered in a ascendent way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74e86f1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip620\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip620)\" d=\"M0 1600 L2400 1600 L2400 0 L0 0  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip621\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip620)\" d=\"M205.121 1423.18 L2352.76 1423.18 L2352.76 123.472 L205.121 123.472  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip622\">\n",
       "    <rect x=\"205\" y=\"123\" width=\"2149\" height=\"1301\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"265.903,1423.18 265.903,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"603.582,1423.18 603.582,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"941.26,1423.18 941.26,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1278.94,1423.18 1278.94,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1616.62,1423.18 1616.62,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1954.3,1423.18 1954.3,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2291.97,1423.18 2291.97,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 2352.76,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.903,1423.18 265.903,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"603.582,1423.18 603.582,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"941.26,1423.18 941.26,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1278.94,1423.18 1278.94,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1616.62,1423.18 1616.62,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1954.3,1423.18 1954.3,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2291.97,1423.18 2291.97,1404.28 \"/>\n",
       "<path clip-path=\"url(#clip620)\" d=\"M265.903 1454.1 Q262.292 1454.1 260.463 1457.66 Q258.658 1461.2 258.658 1468.33 Q258.658 1475.44 260.463 1479.01 Q262.292 1482.55 265.903 1482.55 Q269.537 1482.55 271.343 1479.01 Q273.172 1475.44 273.172 1468.33 Q273.172 1461.2 271.343 1457.66 Q269.537 1454.1 265.903 1454.1 M265.903 1450.39 Q271.713 1450.39 274.769 1455 Q277.847 1459.58 277.847 1468.33 Q277.847 1477.06 274.769 1481.67 Q271.713 1486.25 265.903 1486.25 Q260.093 1486.25 257.014 1481.67 Q253.959 1477.06 253.959 1468.33 Q253.959 1459.58 257.014 1455 Q260.093 1450.39 265.903 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M578.269 1481.64 L585.908 1481.64 L585.908 1455.28 L577.598 1456.95 L577.598 1452.69 L585.862 1451.02 L590.538 1451.02 L590.538 1481.64 L598.176 1481.64 L598.176 1485.58 L578.269 1485.58 L578.269 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M617.621 1454.1 Q614.01 1454.1 612.181 1457.66 Q610.375 1461.2 610.375 1468.33 Q610.375 1475.44 612.181 1479.01 Q614.01 1482.55 617.621 1482.55 Q621.255 1482.55 623.061 1479.01 Q624.889 1475.44 624.889 1468.33 Q624.889 1461.2 623.061 1457.66 Q621.255 1454.1 617.621 1454.1 M617.621 1450.39 Q623.431 1450.39 626.487 1455 Q629.565 1459.58 629.565 1468.33 Q629.565 1477.06 626.487 1481.67 Q623.431 1486.25 617.621 1486.25 Q611.811 1486.25 608.732 1481.67 Q605.676 1477.06 605.676 1468.33 Q605.676 1459.58 608.732 1455 Q611.811 1450.39 617.621 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M920.033 1481.64 L936.353 1481.64 L936.353 1485.58 L914.408 1485.58 L914.408 1481.64 Q917.07 1478.89 921.654 1474.26 Q926.26 1469.61 927.441 1468.27 Q929.686 1465.74 930.566 1464.01 Q931.468 1462.25 931.468 1460.56 Q931.468 1457.8 929.524 1456.07 Q927.603 1454.33 924.501 1454.33 Q922.302 1454.33 919.848 1455.09 Q917.418 1455.86 914.64 1457.41 L914.64 1452.69 Q917.464 1451.55 919.918 1450.97 Q922.371 1450.39 924.408 1450.39 Q929.779 1450.39 932.973 1453.08 Q936.167 1455.77 936.167 1460.26 Q936.167 1462.39 935.357 1464.31 Q934.57 1466.2 932.464 1468.8 Q931.885 1469.47 928.783 1472.69 Q925.681 1475.88 920.033 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M956.167 1454.1 Q952.556 1454.1 950.728 1457.66 Q948.922 1461.2 948.922 1468.33 Q948.922 1475.44 950.728 1479.01 Q952.556 1482.55 956.167 1482.55 Q959.802 1482.55 961.607 1479.01 Q963.436 1475.44 963.436 1468.33 Q963.436 1461.2 961.607 1457.66 Q959.802 1454.1 956.167 1454.1 M956.167 1450.39 Q961.977 1450.39 965.033 1455 Q968.112 1459.58 968.112 1468.33 Q968.112 1477.06 965.033 1481.67 Q961.977 1486.25 956.167 1486.25 Q950.357 1486.25 947.278 1481.67 Q944.223 1477.06 944.223 1468.33 Q944.223 1459.58 947.278 1455 Q950.357 1450.39 956.167 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1267.78 1466.95 Q1271.14 1467.66 1273.01 1469.93 Q1274.91 1472.2 1274.91 1475.53 Q1274.91 1480.65 1271.39 1483.45 Q1267.87 1486.25 1261.39 1486.25 Q1259.22 1486.25 1256.9 1485.81 Q1254.61 1485.39 1252.16 1484.54 L1252.16 1480.02 Q1254.1 1481.16 1256.42 1481.74 Q1258.73 1482.32 1261.25 1482.32 Q1265.65 1482.32 1267.94 1480.58 Q1270.26 1478.84 1270.26 1475.53 Q1270.26 1472.48 1268.11 1470.77 Q1265.98 1469.03 1262.16 1469.03 L1258.13 1469.03 L1258.13 1465.19 L1262.34 1465.19 Q1265.79 1465.19 1267.62 1463.82 Q1269.45 1462.43 1269.45 1459.84 Q1269.45 1457.18 1267.55 1455.77 Q1265.67 1454.33 1262.16 1454.33 Q1260.23 1454.33 1258.04 1454.75 Q1255.84 1455.16 1253.2 1456.04 L1253.2 1451.88 Q1255.86 1451.14 1258.17 1450.77 Q1260.51 1450.39 1262.57 1450.39 Q1267.9 1450.39 1271 1452.83 Q1274.1 1455.23 1274.1 1459.35 Q1274.1 1462.22 1272.46 1464.21 Q1270.81 1466.18 1267.78 1466.95 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1293.78 1454.1 Q1290.17 1454.1 1288.34 1457.66 Q1286.53 1461.2 1286.53 1468.33 Q1286.53 1475.44 1288.34 1479.01 Q1290.17 1482.55 1293.78 1482.55 Q1297.41 1482.55 1299.22 1479.01 Q1301.04 1475.44 1301.04 1468.33 Q1301.04 1461.2 1299.22 1457.66 Q1297.41 1454.1 1293.78 1454.1 M1293.78 1450.39 Q1299.59 1450.39 1302.64 1455 Q1305.72 1459.58 1305.72 1468.33 Q1305.72 1477.06 1302.64 1481.67 Q1299.59 1486.25 1293.78 1486.25 Q1287.97 1486.25 1284.89 1481.67 Q1281.83 1477.06 1281.83 1468.33 Q1281.83 1459.58 1284.89 1455 Q1287.97 1450.39 1293.78 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1604.79 1455.09 L1592.98 1473.54 L1604.79 1473.54 L1604.79 1455.09 M1603.56 1451.02 L1609.44 1451.02 L1609.44 1473.54 L1614.37 1473.54 L1614.37 1477.43 L1609.44 1477.43 L1609.44 1485.58 L1604.79 1485.58 L1604.79 1477.43 L1589.19 1477.43 L1589.19 1472.92 L1603.56 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1632.1 1454.1 Q1628.49 1454.1 1626.66 1457.66 Q1624.86 1461.2 1624.86 1468.33 Q1624.86 1475.44 1626.66 1479.01 Q1628.49 1482.55 1632.1 1482.55 Q1635.74 1482.55 1637.54 1479.01 Q1639.37 1475.44 1639.37 1468.33 Q1639.37 1461.2 1637.54 1457.66 Q1635.74 1454.1 1632.1 1454.1 M1632.1 1450.39 Q1637.91 1450.39 1640.97 1455 Q1644.05 1459.58 1644.05 1468.33 Q1644.05 1477.06 1640.97 1481.67 Q1637.91 1486.25 1632.1 1486.25 Q1626.29 1486.25 1623.21 1481.67 Q1620.16 1477.06 1620.16 1468.33 Q1620.16 1459.58 1623.21 1455 Q1626.29 1450.39 1632.1 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1928.99 1451.02 L1947.35 1451.02 L1947.35 1454.96 L1933.28 1454.96 L1933.28 1463.43 Q1934.3 1463.08 1935.31 1462.92 Q1936.33 1462.73 1937.35 1462.73 Q1943.14 1462.73 1946.52 1465.9 Q1949.9 1469.08 1949.9 1474.49 Q1949.9 1480.07 1946.43 1483.17 Q1942.95 1486.25 1936.63 1486.25 Q1934.46 1486.25 1932.19 1485.88 Q1929.94 1485.51 1927.54 1484.77 L1927.54 1480.07 Q1929.62 1481.2 1931.84 1481.76 Q1934.06 1482.32 1936.54 1482.32 Q1940.55 1482.32 1942.88 1480.21 Q1945.22 1478.1 1945.22 1474.49 Q1945.22 1470.88 1942.88 1468.77 Q1940.55 1466.67 1936.54 1466.67 Q1934.67 1466.67 1932.79 1467.08 Q1930.94 1467.5 1928.99 1468.38 L1928.99 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1969.11 1454.1 Q1965.5 1454.1 1963.67 1457.66 Q1961.86 1461.2 1961.86 1468.33 Q1961.86 1475.44 1963.67 1479.01 Q1965.5 1482.55 1969.11 1482.55 Q1972.74 1482.55 1974.55 1479.01 Q1976.38 1475.44 1976.38 1468.33 Q1976.38 1461.2 1974.55 1457.66 Q1972.74 1454.1 1969.11 1454.1 M1969.11 1450.39 Q1974.92 1450.39 1977.98 1455 Q1981.05 1459.58 1981.05 1468.33 Q1981.05 1477.06 1977.98 1481.67 Q1974.92 1486.25 1969.11 1486.25 Q1963.3 1486.25 1960.22 1481.67 Q1957.17 1477.06 1957.17 1468.33 Q1957.17 1459.58 1960.22 1455 Q1963.3 1450.39 1969.11 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M2277.38 1466.44 Q2274.23 1466.44 2272.38 1468.59 Q2270.55 1470.74 2270.55 1474.49 Q2270.55 1478.22 2272.38 1480.39 Q2274.23 1482.55 2277.38 1482.55 Q2280.53 1482.55 2282.36 1480.39 Q2284.21 1478.22 2284.21 1474.49 Q2284.21 1470.74 2282.36 1468.59 Q2280.53 1466.44 2277.38 1466.44 M2286.66 1451.78 L2286.66 1456.04 Q2284.9 1455.21 2283.1 1454.77 Q2281.31 1454.33 2279.55 1454.33 Q2274.93 1454.33 2272.47 1457.45 Q2270.04 1460.58 2269.69 1466.9 Q2271.06 1464.89 2273.12 1463.82 Q2275.18 1462.73 2277.66 1462.73 Q2282.87 1462.73 2285.87 1465.9 Q2288.91 1469.05 2288.91 1474.49 Q2288.91 1479.82 2285.76 1483.03 Q2282.61 1486.25 2277.38 1486.25 Q2271.38 1486.25 2268.21 1481.67 Q2265.04 1477.06 2265.04 1468.33 Q2265.04 1460.14 2268.93 1455.28 Q2272.82 1450.39 2279.37 1450.39 Q2281.13 1450.39 2282.91 1450.74 Q2284.72 1451.09 2286.66 1451.78 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M2306.96 1454.1 Q2303.35 1454.1 2301.52 1457.66 Q2299.72 1461.2 2299.72 1468.33 Q2299.72 1475.44 2301.52 1479.01 Q2303.35 1482.55 2306.96 1482.55 Q2310.6 1482.55 2312.4 1479.01 Q2314.23 1475.44 2314.23 1468.33 Q2314.23 1461.2 2312.4 1457.66 Q2310.6 1454.1 2306.96 1454.1 M2306.96 1450.39 Q2312.77 1450.39 2315.83 1455 Q2318.91 1459.58 2318.91 1468.33 Q2318.91 1477.06 2315.83 1481.67 Q2312.77 1486.25 2306.96 1486.25 Q2301.15 1486.25 2298.07 1481.67 Q2295.02 1477.06 2295.02 1468.33 Q2295.02 1459.58 2298.07 1455 Q2301.15 1450.39 2306.96 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1243.74 1561.26 L1243.74 1548.5 L1233.23 1548.5 L1233.23 1543.22 L1250.1 1543.22 L1250.1 1563.62 Q1246.38 1566.26 1241.89 1567.63 Q1237.4 1568.97 1232.31 1568.97 Q1221.17 1568.97 1214.87 1562.47 Q1208.6 1555.95 1208.6 1544.33 Q1208.6 1532.68 1214.87 1526.19 Q1221.17 1519.66 1232.31 1519.66 Q1236.96 1519.66 1241.13 1520.81 Q1245.33 1521.96 1248.86 1524.18 L1248.86 1531.03 Q1245.3 1528 1241.29 1526.48 Q1237.27 1524.95 1232.85 1524.95 Q1224.13 1524.95 1219.74 1529.82 Q1215.38 1534.69 1215.38 1544.33 Q1215.38 1553.94 1219.74 1558.81 Q1224.13 1563.68 1232.85 1563.68 Q1236.26 1563.68 1238.93 1563.11 Q1241.6 1562.51 1243.74 1561.26 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1277.79 1550.12 Q1270.69 1550.12 1267.96 1551.75 Q1265.22 1553.37 1265.22 1557.29 Q1265.22 1560.4 1267.26 1562.25 Q1269.33 1564.07 1272.86 1564.07 Q1277.73 1564.07 1280.66 1560.63 Q1283.62 1557.16 1283.62 1551.43 L1283.62 1550.12 L1277.79 1550.12 M1289.47 1547.71 L1289.47 1568.04 L1283.62 1568.04 L1283.62 1562.63 Q1281.61 1565.88 1278.62 1567.44 Q1275.63 1568.97 1271.3 1568.97 Q1265.83 1568.97 1262.58 1565.91 Q1259.36 1562.82 1259.36 1557.67 Q1259.36 1551.65 1263.37 1548.6 Q1267.42 1545.54 1275.41 1545.54 L1283.62 1545.54 L1283.62 1544.97 Q1283.62 1540.93 1280.94 1538.73 Q1278.3 1536.5 1273.5 1536.5 Q1270.44 1536.5 1267.54 1537.23 Q1264.65 1537.97 1261.97 1539.43 L1261.97 1534.02 Q1265.19 1532.78 1268.21 1532.17 Q1271.24 1531.54 1274.1 1531.54 Q1281.83 1531.54 1285.65 1535.55 Q1289.47 1539.56 1289.47 1547.71 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1301.54 1532.4 L1307.39 1532.4 L1307.39 1568.04 L1301.54 1568.04 L1301.54 1532.4 M1301.54 1518.52 L1307.39 1518.52 L1307.39 1525.93 L1301.54 1525.93 L1301.54 1518.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1349.28 1546.53 L1349.28 1568.04 L1343.42 1568.04 L1343.42 1546.72 Q1343.42 1541.66 1341.45 1539.14 Q1339.48 1536.63 1335.53 1536.63 Q1330.79 1536.63 1328.05 1539.65 Q1325.31 1542.68 1325.31 1547.9 L1325.31 1568.04 L1319.42 1568.04 L1319.42 1532.4 L1325.31 1532.4 L1325.31 1537.93 Q1327.41 1534.72 1330.25 1533.13 Q1333.11 1531.54 1336.83 1531.54 Q1342.98 1531.54 1346.13 1535.36 Q1349.28 1539.14 1349.28 1546.53 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,1403.47 2352.76,1403.47 \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,1190.14 2352.76,1190.14 \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,976.803 2352.76,976.803 \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,763.47 2352.76,763.47 \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,550.137 2352.76,550.137 \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,336.805 2352.76,336.805 \"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,123.472 2352.76,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 205.121,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1403.47 224.019,1403.47 \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1190.14 224.019,1190.14 \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,976.803 224.019,976.803 \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,763.47 224.019,763.47 \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,550.137 224.019,550.137 \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,336.805 224.019,336.805 \"/>\n",
       "<polyline clip-path=\"url(#clip620)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,123.472 224.019,123.472 \"/>\n",
       "<path clip-path=\"url(#clip620)\" d=\"M157.177 1389.27 Q153.566 1389.27 151.737 1392.83 Q149.931 1396.37 149.931 1403.5 Q149.931 1410.61 151.737 1414.17 Q153.566 1417.72 157.177 1417.72 Q160.811 1417.72 162.616 1414.17 Q164.445 1410.61 164.445 1403.5 Q164.445 1396.37 162.616 1392.83 Q160.811 1389.27 157.177 1389.27 M157.177 1385.56 Q162.987 1385.56 166.042 1390.17 Q169.121 1394.75 169.121 1403.5 Q169.121 1412.23 166.042 1416.84 Q162.987 1421.42 157.177 1421.42 Q151.366 1421.42 148.288 1416.84 Q145.232 1412.23 145.232 1403.5 Q145.232 1394.75 148.288 1390.17 Q151.366 1385.56 157.177 1385.56 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M117.825 1203.48 L125.464 1203.48 L125.464 1177.11 L117.154 1178.78 L117.154 1174.52 L125.418 1172.86 L130.093 1172.86 L130.093 1203.48 L137.732 1203.48 L137.732 1207.42 L117.825 1207.42 L117.825 1203.48 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M157.177 1175.93 Q153.566 1175.93 151.737 1179.5 Q149.931 1183.04 149.931 1190.17 Q149.931 1197.28 151.737 1200.84 Q153.566 1204.38 157.177 1204.38 Q160.811 1204.38 162.616 1200.84 Q164.445 1197.28 164.445 1190.17 Q164.445 1183.04 162.616 1179.5 Q160.811 1175.93 157.177 1175.93 M157.177 1172.23 Q162.987 1172.23 166.042 1176.84 Q169.121 1181.42 169.121 1190.17 Q169.121 1198.9 166.042 1203.5 Q162.987 1208.09 157.177 1208.09 Q151.366 1208.09 148.288 1203.5 Q145.232 1198.9 145.232 1190.17 Q145.232 1181.42 148.288 1176.84 Q151.366 1172.23 157.177 1172.23 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M121.043 990.147 L137.362 990.147 L137.362 994.083 L115.418 994.083 L115.418 990.147 Q118.08 987.393 122.663 982.763 Q127.269 978.111 128.45 976.768 Q130.695 974.245 131.575 972.509 Q132.478 970.749 132.478 969.06 Q132.478 966.305 130.533 964.569 Q128.612 962.833 125.51 962.833 Q123.311 962.833 120.857 963.597 Q118.427 964.361 115.649 965.912 L115.649 961.189 Q118.473 960.055 120.927 959.476 Q123.38 958.898 125.418 958.898 Q130.788 958.898 133.982 961.583 Q137.177 964.268 137.177 968.759 Q137.177 970.888 136.367 972.81 Q135.579 974.708 133.473 977.3 Q132.894 977.972 129.792 981.189 Q126.691 984.384 121.043 990.147 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M157.177 962.601 Q153.566 962.601 151.737 966.166 Q149.931 969.708 149.931 976.837 Q149.931 983.944 151.737 987.509 Q153.566 991.05 157.177 991.05 Q160.811 991.05 162.616 987.509 Q164.445 983.944 164.445 976.837 Q164.445 969.708 162.616 966.166 Q160.811 962.601 157.177 962.601 M157.177 958.898 Q162.987 958.898 166.042 963.504 Q169.121 968.087 169.121 976.837 Q169.121 985.564 166.042 990.171 Q162.987 994.754 157.177 994.754 Q151.366 994.754 148.288 990.171 Q145.232 985.564 145.232 976.837 Q145.232 968.087 148.288 963.504 Q151.366 958.898 157.177 958.898 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M131.181 762.116 Q134.538 762.833 136.413 765.102 Q138.311 767.37 138.311 770.704 Q138.311 775.819 134.792 778.62 Q131.274 781.421 124.793 781.421 Q122.617 781.421 120.302 780.981 Q118.01 780.565 115.556 779.708 L115.556 775.194 Q117.501 776.329 119.816 776.907 Q122.13 777.486 124.654 777.486 Q129.052 777.486 131.343 775.75 Q133.658 774.014 133.658 770.704 Q133.658 767.648 131.505 765.935 Q129.376 764.199 125.556 764.199 L121.529 764.199 L121.529 760.357 L125.742 760.357 Q129.191 760.357 131.019 758.991 Q132.848 757.602 132.848 755.009 Q132.848 752.347 130.95 750.935 Q129.075 749.5 125.556 749.5 Q123.635 749.5 121.436 749.917 Q119.237 750.334 116.598 751.213 L116.598 747.046 Q119.26 746.306 121.575 745.935 Q123.913 745.565 125.973 745.565 Q131.297 745.565 134.399 747.996 Q137.501 750.403 137.501 754.523 Q137.501 757.394 135.857 759.384 Q134.214 761.352 131.181 762.116 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M157.177 749.269 Q153.566 749.269 151.737 752.833 Q149.931 756.375 149.931 763.505 Q149.931 770.611 151.737 774.176 Q153.566 777.718 157.177 777.718 Q160.811 777.718 162.616 774.176 Q164.445 770.611 164.445 763.505 Q164.445 756.375 162.616 752.833 Q160.811 749.269 157.177 749.269 M157.177 745.565 Q162.987 745.565 166.042 750.171 Q169.121 754.755 169.121 763.505 Q169.121 772.232 166.042 776.838 Q162.987 781.421 157.177 781.421 Q151.366 781.421 148.288 776.838 Q145.232 772.232 145.232 763.505 Q145.232 754.755 148.288 750.171 Q151.366 745.565 157.177 745.565 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M129.862 536.931 L118.056 555.38 L129.862 555.38 L129.862 536.931 M128.635 532.857 L134.515 532.857 L134.515 555.38 L139.445 555.38 L139.445 559.269 L134.515 559.269 L134.515 567.417 L129.862 567.417 L129.862 559.269 L114.26 559.269 L114.26 554.755 L128.635 532.857 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M157.177 535.936 Q153.566 535.936 151.737 539.501 Q149.931 543.043 149.931 550.172 Q149.931 557.279 151.737 560.843 Q153.566 564.385 157.177 564.385 Q160.811 564.385 162.616 560.843 Q164.445 557.279 164.445 550.172 Q164.445 543.043 162.616 539.501 Q160.811 535.936 157.177 535.936 M157.177 532.232 Q162.987 532.232 166.042 536.839 Q169.121 541.422 169.121 550.172 Q169.121 558.899 166.042 563.505 Q162.987 568.089 157.177 568.089 Q151.366 568.089 148.288 563.505 Q145.232 558.899 145.232 550.172 Q145.232 541.422 148.288 536.839 Q151.366 532.232 157.177 532.232 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M117.061 319.525 L135.417 319.525 L135.417 323.46 L121.343 323.46 L121.343 331.932 Q122.362 331.585 123.38 331.423 Q124.399 331.238 125.418 331.238 Q131.205 331.238 134.584 334.409 Q137.964 337.58 137.964 342.997 Q137.964 348.576 134.492 351.677 Q131.019 354.756 124.7 354.756 Q122.524 354.756 120.255 354.386 Q118.01 354.015 115.603 353.275 L115.603 348.576 Q117.686 349.71 119.908 350.265 Q122.13 350.821 124.607 350.821 Q128.612 350.821 130.95 348.714 Q133.288 346.608 133.288 342.997 Q133.288 339.386 130.95 337.279 Q128.612 335.173 124.607 335.173 Q122.732 335.173 120.857 335.589 Q119.006 336.006 117.061 336.886 L117.061 319.525 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M157.177 322.603 Q153.566 322.603 151.737 326.168 Q149.931 329.71 149.931 336.839 Q149.931 343.946 151.737 347.511 Q153.566 351.052 157.177 351.052 Q160.811 351.052 162.616 347.511 Q164.445 343.946 164.445 336.839 Q164.445 329.71 162.616 326.168 Q160.811 322.603 157.177 322.603 M157.177 318.9 Q162.987 318.9 166.042 323.506 Q169.121 328.09 169.121 336.839 Q169.121 345.566 166.042 350.173 Q162.987 354.756 157.177 354.756 Q151.366 354.756 148.288 350.173 Q145.232 345.566 145.232 336.839 Q145.232 328.09 148.288 323.506 Q151.366 318.9 157.177 318.9 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M127.593 121.609 Q124.445 121.609 122.593 123.761 Q120.765 125.914 120.765 129.664 Q120.765 133.391 122.593 135.567 Q124.445 137.72 127.593 137.72 Q130.742 137.72 132.57 135.567 Q134.422 133.391 134.422 129.664 Q134.422 125.914 132.57 123.761 Q130.742 121.609 127.593 121.609 M136.876 106.956 L136.876 111.215 Q135.117 110.382 133.311 109.942 Q131.529 109.502 129.769 109.502 Q125.14 109.502 122.686 112.627 Q120.255 115.752 119.908 122.072 Q121.274 120.058 123.334 118.993 Q125.394 117.905 127.871 117.905 Q133.08 117.905 136.089 121.076 Q139.121 124.224 139.121 129.664 Q139.121 134.988 135.973 138.206 Q132.825 141.423 127.593 141.423 Q121.598 141.423 118.427 136.84 Q115.256 132.234 115.256 123.507 Q115.256 115.312 119.144 110.451 Q123.033 105.567 129.584 105.567 Q131.343 105.567 133.126 105.914 Q134.931 106.262 136.876 106.956 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M157.177 109.271 Q153.566 109.271 151.737 112.836 Q149.931 116.377 149.931 123.507 Q149.931 130.613 151.737 134.178 Q153.566 137.72 157.177 137.72 Q160.811 137.72 162.616 134.178 Q164.445 130.613 164.445 123.507 Q164.445 116.377 162.616 112.836 Q160.811 109.271 157.177 109.271 M157.177 105.567 Q162.987 105.567 166.042 110.174 Q169.121 114.757 169.121 123.507 Q169.121 132.234 166.042 136.84 Q162.987 141.423 157.177 141.423 Q151.366 141.423 148.288 136.84 Q145.232 132.234 145.232 123.507 Q145.232 114.757 148.288 110.174 Q151.366 105.567 157.177 105.567 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M16.4842 891.553 L16.4842 864.244 L21.895 864.244 L21.895 885.124 L35.8996 885.124 L35.8996 866.281 L41.3104 866.281 L41.3104 885.124 L64.0042 885.124 L64.0042 891.553 L16.4842 891.553 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M44.7161 827.387 L47.5806 827.387 L47.5806 854.314 Q53.6281 853.932 56.8109 850.685 Q59.9619 847.407 59.9619 841.582 Q59.9619 838.208 59.1344 835.057 Q58.3069 831.875 56.6518 828.755 L62.1899 828.755 Q63.5267 831.906 64.227 835.217 Q64.9272 838.527 64.9272 841.932 Q64.9272 850.462 59.9619 855.46 Q54.9967 860.425 46.5303 860.425 Q37.7774 860.425 32.6531 855.714 Q27.4968 850.972 27.4968 842.951 Q27.4968 835.758 32.1438 831.588 Q36.7589 827.387 44.7161 827.387 M42.9973 833.243 Q38.1912 833.307 35.3266 835.949 Q32.4621 838.559 32.4621 842.887 Q32.4621 847.789 35.2312 850.749 Q38.0002 853.677 43.0292 854.123 L42.9973 833.243 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M46.0847 801.574 Q46.0847 808.672 47.7079 811.409 Q49.3312 814.146 53.2461 814.146 Q56.3653 814.146 58.2114 812.109 Q60.0256 810.04 60.0256 806.507 Q60.0256 801.637 56.5881 798.709 Q53.1188 795.749 47.3897 795.749 L46.0847 795.749 L46.0847 801.574 M43.6657 789.893 L64.0042 789.893 L64.0042 795.749 L58.5933 795.749 Q61.8398 797.754 63.3994 800.746 Q64.9272 803.738 64.9272 808.067 Q64.9272 813.541 61.8716 816.788 Q58.7843 820.003 53.6281 820.003 Q47.6125 820.003 44.5569 815.992 Q41.5014 811.95 41.5014 803.961 L41.5014 795.749 L40.9285 795.749 Q36.8862 795.749 34.6901 798.423 Q32.4621 801.065 32.4621 805.871 Q32.4621 808.926 33.1941 811.823 Q33.9262 814.719 35.3903 817.393 L29.9795 817.393 Q28.7381 814.178 28.1334 811.154 Q27.4968 808.13 27.4968 805.266 Q27.4968 797.532 31.5072 793.712 Q35.5176 789.893 43.6657 789.893 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M18.2347 772.037 L28.3562 772.037 L28.3562 759.974 L32.9077 759.974 L32.9077 772.037 L52.2594 772.037 Q56.6199 772.037 57.8613 770.859 Q59.1026 769.65 59.1026 765.99 L59.1026 759.974 L64.0042 759.974 L64.0042 765.99 Q64.0042 772.769 61.4897 775.347 Q58.9434 777.925 52.2594 777.925 L32.9077 777.925 L32.9077 782.222 L28.3562 782.222 L28.3562 777.925 L18.2347 777.925 L18.2347 772.037 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M49.9359 752.876 L28.3562 752.876 L28.3562 747.02 L49.7131 747.02 Q54.7739 747.02 57.3202 745.046 Q59.8346 743.073 59.8346 739.126 Q59.8346 734.384 56.8109 731.647 Q53.7872 728.877 48.5673 728.877 L28.3562 728.877 L28.3562 723.021 L64.0042 723.021 L64.0042 728.877 L58.5296 728.877 Q61.7762 731.01 63.3676 733.843 Q64.9272 736.644 64.9272 740.368 Q64.9272 746.51 61.1078 749.693 Q57.2883 752.876 49.9359 752.876 M27.4968 738.14 L27.4968 738.14 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M33.8307 690.301 Q33.2578 691.288 33.0032 692.466 Q32.7167 693.611 32.7167 695.012 Q32.7167 699.977 35.9632 702.651 Q39.1779 705.292 45.2253 705.292 L64.0042 705.292 L64.0042 711.181 L28.3562 711.181 L28.3562 705.292 L33.8944 705.292 Q30.6479 703.446 29.0883 700.486 Q27.4968 697.526 27.4968 693.293 Q27.4968 692.688 27.5923 691.956 Q27.656 691.224 27.8151 690.333 L33.8307 690.301 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M44.7161 655.099 L47.5806 655.099 L47.5806 682.026 Q53.6281 681.644 56.8109 678.397 Q59.9619 675.119 59.9619 669.294 Q59.9619 665.921 59.1344 662.77 Q58.3069 659.587 56.6518 656.468 L62.1899 656.468 Q63.5267 659.619 64.227 662.929 Q64.9272 666.239 64.9272 669.645 Q64.9272 678.175 59.9619 683.172 Q54.9967 688.137 46.5303 688.137 Q37.7774 688.137 32.6531 683.426 Q27.4968 678.684 27.4968 670.663 Q27.4968 663.47 32.1438 659.3 Q36.7589 655.099 44.7161 655.099 M42.9973 660.955 Q38.1912 661.019 35.3266 663.661 Q32.4621 666.271 32.4621 670.599 Q32.4621 675.501 35.2312 678.461 Q38.0002 681.389 43.0292 681.835 L42.9973 660.955 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M877.575 12.096 L912.332 12.096 L912.332 18.9825 L885.758 18.9825 L885.758 36.8065 L909.739 36.8065 L909.739 43.6931 L885.758 43.6931 L885.758 72.576 L877.575 72.576 L877.575 12.096 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M959.241 48.0275 L959.241 51.6733 L924.97 51.6733 Q925.457 59.3701 929.588 63.421 Q933.761 67.4314 941.174 67.4314 Q945.468 67.4314 949.478 66.3781 Q953.529 65.3249 957.499 63.2184 L957.499 70.267 Q953.489 71.9684 949.276 72.8596 Q945.063 73.7508 940.728 73.7508 Q929.872 73.7508 923.512 67.4314 Q917.193 61.1119 917.193 50.3365 Q917.193 39.1965 923.188 32.6746 Q929.224 26.1121 939.432 26.1121 Q948.587 26.1121 953.894 32.0264 Q959.241 37.9003 959.241 48.0275 M951.787 45.84 Q951.706 39.7232 948.344 36.0774 Q945.022 32.4315 939.513 32.4315 Q933.275 32.4315 929.507 35.9558 Q925.781 39.4801 925.213 45.8805 L951.787 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M992.094 49.7694 Q983.06 49.7694 979.577 51.8354 Q976.093 53.9013 976.093 58.8839 Q976.093 62.8538 978.685 65.2034 Q981.319 67.5124 985.815 67.5124 Q992.013 67.5124 995.74 63.1374 Q999.507 58.7219 999.507 51.4303 L999.507 49.7694 L992.094 49.7694 M1006.96 46.6907 L1006.96 72.576 L999.507 72.576 L999.507 65.6895 Q996.955 69.8214 993.147 71.8063 Q989.339 73.7508 983.83 73.7508 Q976.863 73.7508 972.731 69.8619 Q968.639 65.9325 968.639 59.3701 Q968.639 51.7138 973.743 47.825 Q978.888 43.9361 989.056 43.9361 L999.507 43.9361 L999.507 43.2069 Q999.507 38.0623 996.104 35.2672 Q992.742 32.4315 986.625 32.4315 Q982.736 32.4315 979.05 33.3632 Q975.364 34.295 971.961 36.1584 L971.961 29.2718 Q976.052 27.692 979.901 26.9223 Q983.749 26.1121 987.395 26.1121 Q997.239 26.1121 1002.1 31.2163 Q1006.96 36.3204 1006.96 46.6907 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1029.69 14.324 L1029.69 27.2059 L1045.04 27.2059 L1045.04 32.9987 L1029.69 32.9987 L1029.69 57.6282 Q1029.69 63.1779 1031.19 64.7578 Q1032.72 66.3376 1037.38 66.3376 L1045.04 66.3376 L1045.04 72.576 L1037.38 72.576 Q1028.75 72.576 1025.47 69.3758 Q1022.19 66.1351 1022.19 57.6282 L1022.19 32.9987 L1016.72 32.9987 L1016.72 27.2059 L1022.19 27.2059 L1022.19 14.324 L1029.69 14.324 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1054.07 54.671 L1054.07 27.2059 L1061.53 27.2059 L1061.53 54.3874 Q1061.53 60.8284 1064.04 64.0691 Q1066.55 67.2693 1071.57 67.2693 Q1077.61 67.2693 1081.09 63.421 Q1084.62 59.5726 1084.62 52.9291 L1084.62 27.2059 L1092.07 27.2059 L1092.07 72.576 L1084.62 72.576 L1084.62 65.6084 Q1081.9 69.7404 1078.3 71.7658 Q1074.73 73.7508 1069.99 73.7508 Q1062.17 73.7508 1058.12 68.8897 Q1054.07 64.0286 1054.07 54.671 M1072.83 26.1121 L1072.83 26.1121 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1133.71 34.1734 Q1132.46 33.4443 1130.96 33.1202 Q1129.5 32.7556 1127.72 32.7556 Q1121.4 32.7556 1118 36.8875 Q1114.63 40.9789 1114.63 48.6757 L1114.63 72.576 L1107.14 72.576 L1107.14 27.2059 L1114.63 27.2059 L1114.63 34.2544 Q1116.98 30.1225 1120.75 28.1376 Q1124.52 26.1121 1129.91 26.1121 Q1130.68 26.1121 1131.61 26.2337 Q1132.54 26.3147 1133.67 26.5172 L1133.71 34.1734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1178.52 48.0275 L1178.52 51.6733 L1144.25 51.6733 Q1144.73 59.3701 1148.86 63.421 Q1153.04 67.4314 1160.45 67.4314 Q1164.74 67.4314 1168.75 66.3781 Q1172.8 65.3249 1176.77 63.2184 L1176.77 70.267 Q1172.76 71.9684 1168.55 72.8596 Q1164.34 73.7508 1160 73.7508 Q1149.15 73.7508 1142.79 67.4314 Q1136.47 61.1119 1136.47 50.3365 Q1136.47 39.1965 1142.46 32.6746 Q1148.5 26.1121 1158.71 26.1121 Q1167.86 26.1121 1173.17 32.0264 Q1178.52 37.9003 1178.52 48.0275 M1171.06 45.84 Q1170.98 39.7232 1167.62 36.0774 Q1164.3 32.4315 1158.79 32.4315 Q1152.55 32.4315 1148.78 35.9558 Q1145.06 39.4801 1144.49 45.8805 L1171.06 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1217.45 12.096 L1225.63 12.096 L1225.63 72.576 L1217.45 72.576 L1217.45 12.096 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1276.91 35.9153 Q1279.71 30.8922 1283.6 28.5022 Q1287.49 26.1121 1292.75 26.1121 Q1299.84 26.1121 1303.69 31.0947 Q1307.54 36.0368 1307.54 45.1919 L1307.54 72.576 L1300.04 72.576 L1300.04 45.4349 Q1300.04 38.913 1297.73 35.7533 Q1295.43 32.5936 1290.69 32.5936 Q1284.89 32.5936 1281.53 36.4419 Q1278.17 40.2903 1278.17 46.9338 L1278.17 72.576 L1270.67 72.576 L1270.67 45.4349 Q1270.67 38.8725 1268.37 35.7533 Q1266.06 32.5936 1261.24 32.5936 Q1255.52 32.5936 1252.16 36.4824 Q1248.8 40.3308 1248.8 46.9338 L1248.8 72.576 L1241.31 72.576 L1241.31 27.2059 L1248.8 27.2059 L1248.8 34.2544 Q1251.35 30.082 1254.92 28.0971 Q1258.48 26.1121 1263.38 26.1121 Q1268.33 26.1121 1271.77 28.6237 Q1275.25 31.1352 1276.91 35.9153 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1329.62 65.7705 L1329.62 89.8329 L1322.12 89.8329 L1322.12 27.2059 L1329.62 27.2059 L1329.62 34.0924 Q1331.96 30.0415 1335.53 28.0971 Q1339.13 26.1121 1344.12 26.1121 Q1352.38 26.1121 1357.53 32.6746 Q1362.71 39.2371 1362.71 49.9314 Q1362.71 60.6258 1357.53 67.1883 Q1352.38 73.7508 1344.12 73.7508 Q1339.13 73.7508 1335.53 71.8063 Q1331.96 69.8214 1329.62 65.7705 M1354.97 49.9314 Q1354.97 41.7081 1351.57 37.0496 Q1348.21 32.3505 1342.29 32.3505 Q1336.38 32.3505 1332.98 37.0496 Q1329.62 41.7081 1329.62 49.9314 Q1329.62 58.1548 1332.98 62.8538 Q1336.38 67.5124 1342.29 67.5124 Q1348.21 67.5124 1351.57 62.8538 Q1354.97 58.1548 1354.97 49.9314 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1392.65 32.4315 Q1386.65 32.4315 1383.17 37.1306 Q1379.68 41.7891 1379.68 49.9314 Q1379.68 58.0738 1383.13 62.7728 Q1386.61 67.4314 1392.65 67.4314 Q1398.6 67.4314 1402.09 62.7323 Q1405.57 58.0333 1405.57 49.9314 Q1405.57 41.8701 1402.09 37.1711 Q1398.6 32.4315 1392.65 32.4315 M1392.65 26.1121 Q1402.37 26.1121 1407.92 32.4315 Q1413.47 38.7509 1413.47 49.9314 Q1413.47 61.0714 1407.92 67.4314 Q1402.37 73.7508 1392.65 73.7508 Q1382.88 73.7508 1377.33 67.4314 Q1371.83 61.0714 1371.83 49.9314 Q1371.83 38.7509 1377.33 32.4315 Q1382.88 26.1121 1392.65 26.1121 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1452.11 34.1734 Q1450.86 33.4443 1449.36 33.1202 Q1447.9 32.7556 1446.12 32.7556 Q1439.8 32.7556 1436.4 36.8875 Q1433.03 40.9789 1433.03 48.6757 L1433.03 72.576 L1425.54 72.576 L1425.54 27.2059 L1433.03 27.2059 L1433.03 34.2544 Q1435.38 30.1225 1439.15 28.1376 Q1442.92 26.1121 1448.31 26.1121 Q1449.08 26.1121 1450.01 26.2337 Q1450.94 26.3147 1452.07 26.5172 L1452.11 34.1734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1467.31 14.324 L1467.31 27.2059 L1482.66 27.2059 L1482.66 32.9987 L1467.31 32.9987 L1467.31 57.6282 Q1467.31 63.1779 1468.8 64.7578 Q1470.34 66.3376 1475 66.3376 L1482.66 66.3376 L1482.66 72.576 L1475 72.576 Q1466.37 72.576 1463.09 69.3758 Q1459.81 66.1351 1459.81 57.6282 L1459.81 32.9987 L1454.34 32.9987 L1454.34 27.2059 L1459.81 27.2059 L1459.81 14.324 L1467.31 14.324 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1513.08 49.7694 Q1504.05 49.7694 1500.56 51.8354 Q1497.08 53.9013 1497.08 58.8839 Q1497.08 62.8538 1499.67 65.2034 Q1502.31 67.5124 1506.8 67.5124 Q1513 67.5124 1516.73 63.1374 Q1520.49 58.7219 1520.49 51.4303 L1520.49 49.7694 L1513.08 49.7694 M1527.95 46.6907 L1527.95 72.576 L1520.49 72.576 L1520.49 65.6895 Q1517.94 69.8214 1514.13 71.8063 Q1510.33 73.7508 1504.82 73.7508 Q1497.85 73.7508 1493.72 69.8619 Q1489.63 65.9325 1489.63 59.3701 Q1489.63 51.7138 1494.73 47.825 Q1499.87 43.9361 1510.04 43.9361 L1520.49 43.9361 L1520.49 43.2069 Q1520.49 38.0623 1517.09 35.2672 Q1513.73 32.4315 1507.61 32.4315 Q1503.72 32.4315 1500.04 33.3632 Q1496.35 34.295 1492.95 36.1584 L1492.95 29.2718 Q1497.04 27.692 1500.89 26.9223 Q1504.74 26.1121 1508.38 26.1121 Q1518.23 26.1121 1523.09 31.2163 Q1527.95 36.3204 1527.95 46.6907 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1581.01 45.1919 L1581.01 72.576 L1573.56 72.576 L1573.56 45.4349 Q1573.56 38.994 1571.05 35.7938 Q1568.54 32.5936 1563.51 32.5936 Q1557.48 32.5936 1553.99 36.4419 Q1550.51 40.2903 1550.51 46.9338 L1550.51 72.576 L1543.02 72.576 L1543.02 27.2059 L1550.51 27.2059 L1550.51 34.2544 Q1553.18 30.163 1556.79 28.1376 Q1560.44 26.1121 1565.18 26.1121 Q1572.99 26.1121 1577 30.9732 Q1581.01 35.7938 1581.01 45.1919 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1628.53 28.9478 L1628.53 35.9153 Q1625.37 34.1734 1622.17 33.3227 Q1619.01 32.4315 1615.77 32.4315 Q1608.52 32.4315 1604.51 37.0496 Q1600.5 41.6271 1600.5 49.9314 Q1600.5 58.2358 1604.51 62.8538 Q1608.52 67.4314 1615.77 67.4314 Q1619.01 67.4314 1622.17 66.5807 Q1625.37 65.6895 1628.53 63.9476 L1628.53 70.8341 Q1625.41 72.2924 1622.05 73.0216 Q1618.73 73.7508 1614.96 73.7508 Q1604.71 73.7508 1598.68 67.3098 Q1592.64 60.8689 1592.64 49.9314 Q1592.64 38.832 1598.72 32.472 Q1604.83 26.1121 1615.45 26.1121 Q1618.89 26.1121 1622.17 26.8413 Q1625.45 27.5299 1628.53 28.9478 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip620)\" d=\"M1680.3 48.0275 L1680.3 51.6733 L1646.03 51.6733 Q1646.52 59.3701 1650.65 63.421 Q1654.82 67.4314 1662.23 67.4314 Q1666.53 67.4314 1670.54 66.3781 Q1674.59 65.3249 1678.56 63.2184 L1678.56 70.267 Q1674.55 71.9684 1670.34 72.8596 Q1666.12 73.7508 1661.79 73.7508 Q1650.93 73.7508 1644.57 67.4314 Q1638.25 61.1119 1638.25 50.3365 Q1638.25 39.1965 1644.25 32.6746 Q1650.28 26.1121 1660.49 26.1121 Q1669.65 26.1121 1674.95 32.0264 Q1680.3 37.9003 1680.3 48.0275 M1672.85 45.84 Q1672.77 39.7232 1669.41 36.0774 Q1666.08 32.4315 1660.57 32.4315 Q1654.34 32.4315 1650.57 35.9558 Q1646.84 39.4801 1646.27 45.8805 L1672.85 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip622)\" d=\"M637.349 1390.67 L265.903 1390.67 L265.903 1373.6 L637.349 1373.6 L637.349 1390.67 L637.349 1390.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"637.349,1390.67 265.903,1390.67 265.903,1373.6 637.349,1373.6 637.349,1390.67 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M806.189 1369.33 L265.903 1369.33 L265.903 1352.27 L806.189 1352.27 L806.189 1369.33 L806.189 1369.33  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"806.189,1369.33 265.903,1369.33 265.903,1352.27 806.189,1352.27 806.189,1369.33 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1414.01 1348 L265.903 1348 L265.903 1330.93 L1414.01 1330.93 L1414.01 1348 L1414.01 1348  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1414.01,1348 265.903,1348 265.903,1330.93 1414.01,1330.93 1414.01,1348 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1312.71 1326.67 L265.903 1326.67 L265.903 1309.6 L1312.71 1309.6 L1312.71 1326.67 L1312.71 1326.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1312.71,1326.67 265.903,1326.67 265.903,1309.6 1312.71,1309.6 1312.71,1326.67 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M2021.83 1305.33 L265.903 1305.33 L265.903 1288.27 L2021.83 1288.27 L2021.83 1305.33 L2021.83 1305.33  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2021.83,1305.33 265.903,1305.33 265.903,1288.27 2021.83,1288.27 2021.83,1305.33 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1481.55 1284 L265.903 1284 L265.903 1266.94 L1481.55 1266.94 L1481.55 1284 L1481.55 1284  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1481.55,1284 265.903,1284 265.903,1266.94 1481.55,1266.94 1481.55,1284 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M772.421 1262.67 L265.903 1262.67 L265.903 1245.6 L772.421 1245.6 L772.421 1262.67 L772.421 1262.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"772.421,1262.67 265.903,1262.67 265.903,1245.6 772.421,1245.6 772.421,1262.67 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M400.974 1241.34 L265.903 1241.34 L265.903 1224.27 L400.974 1224.27 L400.974 1241.34 L400.974 1241.34  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"400.974,1241.34 265.903,1241.34 265.903,1224.27 400.974,1224.27 400.974,1241.34 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1785.46 1220 L265.903 1220 L265.903 1202.94 L1785.46 1202.94 L1785.46 1220 L1785.46 1220  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1785.46,1220 265.903,1220 265.903,1202.94 1785.46,1202.94 1785.46,1220 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1177.63 1198.67 L265.903 1198.67 L265.903 1181.6 L1177.63 1181.6 L1177.63 1198.67 L1177.63 1198.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1177.63,1198.67 265.903,1198.67 265.903,1181.6 1177.63,1181.6 1177.63,1198.67 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M873.724 1177.34 L265.903 1177.34 L265.903 1160.27 L873.724 1160.27 L873.724 1177.34 L873.724 1177.34  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"873.724,1177.34 265.903,1177.34 265.903,1160.27 873.724,1160.27 873.724,1177.34 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M569.814 1156 L265.903 1156 L265.903 1138.94 L569.814 1138.94 L569.814 1156 L569.814 1156  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"569.814,1156 265.903,1156 265.903,1138.94 569.814,1138.94 569.814,1156 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1042.56 1134.67 L265.903 1134.67 L265.903 1117.6 L1042.56 1117.6 L1042.56 1134.67 L1042.56 1134.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1042.56,1134.67 265.903,1134.67 265.903,1117.6 1042.56,1117.6 1042.56,1134.67 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M2123.13 1113.34 L265.903 1113.34 L265.903 1096.27 L2123.13 1096.27 L2123.13 1113.34 L2123.13 1113.34  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2123.13,1113.34 265.903,1113.34 265.903,1096.27 2123.13,1096.27 2123.13,1113.34 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1211.4 1092 L265.903 1092 L265.903 1074.94 L1211.4 1074.94 L1211.4 1092 L1211.4 1092  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1211.4,1092 265.903,1092 265.903,1074.94 1211.4,1074.94 1211.4,1092 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1616.62 1070.67 L265.903 1070.67 L265.903 1053.6 L1616.62 1053.6 L1616.62 1070.67 L1616.62 1070.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1616.62,1070.67 265.903,1070.67 265.903,1053.6 1616.62,1053.6 1616.62,1070.67 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1717.92 1049.34 L265.903 1049.34 L265.903 1032.27 L1717.92 1032.27 L1717.92 1049.34 L1717.92 1049.34  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1717.92,1049.34 265.903,1049.34 265.903,1032.27 1717.92,1032.27 1717.92,1049.34 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1886.76 1028 L265.903 1028 L265.903 1010.94 L1886.76 1010.94 L1886.76 1028 L1886.76 1028  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1886.76,1028 265.903,1028 265.903,1010.94 1886.76,1010.94 1886.76,1028 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1852.99 1006.67 L265.903 1006.67 L265.903 989.603 L1852.99 989.603 L1852.99 1006.67 L1852.99 1006.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1852.99,1006.67 265.903,1006.67 265.903,989.603 1852.99,989.603 1852.99,1006.67 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1751.69 985.336 L265.903 985.336 L265.903 968.269 L1751.69 968.269 L1751.69 985.336 L1751.69 985.336  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1751.69,985.336 265.903,985.336 265.903,968.269 1751.69,968.269 1751.69,985.336 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1988.06 964.003 L265.903 964.003 L265.903 946.936 L1988.06 946.936 L1988.06 964.003 L1988.06 964.003  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1988.06,964.003 265.903,964.003 265.903,946.936 1988.06,946.936 1988.06,964.003 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1515.31 942.669 L265.903 942.669 L265.903 925.603 L1515.31 925.603 L1515.31 942.669 L1515.31 942.669  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1515.31,942.669 265.903,942.669 265.903,925.603 1515.31,925.603 1515.31,942.669 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M299.671 921.336 L265.903 921.336 L265.903 904.27 L299.671 904.27 L299.671 921.336 L299.671 921.336  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"299.671,921.336 265.903,921.336 265.903,904.27 299.671,904.27 299.671,921.336 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M536.046 900.003 L265.903 900.003 L265.903 882.936 L536.046 882.936 L536.046 900.003 L536.046 900.003  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"536.046,900.003 265.903,900.003 265.903,882.936 536.046,882.936 536.046,900.003 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M367.207 878.67 L265.903 878.67 L265.903 861.603 L367.207 861.603 L367.207 878.67 L367.207 878.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"367.207,878.67 265.903,878.67 265.903,861.603 367.207,861.603 367.207,878.67 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M333.439 857.336 L265.903 857.336 L265.903 840.27 L333.439 840.27 L333.439 857.336 L333.439 857.336  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"333.439,857.336 265.903,857.336 265.903,840.27 333.439,840.27 333.439,857.336 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M975.028 836.003 L265.903 836.003 L265.903 818.937 L975.028 818.937 L975.028 836.003 L975.028 836.003  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"975.028,836.003 265.903,836.003 265.903,818.937 975.028,818.937 975.028,836.003 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M941.26 814.67 L265.903 814.67 L265.903 797.603 L941.26 797.603 L941.26 814.67 L941.26 814.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"941.26,814.67 265.903,814.67 265.903,797.603 941.26,797.603 941.26,814.67 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1549.08 793.337 L265.903 793.337 L265.903 776.27 L1549.08 776.27 L1549.08 793.337 L1549.08 793.337  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1549.08,793.337 265.903,793.337 265.903,776.27 1549.08,776.27 1549.08,793.337 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M839.956 772.003 L265.903 772.003 L265.903 754.937 L839.956 754.937 L839.956 772.003 L839.956 772.003  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"839.956,772.003 265.903,772.003 265.903,754.937 839.956,754.937 839.956,772.003 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M434.742 750.67 L265.903 750.67 L265.903 733.603 L434.742 733.603 L434.742 750.67 L434.742 750.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"434.742,750.67 265.903,750.67 265.903,733.603 434.742,733.603 434.742,750.67 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M502.278 729.337 L265.903 729.337 L265.903 712.27 L502.278 712.27 L502.278 729.337 L502.278 729.337  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"502.278,729.337 265.903,729.337 265.903,712.27 502.278,712.27 502.278,729.337 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M468.51 708.004 L265.903 708.004 L265.903 690.937 L468.51 690.937 L468.51 708.004 L468.51 708.004  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"468.51,708.004 265.903,708.004 265.903,690.937 468.51,690.937 468.51,708.004 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M603.582 686.67 L265.903 686.67 L265.903 669.604 L603.582 669.604 L603.582 686.67 L603.582 686.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"603.582,686.67 265.903,686.67 265.903,669.604 603.582,669.604 603.582,686.67 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1278.94 665.337 L265.903 665.337 L265.903 648.27 L1278.94 648.27 L1278.94 665.337 L1278.94 665.337  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1278.94,665.337 265.903,665.337 265.903,648.27 1278.94,648.27 1278.94,665.337 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M671.117 644.004 L265.903 644.004 L265.903 626.937 L671.117 626.937 L671.117 644.004 L671.117 644.004  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"671.117,644.004 265.903,644.004 265.903,626.937 671.117,626.937 671.117,644.004 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1684.15 622.67 L265.903 622.67 L265.903 605.604 L1684.15 605.604 L1684.15 622.67 L1684.15 622.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1684.15,622.67 265.903,622.67 265.903,605.604 1684.15,605.604 1684.15,622.67 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1143.87 601.337 L265.903 601.337 L265.903 584.271 L1143.87 584.271 L1143.87 601.337 L1143.87 601.337  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1143.87,601.337 265.903,601.337 265.903,584.271 1143.87,584.271 1143.87,601.337 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1380.24 580.004 L265.903 580.004 L265.903 562.937 L1380.24 562.937 L1380.24 580.004 L1380.24 580.004  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1380.24,580.004 265.903,580.004 265.903,562.937 1380.24,562.937 1380.24,580.004 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1819.22 558.671 L265.903 558.671 L265.903 541.604 L1819.22 541.604 L1819.22 558.671 L1819.22 558.671  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1819.22,558.671 265.903,558.671 265.903,541.604 1819.22,541.604 1819.22,558.671 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1110.1 537.337 L265.903 537.337 L265.903 520.271 L1110.1 520.271 L1110.1 537.337 L1110.1 537.337  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1110.1,537.337 265.903,537.337 265.903,520.271 1110.1,520.271 1110.1,537.337 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M2089.37 516.004 L265.903 516.004 L265.903 498.938 L2089.37 498.938 L2089.37 516.004 L2089.37 516.004  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2089.37,516.004 265.903,516.004 265.903,498.938 2089.37,498.938 2089.37,516.004 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1346.47 494.671 L265.903 494.671 L265.903 477.604 L1346.47 477.604 L1346.47 494.671 L1346.47 494.671  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1346.47,494.671 265.903,494.671 265.903,477.604 1346.47,477.604 1346.47,494.671 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M2190.67 473.338 L265.903 473.338 L265.903 456.271 L2190.67 456.271 L2190.67 473.338 L2190.67 473.338  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2190.67,473.338 265.903,473.338 265.903,456.271 2190.67,456.271 2190.67,473.338 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1076.33 452.004 L265.903 452.004 L265.903 434.938 L1076.33 434.938 L1076.33 452.004 L1076.33 452.004  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1076.33,452.004 265.903,452.004 265.903,434.938 1076.33,434.938 1076.33,452.004 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M907.492 430.671 L265.903 430.671 L265.903 413.605 L907.492 413.605 L907.492 430.671 L907.492 430.671  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"907.492,430.671 265.903,430.671 265.903,413.605 907.492,413.605 907.492,430.671 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1582.85 409.338 L265.903 409.338 L265.903 392.271 L1582.85 392.271 L1582.85 409.338 L1582.85 409.338  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1582.85,409.338 265.903,409.338 265.903,392.271 1582.85,392.271 1582.85,409.338 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M1008.8 388.005 L265.903 388.005 L265.903 370.938 L1008.8 370.938 L1008.8 388.005 L1008.8 388.005  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1008.8,388.005 265.903,388.005 265.903,370.938 1008.8,370.938 1008.8,388.005 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M2258.21 366.671 L265.903 366.671 L265.903 349.605 L2258.21 349.605 L2258.21 366.671 L2258.21 366.671  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2258.21,366.671 265.903,366.671 265.903,349.605 2258.21,349.605 2258.21,366.671 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M2156.9 345.338 L265.903 345.338 L265.903 328.271 L2156.9 328.271 L2156.9 345.338 L2156.9 345.338  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2156.9,345.338 265.903,345.338 265.903,328.271 2156.9,328.271 2156.9,345.338 \"/>\n",
       "<path clip-path=\"url(#clip622)\" d=\"M2291.97 324.005 L265.903 324.005 L265.903 306.938 L2291.97 306.938 L2291.97 324.005 L2291.97 324.005  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip622)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2291.97,324.005 265.903,324.005 265.903,306.938 2291.97,306.938 2291.97,324.005 \"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"299.671\" cy=\"1168.8\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"333.439\" cy=\"1062.14\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"367.207\" cy=\"678.137\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"400.974\" cy=\"742.137\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"434.742\" cy=\"294.138\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"468.51\" cy=\"635.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"502.278\" cy=\"1083.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"536.046\" cy=\"1318.13\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"569.814\" cy=\"443.471\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"603.582\" cy=\"827.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"637.349\" cy=\"1019.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"671.117\" cy=\"1211.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"704.885\" cy=\"912.803\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"738.653\" cy=\"230.138\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"772.421\" cy=\"806.137\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"806.189\" cy=\"550.137\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"839.956\" cy=\"486.138\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"873.724\" cy=\"379.471\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"907.492\" cy=\"400.805\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"941.26\" cy=\"464.804\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"975.028\" cy=\"315.471\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1008.8\" cy=\"614.137\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1042.56\" cy=\"1382.13\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1076.33\" cy=\"1232.8\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1110.1\" cy=\"1339.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1143.87\" cy=\"1360.8\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1177.63\" cy=\"955.469\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1211.4\" cy=\"976.803\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1245.17\" cy=\"592.804\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1278.94\" cy=\"1040.8\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1312.71\" cy=\"1296.8\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1346.47\" cy=\"1254.14\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1380.24\" cy=\"1275.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1414.01\" cy=\"1190.14\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1447.78\" cy=\"763.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1481.55\" cy=\"1147.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1515.31\" cy=\"507.471\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1549.08\" cy=\"848.803\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1582.85\" cy=\"699.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1616.62\" cy=\"422.138\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1650.38\" cy=\"870.136\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1684.15\" cy=\"251.472\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1717.92\" cy=\"720.803\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1751.69\" cy=\"187.472\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1785.46\" cy=\"891.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1819.22\" cy=\"998.136\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1852.99\" cy=\"571.471\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1886.76\" cy=\"934.136\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1920.53\" cy=\"144.805\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip622)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1954.3\" cy=\"208.805\" r=\"2\"/>\n",
       "</svg>\n"
      ],
      "text/html": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip670\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip670)\" d=\"M0 1600 L2400 1600 L2400 0 L0 0  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip671\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip670)\" d=\"M205.121 1423.18 L2352.76 1423.18 L2352.76 123.472 L205.121 123.472  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip672\">\n",
       "    <rect x=\"205\" y=\"123\" width=\"2149\" height=\"1301\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"265.903,1423.18 265.903,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"603.582,1423.18 603.582,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"941.26,1423.18 941.26,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1278.94,1423.18 1278.94,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1616.62,1423.18 1616.62,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1954.3,1423.18 1954.3,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2291.97,1423.18 2291.97,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip670)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 2352.76,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip670)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.903,1423.18 265.903,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip670)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"603.582,1423.18 603.582,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip670)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"941.26,1423.18 941.26,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip670)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1278.94,1423.18 1278.94,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip670)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1616.62,1423.18 1616.62,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip670)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1954.3,1423.18 1954.3,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip670)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2291.97,1423.18 2291.97,1404.28 \"/>\n",
       "<path clip-path=\"url(#clip670)\" d=\"M265.903 1454.1 Q262.292 1454.1 260.463 1457.66 Q258.658 1461.2 258.658 1468.33 Q258.658 1475.44 260.463 1479.01 Q262.292 1482.55 265.903 1482.55 Q269.537 1482.55 271.343 1479.01 Q273.172 1475.44 273.172 1468.33 Q273.172 1461.2 271.343 1457.66 Q269.537 1454.1 265.903 1454.1 M265.903 1450.39 Q271.713 1450.39 274.769 1455 Q277.847 1459.58 277.847 1468.33 Q277.847 1477.06 274.769 1481.67 Q271.713 1486.25 265.903 1486.25 Q260.093 1486.25 257.014 1481.67 Q253.959 1477.06 253.959 1468.33 Q253.959 1459.58 257.014 1455 Q260.093 1450.39 265.903 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M578.269 1481.64 L585.908 1481.64 L585.908 1455.28 L577.598 1456.95 L577.598 1452.69 L585.862 1451.02 L590.538 1451.02 L590.538 1481.64 L598.176 1481.64 L598.176 1485.58 L578.269 1485.58 L578.269 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M617.621 1454.1 Q614.01 1454.1 612.181 1457.66 Q610.375 1461.2 610.375 1468.33 Q610.375 1475.44 612.181 1479.01 Q614.01 1482.55 617.621 1482.55 Q621.255 1482.55 623.061 1479.01 Q624.889 1475.44 624.889 1468.33 Q624.889 1461.2 623.061 1457.66 Q621.255 1454.1 617.621 1454.1 M617.621 1450.39 Q623.431 1450.39 626.487 1455 Q629.565 1459.58 629.565 1468.33 Q629.565 1477.06 626.487 1481.67 Q623.431 1486.25 617.621 1486.25 Q611.811 1486.25 608.732 1481.67 Q605.676 1477.06 605.676 1468.33 Q605.676 1459.58 608.732 1455 Q611.811 1450.39 617.621 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M920.033 1481.64 L936.353 1481.64 L936.353 1485.58 L914.408 1485.58 L914.408 1481.64 Q917.07 1478.89 921.654 1474.26 Q926.26 1469.61 927.441 1468.27 Q929.686 1465.74 930.566 1464.01 Q931.468 1462.25 931.468 1460.56 Q931.468 1457.8 929.524 1456.07 Q927.603 1454.33 924.501 1454.33 Q922.302 1454.33 919.848 1455.09 Q917.418 1455.86 914.64 1457.41 L914.64 1452.69 Q917.464 1451.55 919.918 1450.97 Q922.371 1450.39 924.408 1450.39 Q929.779 1450.39 932.973 1453.08 Q936.167 1455.77 936.167 1460.26 Q936.167 1462.39 935.357 1464.31 Q934.57 1466.2 932.464 1468.8 Q931.885 1469.47 928.783 1472.69 Q925.681 1475.88 920.033 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M956.167 1454.1 Q952.556 1454.1 950.728 1457.66 Q948.922 1461.2 948.922 1468.33 Q948.922 1475.44 950.728 1479.01 Q952.556 1482.55 956.167 1482.55 Q959.802 1482.55 961.607 1479.01 Q963.436 1475.44 963.436 1468.33 Q963.436 1461.2 961.607 1457.66 Q959.802 1454.1 956.167 1454.1 M956.167 1450.39 Q961.977 1450.39 965.033 1455 Q968.112 1459.58 968.112 1468.33 Q968.112 1477.06 965.033 1481.67 Q961.977 1486.25 956.167 1486.25 Q950.357 1486.25 947.278 1481.67 Q944.223 1477.06 944.223 1468.33 Q944.223 1459.58 947.278 1455 Q950.357 1450.39 956.167 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1267.78 1466.95 Q1271.14 1467.66 1273.01 1469.93 Q1274.91 1472.2 1274.91 1475.53 Q1274.91 1480.65 1271.39 1483.45 Q1267.87 1486.25 1261.39 1486.25 Q1259.22 1486.25 1256.9 1485.81 Q1254.61 1485.39 1252.16 1484.54 L1252.16 1480.02 Q1254.1 1481.16 1256.42 1481.74 Q1258.73 1482.32 1261.25 1482.32 Q1265.65 1482.32 1267.94 1480.58 Q1270.26 1478.84 1270.26 1475.53 Q1270.26 1472.48 1268.11 1470.77 Q1265.98 1469.03 1262.16 1469.03 L1258.13 1469.03 L1258.13 1465.19 L1262.34 1465.19 Q1265.79 1465.19 1267.62 1463.82 Q1269.45 1462.43 1269.45 1459.84 Q1269.45 1457.18 1267.55 1455.77 Q1265.67 1454.33 1262.16 1454.33 Q1260.23 1454.33 1258.04 1454.75 Q1255.84 1455.16 1253.2 1456.04 L1253.2 1451.88 Q1255.86 1451.14 1258.17 1450.77 Q1260.51 1450.39 1262.57 1450.39 Q1267.9 1450.39 1271 1452.83 Q1274.1 1455.23 1274.1 1459.35 Q1274.1 1462.22 1272.46 1464.21 Q1270.81 1466.18 1267.78 1466.95 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1293.78 1454.1 Q1290.17 1454.1 1288.34 1457.66 Q1286.53 1461.2 1286.53 1468.33 Q1286.53 1475.44 1288.34 1479.01 Q1290.17 1482.55 1293.78 1482.55 Q1297.41 1482.55 1299.22 1479.01 Q1301.04 1475.44 1301.04 1468.33 Q1301.04 1461.2 1299.22 1457.66 Q1297.41 1454.1 1293.78 1454.1 M1293.78 1450.39 Q1299.59 1450.39 1302.64 1455 Q1305.72 1459.58 1305.72 1468.33 Q1305.72 1477.06 1302.64 1481.67 Q1299.59 1486.25 1293.78 1486.25 Q1287.97 1486.25 1284.89 1481.67 Q1281.83 1477.06 1281.83 1468.33 Q1281.83 1459.58 1284.89 1455 Q1287.97 1450.39 1293.78 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1604.79 1455.09 L1592.98 1473.54 L1604.79 1473.54 L1604.79 1455.09 M1603.56 1451.02 L1609.44 1451.02 L1609.44 1473.54 L1614.37 1473.54 L1614.37 1477.43 L1609.44 1477.43 L1609.44 1485.58 L1604.79 1485.58 L1604.79 1477.43 L1589.19 1477.43 L1589.19 1472.92 L1603.56 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1632.1 1454.1 Q1628.49 1454.1 1626.66 1457.66 Q1624.86 1461.2 1624.86 1468.33 Q1624.86 1475.44 1626.66 1479.01 Q1628.49 1482.55 1632.1 1482.55 Q1635.74 1482.55 1637.54 1479.01 Q1639.37 1475.44 1639.37 1468.33 Q1639.37 1461.2 1637.54 1457.66 Q1635.74 1454.1 1632.1 1454.1 M1632.1 1450.39 Q1637.91 1450.39 1640.97 1455 Q1644.05 1459.58 1644.05 1468.33 Q1644.05 1477.06 1640.97 1481.67 Q1637.91 1486.25 1632.1 1486.25 Q1626.29 1486.25 1623.21 1481.67 Q1620.16 1477.06 1620.16 1468.33 Q1620.16 1459.58 1623.21 1455 Q1626.29 1450.39 1632.1 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1928.99 1451.02 L1947.35 1451.02 L1947.35 1454.96 L1933.28 1454.96 L1933.28 1463.43 Q1934.3 1463.08 1935.31 1462.92 Q1936.33 1462.73 1937.35 1462.73 Q1943.14 1462.73 1946.52 1465.9 Q1949.9 1469.08 1949.9 1474.49 Q1949.9 1480.07 1946.43 1483.17 Q1942.95 1486.25 1936.63 1486.25 Q1934.46 1486.25 1932.19 1485.88 Q1929.94 1485.51 1927.54 1484.77 L1927.54 1480.07 Q1929.62 1481.2 1931.84 1481.76 Q1934.06 1482.32 1936.54 1482.32 Q1940.55 1482.32 1942.88 1480.21 Q1945.22 1478.1 1945.22 1474.49 Q1945.22 1470.88 1942.88 1468.77 Q1940.55 1466.67 1936.54 1466.67 Q1934.67 1466.67 1932.79 1467.08 Q1930.94 1467.5 1928.99 1468.38 L1928.99 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1969.11 1454.1 Q1965.5 1454.1 1963.67 1457.66 Q1961.86 1461.2 1961.86 1468.33 Q1961.86 1475.44 1963.67 1479.01 Q1965.5 1482.55 1969.11 1482.55 Q1972.74 1482.55 1974.55 1479.01 Q1976.38 1475.44 1976.38 1468.33 Q1976.38 1461.2 1974.55 1457.66 Q1972.74 1454.1 1969.11 1454.1 M1969.11 1450.39 Q1974.92 1450.39 1977.98 1455 Q1981.05 1459.58 1981.05 1468.33 Q1981.05 1477.06 1977.98 1481.67 Q1974.92 1486.25 1969.11 1486.25 Q1963.3 1486.25 1960.22 1481.67 Q1957.17 1477.06 1957.17 1468.33 Q1957.17 1459.58 1960.22 1455 Q1963.3 1450.39 1969.11 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M2277.38 1466.44 Q2274.23 1466.44 2272.38 1468.59 Q2270.55 1470.74 2270.55 1474.49 Q2270.55 1478.22 2272.38 1480.39 Q2274.23 1482.55 2277.38 1482.55 Q2280.53 1482.55 2282.36 1480.39 Q2284.21 1478.22 2284.21 1474.49 Q2284.21 1470.74 2282.36 1468.59 Q2280.53 1466.44 2277.38 1466.44 M2286.66 1451.78 L2286.66 1456.04 Q2284.9 1455.21 2283.1 1454.77 Q2281.31 1454.33 2279.55 1454.33 Q2274.93 1454.33 2272.47 1457.45 Q2270.04 1460.58 2269.69 1466.9 Q2271.06 1464.89 2273.12 1463.82 Q2275.18 1462.73 2277.66 1462.73 Q2282.87 1462.73 2285.87 1465.9 Q2288.91 1469.05 2288.91 1474.49 Q2288.91 1479.82 2285.76 1483.03 Q2282.61 1486.25 2277.38 1486.25 Q2271.38 1486.25 2268.21 1481.67 Q2265.04 1477.06 2265.04 1468.33 Q2265.04 1460.14 2268.93 1455.28 Q2272.82 1450.39 2279.37 1450.39 Q2281.13 1450.39 2282.91 1450.74 Q2284.72 1451.09 2286.66 1451.78 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M2306.96 1454.1 Q2303.35 1454.1 2301.52 1457.66 Q2299.72 1461.2 2299.72 1468.33 Q2299.72 1475.44 2301.52 1479.01 Q2303.35 1482.55 2306.96 1482.55 Q2310.6 1482.55 2312.4 1479.01 Q2314.23 1475.44 2314.23 1468.33 Q2314.23 1461.2 2312.4 1457.66 Q2310.6 1454.1 2306.96 1454.1 M2306.96 1450.39 Q2312.77 1450.39 2315.83 1455 Q2318.91 1459.58 2318.91 1468.33 Q2318.91 1477.06 2315.83 1481.67 Q2312.77 1486.25 2306.96 1486.25 Q2301.15 1486.25 2298.07 1481.67 Q2295.02 1477.06 2295.02 1468.33 Q2295.02 1459.58 2298.07 1455 Q2301.15 1450.39 2306.96 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1243.74 1561.26 L1243.74 1548.5 L1233.23 1548.5 L1233.23 1543.22 L1250.1 1543.22 L1250.1 1563.62 Q1246.38 1566.26 1241.89 1567.63 Q1237.4 1568.97 1232.31 1568.97 Q1221.17 1568.97 1214.87 1562.47 Q1208.6 1555.95 1208.6 1544.33 Q1208.6 1532.68 1214.87 1526.19 Q1221.17 1519.66 1232.31 1519.66 Q1236.96 1519.66 1241.13 1520.81 Q1245.33 1521.96 1248.86 1524.18 L1248.86 1531.03 Q1245.3 1528 1241.29 1526.48 Q1237.27 1524.95 1232.85 1524.95 Q1224.13 1524.95 1219.74 1529.82 Q1215.38 1534.69 1215.38 1544.33 Q1215.38 1553.94 1219.74 1558.81 Q1224.13 1563.68 1232.85 1563.68 Q1236.26 1563.68 1238.93 1563.11 Q1241.6 1562.51 1243.74 1561.26 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1277.79 1550.12 Q1270.69 1550.12 1267.96 1551.75 Q1265.22 1553.37 1265.22 1557.29 Q1265.22 1560.4 1267.26 1562.25 Q1269.33 1564.07 1272.86 1564.07 Q1277.73 1564.07 1280.66 1560.63 Q1283.62 1557.16 1283.62 1551.43 L1283.62 1550.12 L1277.79 1550.12 M1289.47 1547.71 L1289.47 1568.04 L1283.62 1568.04 L1283.62 1562.63 Q1281.61 1565.88 1278.62 1567.44 Q1275.63 1568.97 1271.3 1568.97 Q1265.83 1568.97 1262.58 1565.91 Q1259.36 1562.82 1259.36 1557.67 Q1259.36 1551.65 1263.37 1548.6 Q1267.42 1545.54 1275.41 1545.54 L1283.62 1545.54 L1283.62 1544.97 Q1283.62 1540.93 1280.94 1538.73 Q1278.3 1536.5 1273.5 1536.5 Q1270.44 1536.5 1267.54 1537.23 Q1264.65 1537.97 1261.97 1539.43 L1261.97 1534.02 Q1265.19 1532.78 1268.21 1532.17 Q1271.24 1531.54 1274.1 1531.54 Q1281.83 1531.54 1285.65 1535.55 Q1289.47 1539.56 1289.47 1547.71 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1301.54 1532.4 L1307.39 1532.4 L1307.39 1568.04 L1301.54 1568.04 L1301.54 1532.4 M1301.54 1518.52 L1307.39 1518.52 L1307.39 1525.93 L1301.54 1525.93 L1301.54 1518.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1349.28 1546.53 L1349.28 1568.04 L1343.42 1568.04 L1343.42 1546.72 Q1343.42 1541.66 1341.45 1539.14 Q1339.48 1536.63 1335.53 1536.63 Q1330.79 1536.63 1328.05 1539.65 Q1325.31 1542.68 1325.31 1547.9 L1325.31 1568.04 L1319.42 1568.04 L1319.42 1532.4 L1325.31 1532.4 L1325.31 1537.93 Q1327.41 1534.72 1330.25 1533.13 Q1333.11 1531.54 1336.83 1531.54 Q1342.98 1531.54 1346.13 1535.36 Q1349.28 1539.14 1349.28 1546.53 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,1403.47 2352.76,1403.47 \"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,1190.14 2352.76,1190.14 \"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,976.803 2352.76,976.803 \"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,763.47 2352.76,763.47 \"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,550.137 2352.76,550.137 \"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,336.805 2352.76,336.805 \"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,123.472 2352.76,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip670)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 205.121,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip670)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1403.47 224.019,1403.47 \"/>\n",
       "<polyline clip-path=\"url(#clip670)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1190.14 224.019,1190.14 \"/>\n",
       "<polyline clip-path=\"url(#clip670)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,976.803 224.019,976.803 \"/>\n",
       "<polyline clip-path=\"url(#clip670)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,763.47 224.019,763.47 \"/>\n",
       "<polyline clip-path=\"url(#clip670)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,550.137 224.019,550.137 \"/>\n",
       "<polyline clip-path=\"url(#clip670)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,336.805 224.019,336.805 \"/>\n",
       "<polyline clip-path=\"url(#clip670)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,123.472 224.019,123.472 \"/>\n",
       "<path clip-path=\"url(#clip670)\" d=\"M157.177 1389.27 Q153.566 1389.27 151.737 1392.83 Q149.931 1396.37 149.931 1403.5 Q149.931 1410.61 151.737 1414.17 Q153.566 1417.72 157.177 1417.72 Q160.811 1417.72 162.616 1414.17 Q164.445 1410.61 164.445 1403.5 Q164.445 1396.37 162.616 1392.83 Q160.811 1389.27 157.177 1389.27 M157.177 1385.56 Q162.987 1385.56 166.042 1390.17 Q169.121 1394.75 169.121 1403.5 Q169.121 1412.23 166.042 1416.84 Q162.987 1421.42 157.177 1421.42 Q151.366 1421.42 148.288 1416.84 Q145.232 1412.23 145.232 1403.5 Q145.232 1394.75 148.288 1390.17 Q151.366 1385.56 157.177 1385.56 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M117.825 1203.48 L125.464 1203.48 L125.464 1177.11 L117.154 1178.78 L117.154 1174.52 L125.418 1172.86 L130.093 1172.86 L130.093 1203.48 L137.732 1203.48 L137.732 1207.42 L117.825 1207.42 L117.825 1203.48 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M157.177 1175.93 Q153.566 1175.93 151.737 1179.5 Q149.931 1183.04 149.931 1190.17 Q149.931 1197.28 151.737 1200.84 Q153.566 1204.38 157.177 1204.38 Q160.811 1204.38 162.616 1200.84 Q164.445 1197.28 164.445 1190.17 Q164.445 1183.04 162.616 1179.5 Q160.811 1175.93 157.177 1175.93 M157.177 1172.23 Q162.987 1172.23 166.042 1176.84 Q169.121 1181.42 169.121 1190.17 Q169.121 1198.9 166.042 1203.5 Q162.987 1208.09 157.177 1208.09 Q151.366 1208.09 148.288 1203.5 Q145.232 1198.9 145.232 1190.17 Q145.232 1181.42 148.288 1176.84 Q151.366 1172.23 157.177 1172.23 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M121.043 990.147 L137.362 990.147 L137.362 994.083 L115.418 994.083 L115.418 990.147 Q118.08 987.393 122.663 982.763 Q127.269 978.111 128.45 976.768 Q130.695 974.245 131.575 972.509 Q132.478 970.749 132.478 969.06 Q132.478 966.305 130.533 964.569 Q128.612 962.833 125.51 962.833 Q123.311 962.833 120.857 963.597 Q118.427 964.361 115.649 965.912 L115.649 961.189 Q118.473 960.055 120.927 959.476 Q123.38 958.898 125.418 958.898 Q130.788 958.898 133.982 961.583 Q137.177 964.268 137.177 968.759 Q137.177 970.888 136.367 972.81 Q135.579 974.708 133.473 977.3 Q132.894 977.972 129.792 981.189 Q126.691 984.384 121.043 990.147 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M157.177 962.601 Q153.566 962.601 151.737 966.166 Q149.931 969.708 149.931 976.837 Q149.931 983.944 151.737 987.509 Q153.566 991.05 157.177 991.05 Q160.811 991.05 162.616 987.509 Q164.445 983.944 164.445 976.837 Q164.445 969.708 162.616 966.166 Q160.811 962.601 157.177 962.601 M157.177 958.898 Q162.987 958.898 166.042 963.504 Q169.121 968.087 169.121 976.837 Q169.121 985.564 166.042 990.171 Q162.987 994.754 157.177 994.754 Q151.366 994.754 148.288 990.171 Q145.232 985.564 145.232 976.837 Q145.232 968.087 148.288 963.504 Q151.366 958.898 157.177 958.898 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M131.181 762.116 Q134.538 762.833 136.413 765.102 Q138.311 767.37 138.311 770.704 Q138.311 775.819 134.792 778.62 Q131.274 781.421 124.793 781.421 Q122.617 781.421 120.302 780.981 Q118.01 780.565 115.556 779.708 L115.556 775.194 Q117.501 776.329 119.816 776.907 Q122.13 777.486 124.654 777.486 Q129.052 777.486 131.343 775.75 Q133.658 774.014 133.658 770.704 Q133.658 767.648 131.505 765.935 Q129.376 764.199 125.556 764.199 L121.529 764.199 L121.529 760.357 L125.742 760.357 Q129.191 760.357 131.019 758.991 Q132.848 757.602 132.848 755.009 Q132.848 752.347 130.95 750.935 Q129.075 749.5 125.556 749.5 Q123.635 749.5 121.436 749.917 Q119.237 750.334 116.598 751.213 L116.598 747.046 Q119.26 746.306 121.575 745.935 Q123.913 745.565 125.973 745.565 Q131.297 745.565 134.399 747.996 Q137.501 750.403 137.501 754.523 Q137.501 757.394 135.857 759.384 Q134.214 761.352 131.181 762.116 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M157.177 749.269 Q153.566 749.269 151.737 752.833 Q149.931 756.375 149.931 763.505 Q149.931 770.611 151.737 774.176 Q153.566 777.718 157.177 777.718 Q160.811 777.718 162.616 774.176 Q164.445 770.611 164.445 763.505 Q164.445 756.375 162.616 752.833 Q160.811 749.269 157.177 749.269 M157.177 745.565 Q162.987 745.565 166.042 750.171 Q169.121 754.755 169.121 763.505 Q169.121 772.232 166.042 776.838 Q162.987 781.421 157.177 781.421 Q151.366 781.421 148.288 776.838 Q145.232 772.232 145.232 763.505 Q145.232 754.755 148.288 750.171 Q151.366 745.565 157.177 745.565 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M129.862 536.931 L118.056 555.38 L129.862 555.38 L129.862 536.931 M128.635 532.857 L134.515 532.857 L134.515 555.38 L139.445 555.38 L139.445 559.269 L134.515 559.269 L134.515 567.417 L129.862 567.417 L129.862 559.269 L114.26 559.269 L114.26 554.755 L128.635 532.857 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M157.177 535.936 Q153.566 535.936 151.737 539.501 Q149.931 543.043 149.931 550.172 Q149.931 557.279 151.737 560.843 Q153.566 564.385 157.177 564.385 Q160.811 564.385 162.616 560.843 Q164.445 557.279 164.445 550.172 Q164.445 543.043 162.616 539.501 Q160.811 535.936 157.177 535.936 M157.177 532.232 Q162.987 532.232 166.042 536.839 Q169.121 541.422 169.121 550.172 Q169.121 558.899 166.042 563.505 Q162.987 568.089 157.177 568.089 Q151.366 568.089 148.288 563.505 Q145.232 558.899 145.232 550.172 Q145.232 541.422 148.288 536.839 Q151.366 532.232 157.177 532.232 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M117.061 319.525 L135.417 319.525 L135.417 323.46 L121.343 323.46 L121.343 331.932 Q122.362 331.585 123.38 331.423 Q124.399 331.238 125.418 331.238 Q131.205 331.238 134.584 334.409 Q137.964 337.58 137.964 342.997 Q137.964 348.576 134.492 351.677 Q131.019 354.756 124.7 354.756 Q122.524 354.756 120.255 354.386 Q118.01 354.015 115.603 353.275 L115.603 348.576 Q117.686 349.71 119.908 350.265 Q122.13 350.821 124.607 350.821 Q128.612 350.821 130.95 348.714 Q133.288 346.608 133.288 342.997 Q133.288 339.386 130.95 337.279 Q128.612 335.173 124.607 335.173 Q122.732 335.173 120.857 335.589 Q119.006 336.006 117.061 336.886 L117.061 319.525 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M157.177 322.603 Q153.566 322.603 151.737 326.168 Q149.931 329.71 149.931 336.839 Q149.931 343.946 151.737 347.511 Q153.566 351.052 157.177 351.052 Q160.811 351.052 162.616 347.511 Q164.445 343.946 164.445 336.839 Q164.445 329.71 162.616 326.168 Q160.811 322.603 157.177 322.603 M157.177 318.9 Q162.987 318.9 166.042 323.506 Q169.121 328.09 169.121 336.839 Q169.121 345.566 166.042 350.173 Q162.987 354.756 157.177 354.756 Q151.366 354.756 148.288 350.173 Q145.232 345.566 145.232 336.839 Q145.232 328.09 148.288 323.506 Q151.366 318.9 157.177 318.9 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M127.593 121.609 Q124.445 121.609 122.593 123.761 Q120.765 125.914 120.765 129.664 Q120.765 133.391 122.593 135.567 Q124.445 137.72 127.593 137.72 Q130.742 137.72 132.57 135.567 Q134.422 133.391 134.422 129.664 Q134.422 125.914 132.57 123.761 Q130.742 121.609 127.593 121.609 M136.876 106.956 L136.876 111.215 Q135.117 110.382 133.311 109.942 Q131.529 109.502 129.769 109.502 Q125.14 109.502 122.686 112.627 Q120.255 115.752 119.908 122.072 Q121.274 120.058 123.334 118.993 Q125.394 117.905 127.871 117.905 Q133.08 117.905 136.089 121.076 Q139.121 124.224 139.121 129.664 Q139.121 134.988 135.973 138.206 Q132.825 141.423 127.593 141.423 Q121.598 141.423 118.427 136.84 Q115.256 132.234 115.256 123.507 Q115.256 115.312 119.144 110.451 Q123.033 105.567 129.584 105.567 Q131.343 105.567 133.126 105.914 Q134.931 106.262 136.876 106.956 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M157.177 109.271 Q153.566 109.271 151.737 112.836 Q149.931 116.377 149.931 123.507 Q149.931 130.613 151.737 134.178 Q153.566 137.72 157.177 137.72 Q160.811 137.72 162.616 134.178 Q164.445 130.613 164.445 123.507 Q164.445 116.377 162.616 112.836 Q160.811 109.271 157.177 109.271 M157.177 105.567 Q162.987 105.567 166.042 110.174 Q169.121 114.757 169.121 123.507 Q169.121 132.234 166.042 136.84 Q162.987 141.423 157.177 141.423 Q151.366 141.423 148.288 136.84 Q145.232 132.234 145.232 123.507 Q145.232 114.757 148.288 110.174 Q151.366 105.567 157.177 105.567 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M16.4842 891.553 L16.4842 864.244 L21.895 864.244 L21.895 885.124 L35.8996 885.124 L35.8996 866.281 L41.3104 866.281 L41.3104 885.124 L64.0042 885.124 L64.0042 891.553 L16.4842 891.553 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M44.7161 827.387 L47.5806 827.387 L47.5806 854.314 Q53.6281 853.932 56.8109 850.685 Q59.9619 847.407 59.9619 841.582 Q59.9619 838.208 59.1344 835.057 Q58.3069 831.875 56.6518 828.755 L62.1899 828.755 Q63.5267 831.906 64.227 835.217 Q64.9272 838.527 64.9272 841.932 Q64.9272 850.462 59.9619 855.46 Q54.9967 860.425 46.5303 860.425 Q37.7774 860.425 32.6531 855.714 Q27.4968 850.972 27.4968 842.951 Q27.4968 835.758 32.1438 831.588 Q36.7589 827.387 44.7161 827.387 M42.9973 833.243 Q38.1912 833.307 35.3266 835.949 Q32.4621 838.559 32.4621 842.887 Q32.4621 847.789 35.2312 850.749 Q38.0002 853.677 43.0292 854.123 L42.9973 833.243 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M46.0847 801.574 Q46.0847 808.672 47.7079 811.409 Q49.3312 814.146 53.2461 814.146 Q56.3653 814.146 58.2114 812.109 Q60.0256 810.04 60.0256 806.507 Q60.0256 801.637 56.5881 798.709 Q53.1188 795.749 47.3897 795.749 L46.0847 795.749 L46.0847 801.574 M43.6657 789.893 L64.0042 789.893 L64.0042 795.749 L58.5933 795.749 Q61.8398 797.754 63.3994 800.746 Q64.9272 803.738 64.9272 808.067 Q64.9272 813.541 61.8716 816.788 Q58.7843 820.003 53.6281 820.003 Q47.6125 820.003 44.5569 815.992 Q41.5014 811.95 41.5014 803.961 L41.5014 795.749 L40.9285 795.749 Q36.8862 795.749 34.6901 798.423 Q32.4621 801.065 32.4621 805.871 Q32.4621 808.926 33.1941 811.823 Q33.9262 814.719 35.3903 817.393 L29.9795 817.393 Q28.7381 814.178 28.1334 811.154 Q27.4968 808.13 27.4968 805.266 Q27.4968 797.532 31.5072 793.712 Q35.5176 789.893 43.6657 789.893 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M18.2347 772.037 L28.3562 772.037 L28.3562 759.974 L32.9077 759.974 L32.9077 772.037 L52.2594 772.037 Q56.6199 772.037 57.8613 770.859 Q59.1026 769.65 59.1026 765.99 L59.1026 759.974 L64.0042 759.974 L64.0042 765.99 Q64.0042 772.769 61.4897 775.347 Q58.9434 777.925 52.2594 777.925 L32.9077 777.925 L32.9077 782.222 L28.3562 782.222 L28.3562 777.925 L18.2347 777.925 L18.2347 772.037 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M49.9359 752.876 L28.3562 752.876 L28.3562 747.02 L49.7131 747.02 Q54.7739 747.02 57.3202 745.046 Q59.8346 743.073 59.8346 739.126 Q59.8346 734.384 56.8109 731.647 Q53.7872 728.877 48.5673 728.877 L28.3562 728.877 L28.3562 723.021 L64.0042 723.021 L64.0042 728.877 L58.5296 728.877 Q61.7762 731.01 63.3676 733.843 Q64.9272 736.644 64.9272 740.368 Q64.9272 746.51 61.1078 749.693 Q57.2883 752.876 49.9359 752.876 M27.4968 738.14 L27.4968 738.14 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M33.8307 690.301 Q33.2578 691.288 33.0032 692.466 Q32.7167 693.611 32.7167 695.012 Q32.7167 699.977 35.9632 702.651 Q39.1779 705.292 45.2253 705.292 L64.0042 705.292 L64.0042 711.181 L28.3562 711.181 L28.3562 705.292 L33.8944 705.292 Q30.6479 703.446 29.0883 700.486 Q27.4968 697.526 27.4968 693.293 Q27.4968 692.688 27.5923 691.956 Q27.656 691.224 27.8151 690.333 L33.8307 690.301 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M44.7161 655.099 L47.5806 655.099 L47.5806 682.026 Q53.6281 681.644 56.8109 678.397 Q59.9619 675.119 59.9619 669.294 Q59.9619 665.921 59.1344 662.77 Q58.3069 659.587 56.6518 656.468 L62.1899 656.468 Q63.5267 659.619 64.227 662.929 Q64.9272 666.239 64.9272 669.645 Q64.9272 678.175 59.9619 683.172 Q54.9967 688.137 46.5303 688.137 Q37.7774 688.137 32.6531 683.426 Q27.4968 678.684 27.4968 670.663 Q27.4968 663.47 32.1438 659.3 Q36.7589 655.099 44.7161 655.099 M42.9973 660.955 Q38.1912 661.019 35.3266 663.661 Q32.4621 666.271 32.4621 670.599 Q32.4621 675.501 35.2312 678.461 Q38.0002 681.389 43.0292 681.835 L42.9973 660.955 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M877.575 12.096 L912.332 12.096 L912.332 18.9825 L885.758 18.9825 L885.758 36.8065 L909.739 36.8065 L909.739 43.6931 L885.758 43.6931 L885.758 72.576 L877.575 72.576 L877.575 12.096 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M959.241 48.0275 L959.241 51.6733 L924.97 51.6733 Q925.457 59.3701 929.588 63.421 Q933.761 67.4314 941.174 67.4314 Q945.468 67.4314 949.478 66.3781 Q953.529 65.3249 957.499 63.2184 L957.499 70.267 Q953.489 71.9684 949.276 72.8596 Q945.063 73.7508 940.728 73.7508 Q929.872 73.7508 923.512 67.4314 Q917.193 61.1119 917.193 50.3365 Q917.193 39.1965 923.188 32.6746 Q929.224 26.1121 939.432 26.1121 Q948.587 26.1121 953.894 32.0264 Q959.241 37.9003 959.241 48.0275 M951.787 45.84 Q951.706 39.7232 948.344 36.0774 Q945.022 32.4315 939.513 32.4315 Q933.275 32.4315 929.507 35.9558 Q925.781 39.4801 925.213 45.8805 L951.787 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M992.094 49.7694 Q983.06 49.7694 979.577 51.8354 Q976.093 53.9013 976.093 58.8839 Q976.093 62.8538 978.685 65.2034 Q981.319 67.5124 985.815 67.5124 Q992.013 67.5124 995.74 63.1374 Q999.507 58.7219 999.507 51.4303 L999.507 49.7694 L992.094 49.7694 M1006.96 46.6907 L1006.96 72.576 L999.507 72.576 L999.507 65.6895 Q996.955 69.8214 993.147 71.8063 Q989.339 73.7508 983.83 73.7508 Q976.863 73.7508 972.731 69.8619 Q968.639 65.9325 968.639 59.3701 Q968.639 51.7138 973.743 47.825 Q978.888 43.9361 989.056 43.9361 L999.507 43.9361 L999.507 43.2069 Q999.507 38.0623 996.104 35.2672 Q992.742 32.4315 986.625 32.4315 Q982.736 32.4315 979.05 33.3632 Q975.364 34.295 971.961 36.1584 L971.961 29.2718 Q976.052 27.692 979.901 26.9223 Q983.749 26.1121 987.395 26.1121 Q997.239 26.1121 1002.1 31.2163 Q1006.96 36.3204 1006.96 46.6907 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1029.69 14.324 L1029.69 27.2059 L1045.04 27.2059 L1045.04 32.9987 L1029.69 32.9987 L1029.69 57.6282 Q1029.69 63.1779 1031.19 64.7578 Q1032.72 66.3376 1037.38 66.3376 L1045.04 66.3376 L1045.04 72.576 L1037.38 72.576 Q1028.75 72.576 1025.47 69.3758 Q1022.19 66.1351 1022.19 57.6282 L1022.19 32.9987 L1016.72 32.9987 L1016.72 27.2059 L1022.19 27.2059 L1022.19 14.324 L1029.69 14.324 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1054.07 54.671 L1054.07 27.2059 L1061.53 27.2059 L1061.53 54.3874 Q1061.53 60.8284 1064.04 64.0691 Q1066.55 67.2693 1071.57 67.2693 Q1077.61 67.2693 1081.09 63.421 Q1084.62 59.5726 1084.62 52.9291 L1084.62 27.2059 L1092.07 27.2059 L1092.07 72.576 L1084.62 72.576 L1084.62 65.6084 Q1081.9 69.7404 1078.3 71.7658 Q1074.73 73.7508 1069.99 73.7508 Q1062.17 73.7508 1058.12 68.8897 Q1054.07 64.0286 1054.07 54.671 M1072.83 26.1121 L1072.83 26.1121 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1133.71 34.1734 Q1132.46 33.4443 1130.96 33.1202 Q1129.5 32.7556 1127.72 32.7556 Q1121.4 32.7556 1118 36.8875 Q1114.63 40.9789 1114.63 48.6757 L1114.63 72.576 L1107.14 72.576 L1107.14 27.2059 L1114.63 27.2059 L1114.63 34.2544 Q1116.98 30.1225 1120.75 28.1376 Q1124.52 26.1121 1129.91 26.1121 Q1130.68 26.1121 1131.61 26.2337 Q1132.54 26.3147 1133.67 26.5172 L1133.71 34.1734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1178.52 48.0275 L1178.52 51.6733 L1144.25 51.6733 Q1144.73 59.3701 1148.86 63.421 Q1153.04 67.4314 1160.45 67.4314 Q1164.74 67.4314 1168.75 66.3781 Q1172.8 65.3249 1176.77 63.2184 L1176.77 70.267 Q1172.76 71.9684 1168.55 72.8596 Q1164.34 73.7508 1160 73.7508 Q1149.15 73.7508 1142.79 67.4314 Q1136.47 61.1119 1136.47 50.3365 Q1136.47 39.1965 1142.46 32.6746 Q1148.5 26.1121 1158.71 26.1121 Q1167.86 26.1121 1173.17 32.0264 Q1178.52 37.9003 1178.52 48.0275 M1171.06 45.84 Q1170.98 39.7232 1167.62 36.0774 Q1164.3 32.4315 1158.79 32.4315 Q1152.55 32.4315 1148.78 35.9558 Q1145.06 39.4801 1144.49 45.8805 L1171.06 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1217.45 12.096 L1225.63 12.096 L1225.63 72.576 L1217.45 72.576 L1217.45 12.096 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1276.91 35.9153 Q1279.71 30.8922 1283.6 28.5022 Q1287.49 26.1121 1292.75 26.1121 Q1299.84 26.1121 1303.69 31.0947 Q1307.54 36.0368 1307.54 45.1919 L1307.54 72.576 L1300.04 72.576 L1300.04 45.4349 Q1300.04 38.913 1297.73 35.7533 Q1295.43 32.5936 1290.69 32.5936 Q1284.89 32.5936 1281.53 36.4419 Q1278.17 40.2903 1278.17 46.9338 L1278.17 72.576 L1270.67 72.576 L1270.67 45.4349 Q1270.67 38.8725 1268.37 35.7533 Q1266.06 32.5936 1261.24 32.5936 Q1255.52 32.5936 1252.16 36.4824 Q1248.8 40.3308 1248.8 46.9338 L1248.8 72.576 L1241.31 72.576 L1241.31 27.2059 L1248.8 27.2059 L1248.8 34.2544 Q1251.35 30.082 1254.92 28.0971 Q1258.48 26.1121 1263.38 26.1121 Q1268.33 26.1121 1271.77 28.6237 Q1275.25 31.1352 1276.91 35.9153 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1329.62 65.7705 L1329.62 89.8329 L1322.12 89.8329 L1322.12 27.2059 L1329.62 27.2059 L1329.62 34.0924 Q1331.96 30.0415 1335.53 28.0971 Q1339.13 26.1121 1344.12 26.1121 Q1352.38 26.1121 1357.53 32.6746 Q1362.71 39.2371 1362.71 49.9314 Q1362.71 60.6258 1357.53 67.1883 Q1352.38 73.7508 1344.12 73.7508 Q1339.13 73.7508 1335.53 71.8063 Q1331.96 69.8214 1329.62 65.7705 M1354.97 49.9314 Q1354.97 41.7081 1351.57 37.0496 Q1348.21 32.3505 1342.29 32.3505 Q1336.38 32.3505 1332.98 37.0496 Q1329.62 41.7081 1329.62 49.9314 Q1329.62 58.1548 1332.98 62.8538 Q1336.38 67.5124 1342.29 67.5124 Q1348.21 67.5124 1351.57 62.8538 Q1354.97 58.1548 1354.97 49.9314 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1392.65 32.4315 Q1386.65 32.4315 1383.17 37.1306 Q1379.68 41.7891 1379.68 49.9314 Q1379.68 58.0738 1383.13 62.7728 Q1386.61 67.4314 1392.65 67.4314 Q1398.6 67.4314 1402.09 62.7323 Q1405.57 58.0333 1405.57 49.9314 Q1405.57 41.8701 1402.09 37.1711 Q1398.6 32.4315 1392.65 32.4315 M1392.65 26.1121 Q1402.37 26.1121 1407.92 32.4315 Q1413.47 38.7509 1413.47 49.9314 Q1413.47 61.0714 1407.92 67.4314 Q1402.37 73.7508 1392.65 73.7508 Q1382.88 73.7508 1377.33 67.4314 Q1371.83 61.0714 1371.83 49.9314 Q1371.83 38.7509 1377.33 32.4315 Q1382.88 26.1121 1392.65 26.1121 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1452.11 34.1734 Q1450.86 33.4443 1449.36 33.1202 Q1447.9 32.7556 1446.12 32.7556 Q1439.8 32.7556 1436.4 36.8875 Q1433.03 40.9789 1433.03 48.6757 L1433.03 72.576 L1425.54 72.576 L1425.54 27.2059 L1433.03 27.2059 L1433.03 34.2544 Q1435.38 30.1225 1439.15 28.1376 Q1442.92 26.1121 1448.31 26.1121 Q1449.08 26.1121 1450.01 26.2337 Q1450.94 26.3147 1452.07 26.5172 L1452.11 34.1734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1467.31 14.324 L1467.31 27.2059 L1482.66 27.2059 L1482.66 32.9987 L1467.31 32.9987 L1467.31 57.6282 Q1467.31 63.1779 1468.8 64.7578 Q1470.34 66.3376 1475 66.3376 L1482.66 66.3376 L1482.66 72.576 L1475 72.576 Q1466.37 72.576 1463.09 69.3758 Q1459.81 66.1351 1459.81 57.6282 L1459.81 32.9987 L1454.34 32.9987 L1454.34 27.2059 L1459.81 27.2059 L1459.81 14.324 L1467.31 14.324 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1513.08 49.7694 Q1504.05 49.7694 1500.56 51.8354 Q1497.08 53.9013 1497.08 58.8839 Q1497.08 62.8538 1499.67 65.2034 Q1502.31 67.5124 1506.8 67.5124 Q1513 67.5124 1516.73 63.1374 Q1520.49 58.7219 1520.49 51.4303 L1520.49 49.7694 L1513.08 49.7694 M1527.95 46.6907 L1527.95 72.576 L1520.49 72.576 L1520.49 65.6895 Q1517.94 69.8214 1514.13 71.8063 Q1510.33 73.7508 1504.82 73.7508 Q1497.85 73.7508 1493.72 69.8619 Q1489.63 65.9325 1489.63 59.3701 Q1489.63 51.7138 1494.73 47.825 Q1499.87 43.9361 1510.04 43.9361 L1520.49 43.9361 L1520.49 43.2069 Q1520.49 38.0623 1517.09 35.2672 Q1513.73 32.4315 1507.61 32.4315 Q1503.72 32.4315 1500.04 33.3632 Q1496.35 34.295 1492.95 36.1584 L1492.95 29.2718 Q1497.04 27.692 1500.89 26.9223 Q1504.74 26.1121 1508.38 26.1121 Q1518.23 26.1121 1523.09 31.2163 Q1527.95 36.3204 1527.95 46.6907 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1581.01 45.1919 L1581.01 72.576 L1573.56 72.576 L1573.56 45.4349 Q1573.56 38.994 1571.05 35.7938 Q1568.54 32.5936 1563.51 32.5936 Q1557.48 32.5936 1553.99 36.4419 Q1550.51 40.2903 1550.51 46.9338 L1550.51 72.576 L1543.02 72.576 L1543.02 27.2059 L1550.51 27.2059 L1550.51 34.2544 Q1553.18 30.163 1556.79 28.1376 Q1560.44 26.1121 1565.18 26.1121 Q1572.99 26.1121 1577 30.9732 Q1581.01 35.7938 1581.01 45.1919 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1628.53 28.9478 L1628.53 35.9153 Q1625.37 34.1734 1622.17 33.3227 Q1619.01 32.4315 1615.77 32.4315 Q1608.52 32.4315 1604.51 37.0496 Q1600.5 41.6271 1600.5 49.9314 Q1600.5 58.2358 1604.51 62.8538 Q1608.52 67.4314 1615.77 67.4314 Q1619.01 67.4314 1622.17 66.5807 Q1625.37 65.6895 1628.53 63.9476 L1628.53 70.8341 Q1625.41 72.2924 1622.05 73.0216 Q1618.73 73.7508 1614.96 73.7508 Q1604.71 73.7508 1598.68 67.3098 Q1592.64 60.8689 1592.64 49.9314 Q1592.64 38.832 1598.72 32.472 Q1604.83 26.1121 1615.45 26.1121 Q1618.89 26.1121 1622.17 26.8413 Q1625.45 27.5299 1628.53 28.9478 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip670)\" d=\"M1680.3 48.0275 L1680.3 51.6733 L1646.03 51.6733 Q1646.52 59.3701 1650.65 63.421 Q1654.82 67.4314 1662.23 67.4314 Q1666.53 67.4314 1670.54 66.3781 Q1674.59 65.3249 1678.56 63.2184 L1678.56 70.267 Q1674.55 71.9684 1670.34 72.8596 Q1666.12 73.7508 1661.79 73.7508 Q1650.93 73.7508 1644.57 67.4314 Q1638.25 61.1119 1638.25 50.3365 Q1638.25 39.1965 1644.25 32.6746 Q1650.28 26.1121 1660.49 26.1121 Q1669.65 26.1121 1674.95 32.0264 Q1680.3 37.9003 1680.3 48.0275 M1672.85 45.84 Q1672.77 39.7232 1669.41 36.0774 Q1666.08 32.4315 1660.57 32.4315 Q1654.34 32.4315 1650.57 35.9558 Q1646.84 39.4801 1646.27 45.8805 L1672.85 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip672)\" d=\"M637.349 1390.67 L265.903 1390.67 L265.903 1373.6 L637.349 1373.6 L637.349 1390.67 L637.349 1390.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"637.349,1390.67 265.903,1390.67 265.903,1373.6 637.349,1373.6 637.349,1390.67 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M806.189 1369.33 L265.903 1369.33 L265.903 1352.27 L806.189 1352.27 L806.189 1369.33 L806.189 1369.33  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"806.189,1369.33 265.903,1369.33 265.903,1352.27 806.189,1352.27 806.189,1369.33 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1414.01 1348 L265.903 1348 L265.903 1330.93 L1414.01 1330.93 L1414.01 1348 L1414.01 1348  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1414.01,1348 265.903,1348 265.903,1330.93 1414.01,1330.93 1414.01,1348 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1312.71 1326.67 L265.903 1326.67 L265.903 1309.6 L1312.71 1309.6 L1312.71 1326.67 L1312.71 1326.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1312.71,1326.67 265.903,1326.67 265.903,1309.6 1312.71,1309.6 1312.71,1326.67 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M2021.83 1305.33 L265.903 1305.33 L265.903 1288.27 L2021.83 1288.27 L2021.83 1305.33 L2021.83 1305.33  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2021.83,1305.33 265.903,1305.33 265.903,1288.27 2021.83,1288.27 2021.83,1305.33 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1481.55 1284 L265.903 1284 L265.903 1266.94 L1481.55 1266.94 L1481.55 1284 L1481.55 1284  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1481.55,1284 265.903,1284 265.903,1266.94 1481.55,1266.94 1481.55,1284 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M772.421 1262.67 L265.903 1262.67 L265.903 1245.6 L772.421 1245.6 L772.421 1262.67 L772.421 1262.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"772.421,1262.67 265.903,1262.67 265.903,1245.6 772.421,1245.6 772.421,1262.67 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M400.974 1241.34 L265.903 1241.34 L265.903 1224.27 L400.974 1224.27 L400.974 1241.34 L400.974 1241.34  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"400.974,1241.34 265.903,1241.34 265.903,1224.27 400.974,1224.27 400.974,1241.34 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1785.46 1220 L265.903 1220 L265.903 1202.94 L1785.46 1202.94 L1785.46 1220 L1785.46 1220  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1785.46,1220 265.903,1220 265.903,1202.94 1785.46,1202.94 1785.46,1220 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1177.63 1198.67 L265.903 1198.67 L265.903 1181.6 L1177.63 1181.6 L1177.63 1198.67 L1177.63 1198.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1177.63,1198.67 265.903,1198.67 265.903,1181.6 1177.63,1181.6 1177.63,1198.67 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M873.724 1177.34 L265.903 1177.34 L265.903 1160.27 L873.724 1160.27 L873.724 1177.34 L873.724 1177.34  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"873.724,1177.34 265.903,1177.34 265.903,1160.27 873.724,1160.27 873.724,1177.34 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M569.814 1156 L265.903 1156 L265.903 1138.94 L569.814 1138.94 L569.814 1156 L569.814 1156  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"569.814,1156 265.903,1156 265.903,1138.94 569.814,1138.94 569.814,1156 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1042.56 1134.67 L265.903 1134.67 L265.903 1117.6 L1042.56 1117.6 L1042.56 1134.67 L1042.56 1134.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1042.56,1134.67 265.903,1134.67 265.903,1117.6 1042.56,1117.6 1042.56,1134.67 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M2123.13 1113.34 L265.903 1113.34 L265.903 1096.27 L2123.13 1096.27 L2123.13 1113.34 L2123.13 1113.34  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2123.13,1113.34 265.903,1113.34 265.903,1096.27 2123.13,1096.27 2123.13,1113.34 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1211.4 1092 L265.903 1092 L265.903 1074.94 L1211.4 1074.94 L1211.4 1092 L1211.4 1092  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1211.4,1092 265.903,1092 265.903,1074.94 1211.4,1074.94 1211.4,1092 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1616.62 1070.67 L265.903 1070.67 L265.903 1053.6 L1616.62 1053.6 L1616.62 1070.67 L1616.62 1070.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1616.62,1070.67 265.903,1070.67 265.903,1053.6 1616.62,1053.6 1616.62,1070.67 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1717.92 1049.34 L265.903 1049.34 L265.903 1032.27 L1717.92 1032.27 L1717.92 1049.34 L1717.92 1049.34  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1717.92,1049.34 265.903,1049.34 265.903,1032.27 1717.92,1032.27 1717.92,1049.34 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1886.76 1028 L265.903 1028 L265.903 1010.94 L1886.76 1010.94 L1886.76 1028 L1886.76 1028  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1886.76,1028 265.903,1028 265.903,1010.94 1886.76,1010.94 1886.76,1028 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1852.99 1006.67 L265.903 1006.67 L265.903 989.603 L1852.99 989.603 L1852.99 1006.67 L1852.99 1006.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1852.99,1006.67 265.903,1006.67 265.903,989.603 1852.99,989.603 1852.99,1006.67 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1751.69 985.336 L265.903 985.336 L265.903 968.269 L1751.69 968.269 L1751.69 985.336 L1751.69 985.336  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1751.69,985.336 265.903,985.336 265.903,968.269 1751.69,968.269 1751.69,985.336 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1988.06 964.003 L265.903 964.003 L265.903 946.936 L1988.06 946.936 L1988.06 964.003 L1988.06 964.003  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1988.06,964.003 265.903,964.003 265.903,946.936 1988.06,946.936 1988.06,964.003 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1515.31 942.669 L265.903 942.669 L265.903 925.603 L1515.31 925.603 L1515.31 942.669 L1515.31 942.669  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1515.31,942.669 265.903,942.669 265.903,925.603 1515.31,925.603 1515.31,942.669 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M299.671 921.336 L265.903 921.336 L265.903 904.27 L299.671 904.27 L299.671 921.336 L299.671 921.336  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"299.671,921.336 265.903,921.336 265.903,904.27 299.671,904.27 299.671,921.336 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M536.046 900.003 L265.903 900.003 L265.903 882.936 L536.046 882.936 L536.046 900.003 L536.046 900.003  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"536.046,900.003 265.903,900.003 265.903,882.936 536.046,882.936 536.046,900.003 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M367.207 878.67 L265.903 878.67 L265.903 861.603 L367.207 861.603 L367.207 878.67 L367.207 878.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"367.207,878.67 265.903,878.67 265.903,861.603 367.207,861.603 367.207,878.67 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M333.439 857.336 L265.903 857.336 L265.903 840.27 L333.439 840.27 L333.439 857.336 L333.439 857.336  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"333.439,857.336 265.903,857.336 265.903,840.27 333.439,840.27 333.439,857.336 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M975.028 836.003 L265.903 836.003 L265.903 818.937 L975.028 818.937 L975.028 836.003 L975.028 836.003  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"975.028,836.003 265.903,836.003 265.903,818.937 975.028,818.937 975.028,836.003 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M941.26 814.67 L265.903 814.67 L265.903 797.603 L941.26 797.603 L941.26 814.67 L941.26 814.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"941.26,814.67 265.903,814.67 265.903,797.603 941.26,797.603 941.26,814.67 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1549.08 793.337 L265.903 793.337 L265.903 776.27 L1549.08 776.27 L1549.08 793.337 L1549.08 793.337  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1549.08,793.337 265.903,793.337 265.903,776.27 1549.08,776.27 1549.08,793.337 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M839.956 772.003 L265.903 772.003 L265.903 754.937 L839.956 754.937 L839.956 772.003 L839.956 772.003  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"839.956,772.003 265.903,772.003 265.903,754.937 839.956,754.937 839.956,772.003 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M434.742 750.67 L265.903 750.67 L265.903 733.603 L434.742 733.603 L434.742 750.67 L434.742 750.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"434.742,750.67 265.903,750.67 265.903,733.603 434.742,733.603 434.742,750.67 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M502.278 729.337 L265.903 729.337 L265.903 712.27 L502.278 712.27 L502.278 729.337 L502.278 729.337  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"502.278,729.337 265.903,729.337 265.903,712.27 502.278,712.27 502.278,729.337 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M468.51 708.004 L265.903 708.004 L265.903 690.937 L468.51 690.937 L468.51 708.004 L468.51 708.004  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"468.51,708.004 265.903,708.004 265.903,690.937 468.51,690.937 468.51,708.004 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M603.582 686.67 L265.903 686.67 L265.903 669.604 L603.582 669.604 L603.582 686.67 L603.582 686.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"603.582,686.67 265.903,686.67 265.903,669.604 603.582,669.604 603.582,686.67 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1278.94 665.337 L265.903 665.337 L265.903 648.27 L1278.94 648.27 L1278.94 665.337 L1278.94 665.337  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1278.94,665.337 265.903,665.337 265.903,648.27 1278.94,648.27 1278.94,665.337 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M671.117 644.004 L265.903 644.004 L265.903 626.937 L671.117 626.937 L671.117 644.004 L671.117 644.004  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"671.117,644.004 265.903,644.004 265.903,626.937 671.117,626.937 671.117,644.004 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1684.15 622.67 L265.903 622.67 L265.903 605.604 L1684.15 605.604 L1684.15 622.67 L1684.15 622.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1684.15,622.67 265.903,622.67 265.903,605.604 1684.15,605.604 1684.15,622.67 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1143.87 601.337 L265.903 601.337 L265.903 584.271 L1143.87 584.271 L1143.87 601.337 L1143.87 601.337  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1143.87,601.337 265.903,601.337 265.903,584.271 1143.87,584.271 1143.87,601.337 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1380.24 580.004 L265.903 580.004 L265.903 562.937 L1380.24 562.937 L1380.24 580.004 L1380.24 580.004  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1380.24,580.004 265.903,580.004 265.903,562.937 1380.24,562.937 1380.24,580.004 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1819.22 558.671 L265.903 558.671 L265.903 541.604 L1819.22 541.604 L1819.22 558.671 L1819.22 558.671  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1819.22,558.671 265.903,558.671 265.903,541.604 1819.22,541.604 1819.22,558.671 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1110.1 537.337 L265.903 537.337 L265.903 520.271 L1110.1 520.271 L1110.1 537.337 L1110.1 537.337  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1110.1,537.337 265.903,537.337 265.903,520.271 1110.1,520.271 1110.1,537.337 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M2089.37 516.004 L265.903 516.004 L265.903 498.938 L2089.37 498.938 L2089.37 516.004 L2089.37 516.004  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2089.37,516.004 265.903,516.004 265.903,498.938 2089.37,498.938 2089.37,516.004 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1346.47 494.671 L265.903 494.671 L265.903 477.604 L1346.47 477.604 L1346.47 494.671 L1346.47 494.671  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1346.47,494.671 265.903,494.671 265.903,477.604 1346.47,477.604 1346.47,494.671 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M2190.67 473.338 L265.903 473.338 L265.903 456.271 L2190.67 456.271 L2190.67 473.338 L2190.67 473.338  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2190.67,473.338 265.903,473.338 265.903,456.271 2190.67,456.271 2190.67,473.338 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1076.33 452.004 L265.903 452.004 L265.903 434.938 L1076.33 434.938 L1076.33 452.004 L1076.33 452.004  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1076.33,452.004 265.903,452.004 265.903,434.938 1076.33,434.938 1076.33,452.004 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M907.492 430.671 L265.903 430.671 L265.903 413.605 L907.492 413.605 L907.492 430.671 L907.492 430.671  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"907.492,430.671 265.903,430.671 265.903,413.605 907.492,413.605 907.492,430.671 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1582.85 409.338 L265.903 409.338 L265.903 392.271 L1582.85 392.271 L1582.85 409.338 L1582.85 409.338  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1582.85,409.338 265.903,409.338 265.903,392.271 1582.85,392.271 1582.85,409.338 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M1008.8 388.005 L265.903 388.005 L265.903 370.938 L1008.8 370.938 L1008.8 388.005 L1008.8 388.005  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1008.8,388.005 265.903,388.005 265.903,370.938 1008.8,370.938 1008.8,388.005 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M2258.21 366.671 L265.903 366.671 L265.903 349.605 L2258.21 349.605 L2258.21 366.671 L2258.21 366.671  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2258.21,366.671 265.903,366.671 265.903,349.605 2258.21,349.605 2258.21,366.671 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M2156.9 345.338 L265.903 345.338 L265.903 328.271 L2156.9 328.271 L2156.9 345.338 L2156.9 345.338  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2156.9,345.338 265.903,345.338 265.903,328.271 2156.9,328.271 2156.9,345.338 \"/>\n",
       "<path clip-path=\"url(#clip672)\" d=\"M2291.97 324.005 L265.903 324.005 L265.903 306.938 L2291.97 306.938 L2291.97 324.005 L2291.97 324.005  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip672)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2291.97,324.005 265.903,324.005 265.903,306.938 2291.97,306.938 2291.97,324.005 \"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"299.671\" cy=\"1168.8\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"333.439\" cy=\"1062.14\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"367.207\" cy=\"678.137\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"400.974\" cy=\"742.137\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"434.742\" cy=\"294.138\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"468.51\" cy=\"635.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"502.278\" cy=\"1083.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"536.046\" cy=\"1318.13\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"569.814\" cy=\"443.471\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"603.582\" cy=\"827.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"637.349\" cy=\"1019.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"671.117\" cy=\"1211.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"704.885\" cy=\"912.803\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"738.653\" cy=\"230.138\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"772.421\" cy=\"806.137\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"806.189\" cy=\"550.137\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"839.956\" cy=\"486.138\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"873.724\" cy=\"379.471\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"907.492\" cy=\"400.805\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"941.26\" cy=\"464.804\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"975.028\" cy=\"315.471\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1008.8\" cy=\"614.137\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1042.56\" cy=\"1382.13\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1076.33\" cy=\"1232.8\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1110.1\" cy=\"1339.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1143.87\" cy=\"1360.8\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1177.63\" cy=\"955.469\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1211.4\" cy=\"976.803\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1245.17\" cy=\"592.804\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1278.94\" cy=\"1040.8\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1312.71\" cy=\"1296.8\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1346.47\" cy=\"1254.14\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1380.24\" cy=\"1275.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1414.01\" cy=\"1190.14\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1447.78\" cy=\"763.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1481.55\" cy=\"1147.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1515.31\" cy=\"507.471\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1549.08\" cy=\"848.803\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1582.85\" cy=\"699.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1616.62\" cy=\"422.138\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1650.38\" cy=\"870.136\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1684.15\" cy=\"251.472\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1717.92\" cy=\"720.803\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1751.69\" cy=\"187.472\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1785.46\" cy=\"891.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1819.22\" cy=\"998.136\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1852.99\" cy=\"571.471\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1886.76\" cy=\"934.136\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1920.53\" cy=\"144.805\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip672)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1954.3\" cy=\"208.805\" r=\"2\"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_gain =  [(first(x),last(x)) for x in importance(model)]\n",
    "feature, gain = first.(feature_gain), last.(feature_gain)\n",
    "\n",
    "using Plots;\n",
    "\n",
    "p = bar(feature, y=gain, orientation=\"h\", legend=false)\n",
    "xlabel!(p,\"Gain\")\n",
    "ylabel!(p,\"Feature\")\n",
    "title!(\"Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d533db36",
   "metadata": {},
   "source": [
    "As you can see, not all features has the same importance. It should be notice that the Feature axis identifies the position in the Vector  feature which is ordered by the gain value by default."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
