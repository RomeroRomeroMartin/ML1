{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95ae012f",
   "metadata": {},
   "source": [
    "# The ScikitLearn.jl library\n",
    "\n",
    "The Scikit-learn library is an open source machine learning library developed for the Python programming language, the first version of which dates back to 2010. It implements a large number of machine learning models, related to tasks such as classification, regression, clustering or dimensionality reduction. These models include Support Vector Machines (SVM), decision trees, random forests, or k-means. It is currently one of the most widely used libraries in the field of machine learning, due to the large number of functionalities it offers as well as its ease of use, since it provides a uniform interface for training and using models. The documentation for this library is available at https://scikit-learn.org/stable/.\n",
    "\n",
    "For Julia, the ScikitLearn.jl library implements this interface and the algorithms contained in the scikit-learn library, supporting both Julia's own models and those of the scikit-learn library. The latter is done by means of the PyCall.jl library, which allows code written in Python to be executed from Julia in a transparent way for the user, who only needs to have ScikitLearn.jl installed. Documentation for this library can be found at https://scikitlearnjl.readthedocs.io/en/latest/.\n",
    "\n",
    "As mentioned above, this library provides a uniform interface for training different models. This is reflected in the fact that the names of the functions for creating and training models will be the same regardless of the models to be developed. In the assignments of this course, in addition to ANNs, the following models available in the scikit-learn library will be used:\n",
    "\n",
    "- Support Vector Machines (SVM)\n",
    "- Decision trees\n",
    "- kNN\n",
    "\n",
    "In order to use these models, it is first necessary to import the library (using ScikitLearn, which must be previously installed with\n",
    "\n",
    "```Julia\n",
    "import Pkg;\n",
    "Pkg.add(\"ScikitLearn\"))\n",
    "```\n",
    "\n",
    "The scikit-learn library offers more than 100 types of  different models. To import the models to be used, you can use @sk_import. In this way, the following lines import respectively the first 3 models mentioned above that will be used in the practices of this subject:\n",
    "\n",
    "```Julia\n",
    "@sk_import svm: SVC\n",
    "@sk_import tree: DecisionTreeClassifier\n",
    "@sk_import neighbours: KNeighborsClassifier\n",
    "```\n",
    "\n",
    "When training a model, the first step is to generate it. This is done with a different function for each model. This function receives as parameters the model's own parameters. Below are 3 examples, one for each type of model that will be used in these course assignments:\n",
    "\n",
    "```Julia\n",
    "model = SVC(kernel=\"rbf\", degree=3, gamma=2, C=1);\n",
    "model = DecisionTreeClassifier(max_depth=4, random_state=1);\n",
    "model = KNeighborsClassifier(3);\n",
    "```\n",
    "\n",
    "An explanation of the parameters accepted by each of these functions can be found in the library documentation. In the particular case of decision trees, as can be seen, one of these parameters is called `random_state`. This parameter controls the randomness in a particular part of the tree construction process, namely in the selection of features to split a node of the tree. The Scikit-Learn library uses a random number generator in this part, which is updated with each call, so that different calls to this function (together with its subsequent calls to the `fit!` function) to train the model will result in different models. To control the randomness of this process and make it deterministic, it is best to give it an integer value as shown in the example. Thus, the creation of a decision tree with a set of desired inputs and outputs and a given set of hyperparameters is a deterministic process. In general, it is more advisable to be able to control the randomness of the whole model development process (cross-validation, etc.) by means of a random seed that is set at the beginning of the whole process.\n",
    "\n",
    "Once created, any of these models can be adjusted with the `fit!` function.\n",
    "\n",
    "### Question\n",
    "\n",
    "What does the fact that the name of this function ends in bang (!) indicate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b8524",
   "metadata": {},
   "source": [
    "`Answer here` fit!() takes an unfitted model and a set of data, performs model fitting using that data, and modifies the model object itself to reflect the fitted parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5151f1",
   "metadata": {},
   "source": [
    "Contrary to the Flux library, where it was necessary to write the ANN training loop, in this library the loop is already implemented, and it is called automatically when the `fit!` function is executed. Therefore, it is not necessary to write the code for the training loop.\n",
    "\n",
    "### Question\n",
    "\n",
    "As in the case of ANNs, a loop is necessary for training several models. Where in the code (inside or outside the loop) will you need to create the model? Which models will need to be trained several times and which ones only once? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7437a0",
   "metadata": {},
   "source": [
    "`Answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dc832c",
   "metadata": {},
   "source": [
    "An example of the use of this function can be seen in the following line:\n",
    "\n",
    "```Julia\n",
    "fit!(model, trainingInputs, trainingTargets);\n",
    "```\n",
    "\n",
    "As can be seen, the first argument of this function is the model, the second is an array of inputs, and the third is a vector of desired outputs. It is important to realise that this parameter with the desired outputs is not an array like in the case of ANNs but a vector whose each element will correspond to the label associated to that pattern, and can be of any type: integer, string, etc. The main reason for this is that there are some models that do not accept desired outputs with the one-hot-encoding.\n",
    "\n",
    "An important issue to consider is the layout of the data to be used. As has been shown in previous assignments, the patterns must be arranged in columns to train an ANN, being each row an attribute. Outside the world of ANNs, and therefore with the rest of the techniques to be used in this course, the patterns are usually assumed to be arranged in rows, and therefore each column in the input matrix corresponds to an attribute, being a much more intuitive way.\n",
    "\n",
    "### Question\n",
    "\n",
    "Which condition must the matrix of inputs and the vector of desired outputs passed as an argument to this function fulfil?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ed4cd",
   "metadata": {},
   "source": [
    "`Answer here` The number of rows in the matrix of inputs must be the same as the number of instances in the desired outputs vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef8087b",
   "metadata": {},
   "source": [
    "Finally, once the model has been trained, it can be used to make predictions. This is done by means of the predict function. An example of its use is shown below:\n",
    "\n",
    "```Julia\n",
    "testOutputs = predict(model, testInputs);\n",
    "```\n",
    "\n",
    "The model being used is an in-memory structure with different fields, and it can be very useful to look up the contents of these fields. To see which fields each model has, you can write the following:\n",
    "\n",
    "```Julia\n",
    "println(keys(model));\n",
    "```\n",
    "\n",
    "Depending on the type of model, there will be different fields. For example, for a kNN, the following fields, among others, could be consulted:\n",
    "\n",
    "```Julia\n",
    "model.n_neighbors\n",
    "model.metric\n",
    "model.weights\n",
    "```\n",
    "\n",
    "For an SVM, some other interesting fields could be the following:\n",
    "\n",
    "```Julia\n",
    "model.C\n",
    "model.support_vectors_\n",
    "model.support_\n",
    "model.support_\n",
    "```\n",
    "\n",
    "In the case of an SVM, a particularly interesting function is `decision_function`, which returns the distances to the hyperplane of the passed patterns. This is useful, for example, to implement a \"one-against-all\" strategy to perform multi-class classification. An example of the use of this function is shown below:\n",
    "\n",
    "```Julia\n",
    "distances = decision_function(model, inputs);\n",
    "```\n",
    "\n",
    "### Question\n",
    "\n",
    "In the case of using decision trees or kNN, a corresponding function is not necessary to perform the \"one-against-all\" strategy, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e501314",
   "metadata": {},
   "source": [
    "`Answer here` \n",
    "\n",
    "Decision trees can support multi-class classification without needing a \"one-against-all\" strategy. Each leaf node in a decision tree corresponds to a specific class, and the decision path from the root to a leaf determines the predicted class.\n",
    "\n",
    "kNN can naturally handle multiclass classification by considering the neighbors of the query point and assigning the class that appears most frequently among those neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca345045",
   "metadata": {},
   "source": [
    "However, the SVM implementation in the Scikit-Learn library already allows multi-class classification, so it is not necessary to use a \"one-against-all\" strategy for these cases.\n",
    "\n",
    "Finally, it should be noted that these models usually receive pre-processed inputs and outputs, with the most common pre-processing being the normalisation already described in a previous assignment. Therefore, the developed normalisation functions should also be used on the data to be used by these models.\n",
    "\n",
    "In this assignment, you are asked to develop a function called ```modelCrossValidation``` based on the functions developed in previous assignments that allows to validate models in the selected classification problem using the three techniques described here.\n",
    "\n",
    "This function should perform cross-validation and use the metrics deemed most appropriate for the specific problem. This cross-validation can be done by modifying the code developed in the previous assignment.\n",
    "\n",
    "This function must receive the following parameters:\n",
    "\n",
    "- Algorithm to be trained, among the 4 used in this course, together with its parameter. The most important parameters to specify for each technique are:\n",
    "    </br>\n",
    "    \n",
    "    - ANN\n",
    "        - Architecture (number of hidden layers and number of neurons in each hidden layer) and transfer funtion in each layer. In \"shallow\" networks such as those used in this course, the transfer function has less impact, so a standard one, shuch as `tansig` or `logsig`, can be used.\n",
    "        - Learning rate\n",
    "        - Ratio of patterns used for validation\n",
    "        - Number of consecutive iterations without improving the validation loss to stop the process\n",
    "        - Number of times each ANN is trained.\n",
    "        \n",
    "        ### Question\n",
    "        \n",
    "        Why should a linear transfer function not be used for neurons in the hidden layers?\n",
    "        \n",
    "        ```Answer here``` Linear transfer function should not be used for neurons in the hidden layers because a neural network with only linear activation functions is essentially equivalent to a single-layer perceptron, and it cannot capture the complexity needed for many real-world problems. For example, if we have a dataset with non linear relationship, a ann with linear transfer function won't be able to address the problem.\n",
    "        \n",
    "        ### Question\n",
    "        \n",
    "        The other models do not have the number of times to train them as a parameter. Why? If you train several times, Which statistical properties will the results of these trainings have?\n",
    "        \n",
    "         ```Answer here``` Because these algorithms typically have deterministic training procedures. Once trained on a specific dataset, the model's parameters are fixed, and retraining it on the same data multiple times would yield the same result.\n",
    "    </br>  \n",
    "    \n",
    "    - SVM\n",
    "        - Kernel (and kernel-specific parameters)\n",
    "        - C\n",
    "        \n",
    "    </br>  \n",
    "    - Decision trees\n",
    "        - Maximum tree depth\n",
    "        \n",
    "    </br>  \n",
    "    - kNN\n",
    "        - k (number of neighbours to be considered)\n",
    "\n",
    "    </br>        \n",
    "- Already standardised input and desired outputs matrices.\n",
    "    </br>  \n",
    "\n",
    "    - As stated above, the desired outputs must be indicated as a vector where each element is the label corresponding to each pattern (therefore, of type `Array{Any,1}`). In the case of ANN training, the desired outputs shall be encoded as done in previous assignments.\n",
    "    \n",
    "    ### Question\n",
    "    \n",
    "    Has it been necessary to standardise the desired outputs? Why?\n",
    "    \n",
    "    ```Answer here``` Standardization or normalization is generally not necessary. However, if we use a ANN to a classification problem, the desired outputs shall be encoded using techniques like one-hot encoding. And if we use one of the other three models, they must be indicated as a vector where each element is the label corresponding to each pattern\n",
    "    \n",
    "    </br>  \n",
    "    - As previously described, in the case of using techniques such as SVM, decision trees or kNN, the one-hot-encoding configuration will not be used. In these cases, the `confusionMatrix` function developed in a previous assignment will be used to calculate the metrics, which accepts as input two vectors (outputs and desired outputs) of type `Array{Any,1}`.\n",
    "    \n",
    "    </br>  \n",
    "- Cross-validation indices. It is important to note that, as in the previous assignment, the partitioning of the patterns in each fold need to be done outside this function, because this allows this same partitioning to be used then training other models. In this way, cross-validation is performed with the same data and the same partitions in all classes.\n",
    "\n",
    "Since most of the code will be the same, do not develop 4 different functions, one for each model, but only one function. Inside it, at the time of generation the model in each fold, and depending on the model, the following changes should be made:\n",
    "\n",
    "- If the model is an ANN, the desired outputs shall be encoded by means of the code developed in previous assignments. As this model is non-deterministic, it will be nevessary to make a new loop to train several ANNs, splitting the training data into training and validation (if validation set is used) and calling the function defined in previous assignments to create and traing an ANN.\n",
    "\n",
    "- If the model is not an ANN, the code that trains the model shall be developed. This code shall be the same for each of the rematining 3 types of models (SVM, decision trees, and KNN), with the line where the model is called being the only difference.\n",
    "\n",
    "In turn, this function should return, at least, the values for the selected metrics. Once this function has been developed, the experimental part of the assignment begins. The objective is to determine which model with a specific combination of hyperparameters offers the best results, for which the above function will be run for each of the 4 types of models, and for each model it will be run with different values in its hyperparameters.\n",
    "\n",
    "- The results obtained should be documented in the report to be produced, for which it will be useful to show the results in tabular and/or graphical form.\n",
    "\n",
    "- When it comes to displaying a confusion matrix in the report, an important question is which one to show given that a lot of trainings have been performed. The cross-validation technique does not generate a final model, but allows comparing different algorithms and configurations to choose the model or parameter configuration that returns the best results. Once chosen, it is necessary to train a \"final\" model from scratch by using all the patterns as the training set, that is, without separating patterns for testing. In this way, the performance of this model and configuration is expected to be slightly higher than that obtained through cross-validation, since more patterns have been used to train it. This is the final model that would be used in production, and from which a confusion matrix can be obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8155a7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg;\n",
    "Pkg.add(\"ScikitLearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4eb0e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mRunning `conda install -y -c anaconda conda` in root environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/martin/.julia/conda/3/x86_64\n",
      "\n",
      "  added / updated specs:\n",
      "    - conda\n",
      "\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2023.7.2~ --> anaconda::ca-certificates-2023.08.22-h06a4308_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            conda-forge/noarch::certifi-2023.7.22~ --> anaconda/linux-64::certifi-2023.7.22-py310h06a4308_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.3.1\n",
      "  latest version: 23.10.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.10.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mRunning `conda install -y -c conda-forge 'libstdcxx-ng>=3.4,<13.0'` in root environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/martin/.julia/conda/3/x86_64\n",
      "\n",
      "  added / updated specs:\n",
      "    - libstdcxx-ng[version='>=3.4,<13.0']\n",
      "\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates    anaconda::ca-certificates-2023.08.22-~ --> conda-forge::ca-certificates-2023.7.22-hbcca054_0 \n",
      "  certifi            anaconda/linux-64::certifi-2023.7.22-~ --> conda-forge/noarch::certifi-2023.7.22-pyhd8ed1ab_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.3.1\n",
      "  latest version: 23.10.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.10.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyObject <class 'sklearn.neighbors._classification.KNeighborsClassifier'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "@sk_import neural_network: MLPClassifier\n",
    "@sk_import svm: SVC\n",
    "@sk_import tree: DecisionTreeClassifier\n",
    "@sk_import neighbors: KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f354b7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix{Float32}\n",
      "Epoch 0: loss: 1.1962069\n",
      "Epoch 1: loss: 1.1845782\n",
      "Epoch 2: loss: 1.1737514\n",
      "Epoch 3: loss: 1.1637354\n",
      "Epoch 4: loss: 1.1545314\n",
      "Epoch 5: loss: 1.1461293\n",
      "Epoch 6: loss: 1.138514\n",
      "Epoch 7: loss: 1.1316622\n",
      "Epoch 8: loss: 1.1255448\n",
      "Epoch 9: loss: 1.1201278\n",
      "Epoch 10: loss: 1.1153723\n",
      "Epoch 11: loss: 1.1112401\n",
      "Epoch 12: loss: 1.107693\n",
      "Epoch 13: loss: 1.104692\n",
      "Epoch 14: loss: 1.1021975\n",
      "Epoch 15: loss: 1.100167\n",
      "Epoch 16: loss: 1.0985558\n",
      "Epoch 17: loss: 1.0973163\n",
      "Epoch 18: loss: 1.0963992\n",
      "Epoch 19: loss: 1.0957538\n",
      "Epoch 20: loss: 1.0953293\n",
      "Epoch 21: loss: 1.0950766\n",
      "Epoch 22: loss: 1.0949469\n",
      "Epoch 23: loss: 1.0948968\n",
      "Epoch 24: loss: 1.0948856\n",
      "Epoch 25: loss: 1.094879\n",
      "Epoch 26: loss: 1.0948478\n",
      "Epoch 27: loss: 1.0947692\n",
      "Epoch 28: loss: 1.094627\n",
      "Epoch 29: loss: 1.0944095\n",
      "Epoch 30: loss: 1.0941108\n",
      "Epoch 31: loss: 1.0937289\n",
      "Epoch 32: loss: 1.0932657\n",
      "Epoch 33: loss: 1.0927247\n",
      "Epoch 34: loss: 1.0921121\n",
      "Epoch 35: loss: 1.0914333\n",
      "Epoch 36: loss: 1.0906954\n",
      "Epoch 37: loss: 1.0899044\n",
      "Epoch 38: loss: 1.0890659\n",
      "Epoch 39: loss: 1.0881835\n",
      "Epoch 40: loss: 1.0872598\n",
      "Epoch 41: loss: 1.0862968\n",
      "Epoch 42: loss: 1.0852942\n",
      "Epoch 43: loss: 1.0842503\n",
      "Epoch 44: loss: 1.0831627\n",
      "Epoch 45: loss: 1.0820278\n",
      "Epoch 46: loss: 1.0808394\n",
      "Epoch 47: loss: 1.0795925\n",
      "Epoch 48: loss: 1.0782801\n",
      "Epoch 49: loss: 1.0768944\n",
      "Epoch 50: loss: 1.075428\n",
      "Epoch 51: loss: 1.0738711\n",
      "Epoch 52: loss: 1.0722167\n",
      "Epoch 53: loss: 1.0704544\n",
      "Epoch 54: loss: 1.068576\n",
      "Epoch 55: loss: 1.0665723\n",
      "Epoch 56: loss: 1.0644331\n",
      "Epoch 57: loss: 1.0621507\n",
      "Epoch 58: loss: 1.0597149\n",
      "Epoch 59: loss: 1.0571171\n",
      "Epoch 60: loss: 1.054348\n",
      "Epoch 61: loss: 1.0513983\n",
      "Epoch 62: loss: 1.0482602\n",
      "Epoch 63: loss: 1.0449241\n",
      "Epoch 64: loss: 1.0413808\n",
      "Epoch 65: loss: 1.0376225\n",
      "Epoch 66: loss: 1.0336404\n",
      "Epoch 67: loss: 1.0294269\n",
      "Epoch 68: loss: 1.0249734\n",
      "Epoch 69: loss: 1.0202732\n",
      "Epoch 70: loss: 1.0153195\n",
      "Epoch 71: loss: 1.0101058\n",
      "Epoch 72: loss: 1.0046263\n",
      "Epoch 73: loss: 0.99887663\n",
      "Epoch 74: loss: 0.9928518\n",
      "Epoch 75: loss: 0.9865498\n",
      "Epoch 76: loss: 0.97996736\n",
      "Epoch 77: loss: 0.97310454\n",
      "Epoch 78: loss: 0.9659615\n",
      "Epoch 79: loss: 0.95854026\n",
      "Epoch 80: loss: 0.95084524\n",
      "Epoch 81: loss: 0.9428819\n",
      "Epoch 82: loss: 0.9346585\n",
      "Epoch 83: loss: 0.9261845\n",
      "Epoch 84: loss: 0.9174714\n",
      "Epoch 85: loss: 0.90853304\n",
      "Epoch 86: loss: 0.89938426\n",
      "Epoch 87: loss: 0.8900419\n",
      "Epoch 88: loss: 0.8805238\n",
      "Epoch 89: loss: 0.87084943\n",
      "Epoch 90: loss: 0.8610388\n",
      "Epoch 91: loss: 0.85111314\n",
      "Epoch 92: loss: 0.8410939\n",
      "Epoch 93: loss: 0.8310034\n",
      "Epoch 94: loss: 0.8208637\n",
      "Epoch 95: loss: 0.8106974\n",
      "Epoch 96: loss: 0.80052584\n",
      "Epoch 97: loss: 0.79037076\n",
      "Epoch 98: loss: 0.7802534\n",
      "Epoch 99: loss: 0.77019376\n",
      "Epoch 100: loss: 0.76021105\n",
      "Epoch 101: loss: 0.75032383\n",
      "Epoch 102: loss: 0.74054915\n",
      "Epoch 103: loss: 0.73090297\n",
      "Epoch 104: loss: 0.7214001\n",
      "Epoch 105: loss: 0.712054\n",
      "Epoch 106: loss: 0.7028767\n",
      "Epoch 107: loss: 0.69387907\n",
      "Epoch 108: loss: 0.6850702\n",
      "Epoch 109: loss: 0.67645836\n",
      "Epoch 110: loss: 0.66805005\n",
      "Epoch 111: loss: 0.6598507\n",
      "Epoch 112: loss: 0.65186465\n",
      "Epoch 113: loss: 0.64409447\n",
      "Epoch 114: loss: 0.6365422\n",
      "Epoch 115: loss: 0.6292088\n",
      "Epoch 116: loss: 0.62209374\n",
      "Epoch 117: loss: 0.6151959\n",
      "Epoch 118: loss: 0.6085134\n",
      "Epoch 119: loss: 0.6020434\n",
      "Epoch 120: loss: 0.59578234\n",
      "Epoch 121: loss: 0.5897258\n",
      "Epoch 122: loss: 0.58386976\n",
      "Epoch 123: loss: 0.5782083\n",
      "Epoch 124: loss: 0.57273614\n",
      "Epoch 125: loss: 0.5674474\n",
      "Epoch 126: loss: 0.56233567\n",
      "Epoch 127: loss: 0.5573944\n",
      "Epoch 128: loss: 0.552617\n",
      "Epoch 129: loss: 0.5479968\n",
      "Epoch 130: loss: 0.54352695\n",
      "Epoch 131: loss: 0.53920037\n",
      "Epoch 132: loss: 0.53501046\n",
      "Epoch 133: loss: 0.5309503\n",
      "Epoch 134: loss: 0.5270131\n",
      "Epoch 135: loss: 0.5231925\n",
      "Epoch 136: loss: 0.51948184\n",
      "Epoch 137: loss: 0.51587486\n",
      "Epoch 138: loss: 0.5123652\n",
      "Epoch 139: loss: 0.5089472\n",
      "Epoch 140: loss: 0.50561506\n",
      "Epoch 141: loss: 0.50236285\n",
      "Epoch 142: loss: 0.49918517\n",
      "Epoch 143: loss: 0.49607712\n",
      "Epoch 144: loss: 0.4930334\n",
      "Epoch 145: loss: 0.49004963\n",
      "Epoch 146: loss: 0.48712057\n",
      "Epoch 147: loss: 0.48424244\n",
      "Epoch 148: loss: 0.48141077\n",
      "Epoch 149: loss: 0.4786215\n",
      "Epoch 150: loss: 0.47587124\n",
      "Epoch 151: loss: 0.47315633\n",
      "Epoch 152: loss: 0.47047347\n",
      "Epoch 153: loss: 0.4678195\n",
      "Epoch 154: loss: 0.46519175\n",
      "Epoch 155: loss: 0.4625871\n",
      "Epoch 156: loss: 0.46000347\n",
      "Epoch 157: loss: 0.4574385\n",
      "Epoch 158: loss: 0.45488992\n",
      "Epoch 159: loss: 0.4523561\n",
      "Epoch 160: loss: 0.44983506\n",
      "Epoch 161: loss: 0.44732514\n",
      "Epoch 162: loss: 0.44482502\n",
      "Epoch 163: loss: 0.44233364\n",
      "Epoch 164: loss: 0.4398496\n",
      "Epoch 165: loss: 0.43737182\n",
      "Epoch 166: loss: 0.43489975\n",
      "Epoch 167: loss: 0.4324323\n",
      "Epoch 168: loss: 0.42996883\n",
      "Epoch 169: loss: 0.42750886\n",
      "Epoch 170: loss: 0.4250518\n",
      "Epoch 171: loss: 0.42259738\n",
      "Epoch 172: loss: 0.42014512\n",
      "Epoch 173: loss: 0.4176948\n",
      "Epoch 174: loss: 0.41524625\n",
      "Epoch 175: loss: 0.41279924\n",
      "Epoch 176: loss: 0.4103537\n",
      "Epoch 177: loss: 0.40790966\n",
      "Epoch 178: loss: 0.40546694\n",
      "Epoch 179: loss: 0.40302566\n",
      "Epoch 180: loss: 0.40058586\n",
      "Epoch 181: loss: 0.39814776\n",
      "Epoch 182: loss: 0.39571133\n",
      "Epoch 183: loss: 0.3932769\n",
      "Epoch 184: loss: 0.39084446\n",
      "Epoch 185: loss: 0.38841432\n",
      "Epoch 186: loss: 0.3859869\n",
      "Epoch 187: loss: 0.3835621\n",
      "Epoch 188: loss: 0.3811403\n",
      "Epoch 189: loss: 0.37872195\n",
      "Epoch 190: loss: 0.37630716\n",
      "Epoch 191: loss: 0.37389633\n",
      "Epoch 192: loss: 0.37148967\n",
      "Epoch 193: loss: 0.36908755\n",
      "Epoch 194: loss: 0.36669028\n",
      "Epoch 195: loss: 0.36429796\n",
      "Epoch 196: loss: 0.36191118\n",
      "Epoch 197: loss: 0.3595302\n",
      "Epoch 198: loss: 0.35715517\n",
      "Epoch 199: loss: 0.35478637\n",
      "Epoch 200: loss: 0.35242432\n",
      "Epoch 201: loss: 0.35006908\n",
      "Epoch 202: loss: 0.3477211\n",
      "Epoch 203: loss: 0.3453805\n",
      "Epoch 204: loss: 0.34304765\n",
      "Epoch 205: loss: 0.3407229\n",
      "Epoch 206: loss: 0.3384062\n",
      "Epoch 207: loss: 0.33609816\n",
      "Epoch 208: loss: 0.33379886\n",
      "Epoch 209: loss: 0.33150855\n",
      "Epoch 210: loss: 0.32922754\n",
      "Epoch 211: loss: 0.32695612\n",
      "Epoch 212: loss: 0.3246944\n",
      "Epoch 213: loss: 0.32244268\n",
      "Epoch 214: loss: 0.3202011\n",
      "Epoch 215: loss: 0.31797016\n",
      "Epoch 216: loss: 0.31574982\n",
      "Epoch 217: loss: 0.3135404\n",
      "Epoch 218: loss: 0.31134206\n",
      "Epoch 219: loss: 0.3091551\n",
      "Epoch 220: loss: 0.3069797\n",
      "Epoch 221: loss: 0.304816\n",
      "Epoch 222: loss: 0.3026642\n",
      "Epoch 223: loss: 0.30052453\n",
      "Epoch 224: loss: 0.29839733\n",
      "Epoch 225: loss: 0.29628244\n",
      "Epoch 226: loss: 0.29418036\n",
      "Epoch 227: loss: 0.29209113\n",
      "Epoch 228: loss: 0.2900148\n",
      "Epoch 229: loss: 0.28795168\n",
      "Epoch 230: loss: 0.2859018\n",
      "Epoch 231: loss: 0.28386548\n",
      "Epoch 232: loss: 0.28184268\n",
      "Epoch 233: loss: 0.27983359\n",
      "Epoch 234: loss: 0.2778383\n",
      "Epoch 235: loss: 0.27585694\n",
      "Epoch 236: loss: 0.2738897\n",
      "Epoch 237: loss: 0.27193654\n",
      "Epoch 238: loss: 0.26999757\n",
      "Epoch 239: loss: 0.26807302\n",
      "Epoch 240: loss: 0.26616278\n",
      "Epoch 241: loss: 0.2642671\n",
      "Epoch 242: loss: 0.26238593\n",
      "Epoch 243: loss: 0.26051942\n",
      "Epoch 244: loss: 0.2586674\n",
      "Epoch 245: loss: 0.25683016\n",
      "Epoch 246: loss: 0.2550077\n",
      "Epoch 247: loss: 0.25319993\n",
      "Epoch 248: loss: 0.251407\n",
      "Epoch 249: loss: 0.24962889\n",
      "Epoch 250: loss: 0.2478655\n",
      "Epoch 251: loss: 0.24611703\n",
      "Epoch 252: loss: 0.24438334\n",
      "Epoch 253: loss: 0.24266455\n",
      "Epoch 254: loss: 0.24096054\n",
      "Epoch 255: loss: 0.23927134\n",
      "Epoch 256: loss: 0.23759702\n",
      "Epoch 257: loss: 0.23593737\n",
      "Epoch 258: loss: 0.2342925\n",
      "Epoch 259: loss: 0.23266234\n",
      "Epoch 260: loss: 0.23104686\n",
      "Epoch 261: loss: 0.22944602\n",
      "Epoch 262: loss: 0.22785975\n",
      "Epoch 263: loss: 0.22628802\n",
      "Epoch 264: loss: 0.2247308\n",
      "Epoch 265: loss: 0.22318801\n",
      "Epoch 266: loss: 0.2216596\n",
      "Epoch 267: loss: 0.22014557\n",
      "Epoch 268: loss: 0.2186457\n",
      "Epoch 269: loss: 0.21716003\n",
      "Epoch 270: loss: 0.2156885\n",
      "Epoch 271: loss: 0.21423098\n",
      "Epoch 272: loss: 0.21278746\n",
      "Epoch 273: loss: 0.21135774\n",
      "Epoch 274: loss: 0.20994191\n",
      "Epoch 275: loss: 0.20853974\n",
      "Epoch 276: loss: 0.20715119\n",
      "Epoch 277: loss: 0.20577618\n",
      "Epoch 278: loss: 0.20441467\n",
      "Epoch 279: loss: 0.20306644\n",
      "Epoch 280: loss: 0.20173156\n",
      "Epoch 281: loss: 0.20040976\n",
      "Epoch 282: loss: 0.19910109\n",
      "Epoch 283: loss: 0.19780536\n",
      "Epoch 284: loss: 0.19652249\n",
      "Epoch 285: loss: 0.19525242\n",
      "Epoch 286: loss: 0.19399503\n",
      "Epoch 287: loss: 0.19275023\n",
      "Epoch 288: loss: 0.1915179\n",
      "Epoch 289: loss: 0.1902979\n",
      "Epoch 290: loss: 0.18909016\n",
      "Epoch 291: loss: 0.18789461\n",
      "Epoch 292: loss: 0.18671109\n",
      "Epoch 293: loss: 0.18553951\n",
      "Epoch 294: loss: 0.18437979\n",
      "Epoch 295: loss: 0.1832317\n",
      "Epoch 296: loss: 0.18209535\n",
      "Epoch 297: loss: 0.18097049\n",
      "Epoch 298: loss: 0.179857\n",
      "Epoch 299: loss: 0.17875484\n",
      "Epoch 300: loss: 0.17766383\n",
      "Epoch 301: loss: 0.17658395\n",
      "Epoch 302: loss: 0.17551504\n",
      "Epoch 303: loss: 0.17445704\n",
      "Epoch 304: loss: 0.17340975\n",
      "Epoch 305: loss: 0.17237312\n",
      "Epoch 306: loss: 0.17134704\n",
      "Epoch 307: loss: 0.17033143\n",
      "Epoch 308: loss: 0.16932611\n",
      "Epoch 309: loss: 0.16833106\n",
      "Epoch 310: loss: 0.16734612\n",
      "Epoch 311: loss: 0.1663712\n",
      "Epoch 312: loss: 0.16540618\n",
      "Epoch 313: loss: 0.164451\n",
      "Epoch 314: loss: 0.16350551\n",
      "Epoch 315: loss: 0.16256969\n",
      "Epoch 316: loss: 0.16164333\n",
      "Epoch 317: loss: 0.16072635\n",
      "Epoch 318: loss: 0.15981865\n",
      "Epoch 319: loss: 0.15892023\n",
      "Epoch 320: loss: 0.1580309\n",
      "Epoch 321: loss: 0.15715052\n",
      "Epoch 322: loss: 0.1562791\n",
      "Epoch 323: loss: 0.15541649\n",
      "Epoch 324: loss: 0.15456252\n",
      "Epoch 325: loss: 0.15371725\n",
      "Epoch 326: loss: 0.1528804\n",
      "Epoch 327: loss: 0.15205207\n",
      "Epoch 328: loss: 0.15123206\n",
      "Epoch 329: loss: 0.15042023\n",
      "Epoch 330: loss: 0.14961657\n",
      "Epoch 331: loss: 0.14882101\n",
      "Epoch 332: loss: 0.14803332\n",
      "Epoch 333: loss: 0.14725356\n",
      "Epoch 334: loss: 0.14648156\n",
      "Epoch 335: loss: 0.14571734\n",
      "Epoch 336: loss: 0.14496064\n",
      "Epoch 337: loss: 0.1442115\n",
      "Epoch 338: loss: 0.14346977\n",
      "Epoch 339: loss: 0.14273542\n",
      "Epoch 340: loss: 0.14200835\n",
      "Epoch 341: loss: 0.14128846\n",
      "Epoch 342: loss: 0.14057563\n",
      "Epoch 343: loss: 0.1398699\n",
      "Epoch 344: loss: 0.13917105\n",
      "Epoch 345: loss: 0.1384791\n",
      "Epoch 346: loss: 0.13779388\n",
      "Epoch 347: loss: 0.13711545\n",
      "Epoch 348: loss: 0.13644356\n",
      "Epoch 349: loss: 0.13577825\n",
      "Epoch 350: loss: 0.13511947\n",
      "Epoch 351: loss: 0.13446702\n",
      "Epoch 352: loss: 0.13382092\n",
      "Epoch 353: loss: 0.13318111\n",
      "Epoch 354: loss: 0.13254742\n",
      "Epoch 355: loss: 0.13191988\n",
      "Epoch 356: loss: 0.13129836\n",
      "Epoch 357: loss: 0.13068281\n",
      "Epoch 358: loss: 0.13007319\n",
      "Epoch 359: loss: 0.1294694\n",
      "Epoch 360: loss: 0.12887135\n",
      "Epoch 361: loss: 0.128279\n",
      "Epoch 362: loss: 0.12769224\n",
      "Epoch 363: loss: 0.12711112\n",
      "Epoch 364: loss: 0.12653549\n",
      "Epoch 365: loss: 0.12596531\n",
      "Epoch 366: loss: 0.12540048\n",
      "Epoch 367: loss: 0.124841005\n",
      "Epoch 368: loss: 0.124286704\n",
      "Epoch 369: loss: 0.12373766\n",
      "Epoch 370: loss: 0.12319365\n",
      "Epoch 371: loss: 0.12265486\n",
      "Epoch 372: loss: 0.122121036\n",
      "Epoch 373: loss: 0.12159218\n",
      "Epoch 374: loss: 0.12106814\n",
      "Epoch 375: loss: 0.12054897\n",
      "Epoch 376: loss: 0.120034635\n",
      "Epoch 377: loss: 0.11952502\n",
      "Epoch 378: loss: 0.11902007\n",
      "Epoch 379: loss: 0.118519716\n",
      "Epoch 380: loss: 0.11802398\n",
      "Epoch 381: loss: 0.117532745\n",
      "Epoch 382: loss: 0.11704594\n",
      "Epoch 383: loss: 0.116563566\n",
      "Epoch 384: loss: 0.116085574\n",
      "Epoch 385: loss: 0.11561189\n",
      "Epoch 386: loss: 0.115142465\n",
      "Epoch 387: loss: 0.11467728\n",
      "Epoch 388: loss: 0.11421623\n",
      "Epoch 389: loss: 0.113759324\n",
      "Epoch 390: loss: 0.11330648\n",
      "Epoch 391: loss: 0.11285767\n",
      "Epoch 392: loss: 0.11241282\n",
      "Epoch 393: loss: 0.11197191\n",
      "Epoch 394: loss: 0.11153489\n",
      "Epoch 395: loss: 0.111101724\n",
      "Epoch 396: loss: 0.110672355\n",
      "Epoch 397: loss: 0.11024677\n",
      "Epoch 398: loss: 0.109824896\n",
      "Epoch 399: loss: 0.10940665\n",
      "Epoch 400: loss: 0.10899207\n",
      "Epoch 401: loss: 0.10858106\n",
      "Epoch 402: loss: 0.10817362\n",
      "Epoch 403: loss: 0.10776971\n",
      "Epoch 404: loss: 0.10736926\n",
      "Epoch 405: loss: 0.10697224\n",
      "Epoch 406: loss: 0.1065786\n",
      "Epoch 407: loss: 0.10618832\n",
      "Epoch 408: loss: 0.10580137\n",
      "Epoch 409: loss: 0.10541772\n",
      "Epoch 410: loss: 0.10503729\n",
      "Epoch 411: loss: 0.10466007\n",
      "Epoch 412: loss: 0.104286045\n",
      "Epoch 413: loss: 0.103915125\n",
      "Epoch 414: loss: 0.10354732\n",
      "Epoch 415: loss: 0.10318261\n",
      "Epoch 416: loss: 0.102820896\n",
      "Epoch 417: loss: 0.10246221\n",
      "Epoch 418: loss: 0.10210647\n",
      "Epoch 419: loss: 0.10175368\n",
      "Epoch 420: loss: 0.10140381\n",
      "Epoch 421: loss: 0.10105677\n",
      "Epoch 422: loss: 0.100712605\n",
      "Epoch 423: loss: 0.10037122\n",
      "Epoch 424: loss: 0.10003261\n",
      "Epoch 425: loss: 0.09969681\n",
      "Epoch 426: loss: 0.09936366\n",
      "Epoch 427: loss: 0.099033244\n",
      "Epoch 428: loss: 0.09870547\n",
      "Epoch 429: loss: 0.09838031\n",
      "Epoch 430: loss: 0.09805777\n",
      "Epoch 431: loss: 0.097737834\n",
      "Epoch 432: loss: 0.09742039\n",
      "Epoch 433: loss: 0.09710548\n",
      "Epoch 434: loss: 0.096793056\n",
      "Epoch 435: loss: 0.09648311\n",
      "Epoch 436: loss: 0.0961756\n",
      "Epoch 437: loss: 0.095870495\n",
      "Epoch 438: loss: 0.09556781\n",
      "Epoch 439: loss: 0.095267445\n",
      "Epoch 440: loss: 0.09496944\n",
      "Epoch 441: loss: 0.094673716\n",
      "Epoch 442: loss: 0.094380304\n",
      "Epoch 443: loss: 0.094089165\n",
      "Epoch 444: loss: 0.09380025\n",
      "Epoch 445: loss: 0.09351355\n",
      "Epoch 446: loss: 0.093229055\n",
      "Epoch 447: loss: 0.092946716\n",
      "Epoch 448: loss: 0.092666544\n",
      "Epoch 449: loss: 0.09238848\n",
      "Epoch 450: loss: 0.09211252\n",
      "Epoch 451: loss: 0.091838665\n",
      "Epoch 452: loss: 0.09156686\n",
      "Epoch 453: loss: 0.09129708\n",
      "Epoch 454: loss: 0.091029294\n",
      "Epoch 455: loss: 0.09076356\n",
      "Epoch 456: loss: 0.09049978\n",
      "Epoch 457: loss: 0.090237916\n",
      "Epoch 458: loss: 0.08997805\n",
      "Epoch 459: loss: 0.089720055\n",
      "Epoch 460: loss: 0.08946399\n",
      "Epoch 461: loss: 0.08920982\n",
      "Epoch 462: loss: 0.08895743\n",
      "Epoch 463: loss: 0.08870695\n",
      "Epoch 464: loss: 0.08845828\n",
      "Epoch 465: loss: 0.08821141\n",
      "Epoch 466: loss: 0.08796629\n",
      "Epoch 467: loss: 0.087722994\n",
      "Epoch 468: loss: 0.08748141\n",
      "Epoch 469: loss: 0.08724158\n",
      "Epoch 470: loss: 0.087003484\n",
      "Epoch 471: loss: 0.08676703\n",
      "Epoch 472: loss: 0.0865323\n",
      "Epoch 473: loss: 0.086299226\n",
      "Epoch 474: loss: 0.08606781\n",
      "Epoch 475: loss: 0.085838035\n",
      "Epoch 476: loss: 0.085609846\n",
      "Epoch 477: loss: 0.085383296\n",
      "Epoch 478: loss: 0.08515831\n",
      "Epoch 479: loss: 0.08493488\n",
      "Epoch 480: loss: 0.084713064\n",
      "Epoch 481: loss: 0.08449275\n",
      "Epoch 482: loss: 0.084273964\n",
      "Epoch 483: loss: 0.08405666\n",
      "Epoch 484: loss: 0.08384094\n",
      "Epoch 485: loss: 0.083626606\n",
      "Epoch 486: loss: 0.083413824\n",
      "Epoch 487: loss: 0.08320246\n",
      "Epoch 488: loss: 0.08299253\n",
      "Epoch 489: loss: 0.08278404\n",
      "Epoch 490: loss: 0.08257695\n",
      "Epoch 491: loss: 0.08237132\n",
      "Epoch 492: loss: 0.082167014\n",
      "Epoch 493: loss: 0.08196409\n",
      "Epoch 494: loss: 0.08176256\n",
      "Epoch 495: loss: 0.08156241\n",
      "Epoch 496: loss: 0.08136353\n",
      "Epoch 497: loss: 0.081166014\n",
      "Epoch 498: loss: 0.080969796\n",
      "Epoch 499: loss: 0.080774896\n",
      "Epoch 500: loss: 0.08058127\n",
      "Epoch 501: loss: 0.080388956\n",
      "Epoch 502: loss: 0.08019789\n",
      "Epoch 503: loss: 0.08000808\n",
      "Epoch 504: loss: 0.0798195\n",
      "Epoch 505: loss: 0.07963217\n",
      "Epoch 506: loss: 0.07944608\n",
      "Epoch 507: loss: 0.07926117\n",
      "Epoch 508: loss: 0.07907748\n",
      "Epoch 509: loss: 0.07889499\n",
      "Epoch 510: loss: 0.078713655\n",
      "Epoch 511: loss: 0.07853354\n",
      "Epoch 512: loss: 0.07835454\n",
      "Epoch 513: loss: 0.07817669\n",
      "Epoch 514: loss: 0.07799999\n",
      "Epoch 515: loss: 0.07782442\n",
      "Epoch 516: loss: 0.07764996\n",
      "Epoch 517: loss: 0.07747664\n",
      "Epoch 518: loss: 0.07730437\n",
      "Epoch 519: loss: 0.077133246\n",
      "Epoch 520: loss: 0.07696317\n",
      "Epoch 521: loss: 0.07679419\n",
      "Epoch 522: loss: 0.07662626\n",
      "Epoch 523: loss: 0.07645936\n",
      "Epoch 524: loss: 0.07629358\n",
      "Epoch 525: loss: 0.0761288\n",
      "Epoch 526: loss: 0.07596503\n",
      "Epoch 527: loss: 0.075802304\n",
      "Epoch 528: loss: 0.07564059\n",
      "Epoch 529: loss: 0.075479865\n",
      "Epoch 530: loss: 0.075320125\n",
      "Epoch 531: loss: 0.07516138\n",
      "Epoch 532: loss: 0.07500365\n",
      "Epoch 533: loss: 0.07484683\n",
      "Epoch 534: loss: 0.07469101\n",
      "Epoch 535: loss: 0.07453614\n",
      "Epoch 536: loss: 0.07438221\n",
      "Epoch 537: loss: 0.07422923\n",
      "Epoch 538: loss: 0.07407716\n",
      "Epoch 539: loss: 0.073926054\n",
      "Epoch 540: loss: 0.07377585\n",
      "Epoch 541: loss: 0.07362654\n",
      "Epoch 542: loss: 0.07347813\n",
      "Epoch 543: loss: 0.073330626\n",
      "Epoch 544: loss: 0.07318401\n",
      "Epoch 545: loss: 0.073038295\n",
      "Epoch 546: loss: 0.072893426\n",
      "Epoch 547: loss: 0.07274943\n",
      "Epoch 548: loss: 0.072606295\n",
      "Epoch 549: loss: 0.07246402\n",
      "Epoch 550: loss: 0.07232256\n",
      "Epoch 551: loss: 0.072182\n",
      "Epoch 552: loss: 0.07204222\n",
      "Epoch 553: loss: 0.0719033\n",
      "Epoch 554: loss: 0.0717652\n",
      "Epoch 555: loss: 0.071627885\n",
      "Epoch 556: loss: 0.07149141\n",
      "Epoch 557: loss: 0.071355715\n",
      "Epoch 558: loss: 0.07122086\n",
      "Epoch 559: loss: 0.07108676\n",
      "Epoch 560: loss: 0.07095343\n",
      "Epoch 561: loss: 0.070820905\n",
      "Epoch 562: loss: 0.07068912\n",
      "Epoch 563: loss: 0.070558146\n",
      "Epoch 564: loss: 0.07042792\n",
      "Epoch 565: loss: 0.07029843\n",
      "Epoch 566: loss: 0.07016972\n",
      "Epoch 567: loss: 0.0700417\n",
      "Epoch 568: loss: 0.06991444\n",
      "Epoch 569: loss: 0.06978793\n",
      "Epoch 570: loss: 0.069662146\n",
      "Epoch 571: loss: 0.06953708\n",
      "Epoch 572: loss: 0.06941272\n",
      "Epoch 573: loss: 0.06928908\n",
      "Epoch 574: loss: 0.06916616\n",
      "Epoch 575: loss: 0.06904394\n",
      "Epoch 576: loss: 0.06892237\n",
      "Epoch 577: loss: 0.06880153\n",
      "Epoch 578: loss: 0.068681344\n",
      "Epoch 579: loss: 0.06856189\n",
      "Epoch 580: loss: 0.06844307\n",
      "Epoch 581: loss: 0.06832494\n",
      "Epoch 582: loss: 0.068207465\n",
      "Epoch 583: loss: 0.068090655\n",
      "Epoch 584: loss: 0.067974515\n",
      "Epoch 585: loss: 0.067859\n",
      "Epoch 586: loss: 0.06774417\n",
      "Epoch 587: loss: 0.06762995\n",
      "Epoch 588: loss: 0.06751639\n",
      "Epoch 589: loss: 0.067403466\n",
      "Epoch 590: loss: 0.06729115\n",
      "Epoch 591: loss: 0.06717945\n",
      "Epoch 592: loss: 0.06706839\n",
      "Epoch 593: loss: 0.066957936\n",
      "Epoch 594: loss: 0.06684811\n",
      "Epoch 595: loss: 0.06673888\n",
      "Epoch 596: loss: 0.06663025\n",
      "Epoch 597: loss: 0.06652222\n",
      "Epoch 598: loss: 0.066414796\n",
      "Epoch 599: loss: 0.06630794\n",
      "Epoch 600: loss: 0.06620169\n",
      "Epoch 601: loss: 0.06609601\n",
      "Epoch 602: loss: 0.06599093\n",
      "Epoch 603: loss: 0.065886416\n",
      "Epoch 604: loss: 0.06578247\n",
      "Epoch 605: loss: 0.065679096\n",
      "Epoch 606: loss: 0.06557626\n",
      "Epoch 607: loss: 0.065473974\n",
      "Epoch 608: loss: 0.06537229\n",
      "Epoch 609: loss: 0.06527114\n",
      "Epoch 610: loss: 0.06517051\n",
      "Epoch 611: loss: 0.06507047\n",
      "Epoch 612: loss: 0.06497093\n",
      "Epoch 613: loss: 0.06487196\n",
      "Epoch 614: loss: 0.064773485\n",
      "Epoch 615: loss: 0.06467557\n",
      "Epoch 616: loss: 0.06457815\n",
      "Epoch 617: loss: 0.064481296\n",
      "Epoch 618: loss: 0.06438492\n",
      "Epoch 619: loss: 0.06428908\n",
      "Epoch 620: loss: 0.06419375\n",
      "Epoch 621: loss: 0.06409895\n",
      "Epoch 622: loss: 0.06400463\n",
      "Epoch 623: loss: 0.06391079\n",
      "Epoch 624: loss: 0.0638175\n",
      "Epoch 625: loss: 0.06372464\n",
      "Epoch 626: loss: 0.06363236\n",
      "Epoch 627: loss: 0.06354048\n",
      "Epoch 628: loss: 0.06344915\n",
      "Epoch 629: loss: 0.063358255\n",
      "Epoch 630: loss: 0.06326786\n",
      "Epoch 631: loss: 0.06317795\n",
      "Epoch 632: loss: 0.06308849\n",
      "Epoch 633: loss: 0.06299954\n",
      "Epoch 634: loss: 0.06291102\n",
      "Epoch 635: loss: 0.06282298\n",
      "Epoch 636: loss: 0.06273539\n",
      "Epoch 637: loss: 0.06264825\n",
      "Epoch 638: loss: 0.062561594\n",
      "Epoch 639: loss: 0.06247537\n",
      "Epoch 640: loss: 0.06238961\n",
      "Epoch 641: loss: 0.06230427\n",
      "Epoch 642: loss: 0.062219385\n",
      "Epoch 643: loss: 0.06213494\n",
      "Epoch 644: loss: 0.062050946\n",
      "Epoch 645: loss: 0.06196738\n",
      "Epoch 646: loss: 0.061884258\n",
      "Epoch 647: loss: 0.061801553\n",
      "Epoch 648: loss: 0.061719272\n",
      "Epoch 649: loss: 0.061637413\n",
      "Epoch 650: loss: 0.061555997\n",
      "Epoch 651: loss: 0.061474953\n",
      "Epoch 652: loss: 0.06139436\n",
      "Epoch 653: loss: 0.061314188\n",
      "Epoch 654: loss: 0.061234385\n",
      "Epoch 655: loss: 0.061155014\n",
      "Epoch 656: loss: 0.061076056\n",
      "Epoch 657: loss: 0.060997497\n",
      "Epoch 658: loss: 0.060919337\n",
      "Epoch 659: loss: 0.06084158\n",
      "Epoch 660: loss: 0.060764223\n",
      "Epoch 661: loss: 0.06068723\n",
      "Epoch 662: loss: 0.06061067\n",
      "Epoch 663: loss: 0.06053445\n",
      "Epoch 664: loss: 0.06045864\n",
      "Epoch 665: loss: 0.060383238\n",
      "Epoch 666: loss: 0.060308196\n",
      "Epoch 667: loss: 0.06023352\n",
      "Epoch 668: loss: 0.060159214\n",
      "Epoch 669: loss: 0.060085315\n",
      "Epoch 670: loss: 0.060011767\n",
      "Epoch 671: loss: 0.059938595\n",
      "Epoch 672: loss: 0.059865788\n",
      "Epoch 673: loss: 0.05979333\n",
      "Epoch 674: loss: 0.059721272\n",
      "Epoch 675: loss: 0.059649557\n",
      "Epoch 676: loss: 0.059578177\n",
      "Epoch 677: loss: 0.059507146\n",
      "Epoch 678: loss: 0.05943652\n",
      "Epoch 679: loss: 0.059366215\n",
      "Epoch 680: loss: 0.059296265\n",
      "Epoch 681: loss: 0.05922666\n",
      "Epoch 682: loss: 0.05915741\n",
      "Epoch 683: loss: 0.059088472\n",
      "Epoch 684: loss: 0.059019923\n",
      "Epoch 685: loss: 0.05895169\n",
      "Epoch 686: loss: 0.058883775\n",
      "Epoch 687: loss: 0.058816224\n",
      "Epoch 688: loss: 0.05874899\n",
      "Epoch 689: loss: 0.05868208\n",
      "Epoch 690: loss: 0.058615524\n",
      "Epoch 691: loss: 0.058549296\n",
      "Epoch 692: loss: 0.05848335\n",
      "Epoch 693: loss: 0.058417764\n",
      "Epoch 694: loss: 0.05835249\n",
      "Epoch 695: loss: 0.058287524\n",
      "Epoch 696: loss: 0.058222897\n",
      "Epoch 697: loss: 0.05815856\n",
      "Epoch 698: loss: 0.058094546\n",
      "Epoch 699: loss: 0.05803086\n",
      "Epoch 700: loss: 0.05796748\n",
      "Epoch 701: loss: 0.057904396\n",
      "Epoch 702: loss: 0.057841606\n",
      "Epoch 703: loss: 0.057779122\n",
      "Epoch 704: loss: 0.057716966\n",
      "Epoch 705: loss: 0.0576551\n",
      "Epoch 706: loss: 0.05759355\n",
      "Epoch 707: loss: 0.057532266\n",
      "Epoch 708: loss: 0.057471294\n",
      "Epoch 709: loss: 0.05741059\n",
      "Epoch 710: loss: 0.05735023\n",
      "Epoch 711: loss: 0.05729014\n",
      "Epoch 712: loss: 0.0572303\n",
      "Epoch 713: loss: 0.057170786\n",
      "Epoch 714: loss: 0.057111558\n",
      "Epoch 715: loss: 0.057052586\n",
      "Epoch 716: loss: 0.056993905\n",
      "Epoch 717: loss: 0.056935526\n",
      "Epoch 718: loss: 0.056877404\n",
      "Epoch 719: loss: 0.05681956\n",
      "Epoch 720: loss: 0.05676199\n",
      "Epoch 721: loss: 0.05670472\n",
      "Epoch 722: loss: 0.056647677\n",
      "Epoch 723: loss: 0.056590933\n",
      "Epoch 724: loss: 0.056534454\n",
      "Epoch 725: loss: 0.05647826\n",
      "Epoch 726: loss: 0.056422304\n",
      "Epoch 727: loss: 0.056366626\n",
      "Epoch 728: loss: 0.0563112\n",
      "Epoch 729: loss: 0.056256052\n",
      "Epoch 730: loss: 0.056201145\n",
      "Epoch 731: loss: 0.056146506\n",
      "Epoch 732: loss: 0.056092117\n",
      "Epoch 733: loss: 0.056037985\n",
      "Epoch 734: loss: 0.05598412\n",
      "Epoch 735: loss: 0.055930506\n",
      "Epoch 736: loss: 0.055877127\n",
      "Epoch 737: loss: 0.055823993\n",
      "Epoch 738: loss: 0.055771127\n",
      "Epoch 739: loss: 0.05571851\n",
      "Epoch 740: loss: 0.05566611\n",
      "Epoch 741: loss: 0.05561395\n",
      "Epoch 742: loss: 0.055562083\n",
      "Epoch 743: loss: 0.055510405\n",
      "Epoch 744: loss: 0.05545901\n",
      "Epoch 745: loss: 0.05540782\n",
      "Epoch 746: loss: 0.055356883\n",
      "Epoch 747: loss: 0.055306166\n",
      "Epoch 748: loss: 0.05525571\n",
      "Epoch 749: loss: 0.055205446\n",
      "Epoch 750: loss: 0.05515545\n",
      "Epoch 751: loss: 0.055105668\n",
      "Epoch 752: loss: 0.05505613\n",
      "Epoch 753: loss: 0.055006802\n",
      "Epoch 754: loss: 0.054957703\n",
      "Epoch 755: loss: 0.05490885\n",
      "Epoch 756: loss: 0.054860186\n",
      "Epoch 757: loss: 0.05481175\n",
      "Epoch 758: loss: 0.054763544\n",
      "Epoch 759: loss: 0.05471556\n",
      "Epoch 760: loss: 0.054667804\n",
      "Epoch 761: loss: 0.054620266\n",
      "Epoch 762: loss: 0.054572932\n",
      "Epoch 763: loss: 0.05452582\n",
      "Epoch 764: loss: 0.054478906\n",
      "Epoch 765: loss: 0.05443222\n",
      "Epoch 766: loss: 0.054385725\n",
      "Epoch 767: loss: 0.05433948\n",
      "Epoch 768: loss: 0.054293416\n",
      "Epoch 769: loss: 0.054247558\n",
      "Epoch 770: loss: 0.054201927\n",
      "Epoch 771: loss: 0.054156464\n",
      "Epoch 772: loss: 0.054111227\n",
      "Epoch 773: loss: 0.054066215\n",
      "Epoch 774: loss: 0.05402137\n",
      "Epoch 775: loss: 0.05397674\n",
      "Epoch 776: loss: 0.053932324\n",
      "Epoch 777: loss: 0.05388808\n",
      "Epoch 778: loss: 0.053844072\n",
      "Epoch 779: loss: 0.053800214\n",
      "Epoch 780: loss: 0.05375656\n",
      "Epoch 781: loss: 0.053713113\n",
      "Epoch 782: loss: 0.053669885\n",
      "Epoch 783: loss: 0.053626798\n",
      "Epoch 784: loss: 0.053583935\n",
      "Epoch 785: loss: 0.053541273\n",
      "Epoch 786: loss: 0.05349877\n",
      "Epoch 787: loss: 0.05345647\n",
      "Epoch 788: loss: 0.053414352\n",
      "Epoch 789: loss: 0.053372417\n",
      "Epoch 790: loss: 0.05333066\n",
      "Epoch 791: loss: 0.05328912\n",
      "Epoch 792: loss: 0.053247735\n",
      "Epoch 793: loss: 0.05320652\n",
      "Epoch 794: loss: 0.053165503\n",
      "Epoch 795: loss: 0.053124677\n",
      "Epoch 796: loss: 0.053084016\n",
      "Epoch 797: loss: 0.053043533\n",
      "Epoch 798: loss: 0.053003237\n",
      "Epoch 799: loss: 0.05296311\n",
      "Epoch 800: loss: 0.052923147\n",
      "Epoch 801: loss: 0.052883375\n",
      "Epoch 802: loss: 0.052843776\n",
      "Epoch 803: loss: 0.05280433\n",
      "Epoch 804: loss: 0.05276506\n",
      "Epoch 805: loss: 0.052725967\n",
      "Epoch 806: loss: 0.052687075\n",
      "Epoch 807: loss: 0.052648295\n",
      "Epoch 808: loss: 0.05260971\n",
      "Epoch 809: loss: 0.052571304\n",
      "Epoch 810: loss: 0.05253303\n",
      "Epoch 811: loss: 0.052494958\n",
      "Epoch 812: loss: 0.052457027\n",
      "Epoch 813: loss: 0.052419245\n",
      "Epoch 814: loss: 0.05238167\n",
      "Epoch 815: loss: 0.05234424\n",
      "Epoch 816: loss: 0.05230695\n",
      "Epoch 817: loss: 0.052269816\n",
      "Epoch 818: loss: 0.052232865\n",
      "Epoch 819: loss: 0.052196056\n",
      "Epoch 820: loss: 0.052159432\n",
      "Epoch 821: loss: 0.052122906\n",
      "Epoch 822: loss: 0.052086603\n",
      "Epoch 823: loss: 0.052050408\n",
      "Epoch 824: loss: 0.052014396\n",
      "Epoch 825: loss: 0.0519785\n",
      "Epoch 826: loss: 0.051942766\n",
      "Epoch 827: loss: 0.051907204\n",
      "Epoch 828: loss: 0.051871795\n",
      "Epoch 829: loss: 0.0518365\n",
      "Epoch 830: loss: 0.05180141\n",
      "Epoch 831: loss: 0.051766437\n",
      "Epoch 832: loss: 0.05173158\n",
      "Epoch 833: loss: 0.051696915\n",
      "Epoch 834: loss: 0.05166237\n",
      "Epoch 835: loss: 0.05162797\n",
      "Epoch 836: loss: 0.051593725\n",
      "Epoch 837: loss: 0.05155963\n",
      "Epoch 838: loss: 0.051525664\n",
      "Epoch 839: loss: 0.051491816\n",
      "Epoch 840: loss: 0.051458154\n",
      "Epoch 841: loss: 0.05142462\n",
      "Epoch 842: loss: 0.051391218\n",
      "Epoch 843: loss: 0.05135797\n",
      "Epoch 844: loss: 0.051324833\n",
      "Epoch 845: loss: 0.051291853\n",
      "Epoch 846: loss: 0.051258992\n",
      "Epoch 847: loss: 0.05122628\n",
      "Epoch 848: loss: 0.051193703\n",
      "Epoch 849: loss: 0.05116126\n",
      "Epoch 850: loss: 0.051128943\n",
      "Epoch 851: loss: 0.05109678\n",
      "Epoch 852: loss: 0.051064737\n",
      "Epoch 853: loss: 0.051032815\n",
      "Epoch 854: loss: 0.051001016\n",
      "Epoch 855: loss: 0.050969373\n",
      "Epoch 856: loss: 0.050937843\n",
      "Epoch 857: loss: 0.05090644\n",
      "Epoch 858: loss: 0.050875206\n",
      "Epoch 859: loss: 0.050844043\n",
      "Epoch 860: loss: 0.050813023\n",
      "Epoch 861: loss: 0.050782166\n",
      "Epoch 862: loss: 0.05075138\n",
      "Epoch 863: loss: 0.050720748\n",
      "Epoch 864: loss: 0.050690226\n",
      "Epoch 865: loss: 0.050659828\n",
      "Epoch 866: loss: 0.05062958\n",
      "Epoch 867: loss: 0.050599433\n",
      "Epoch 868: loss: 0.050569396\n",
      "Epoch 869: loss: 0.050539512\n",
      "Epoch 870: loss: 0.05050972\n",
      "Epoch 871: loss: 0.050480068\n",
      "Epoch 872: loss: 0.050450534\n",
      "Epoch 873: loss: 0.050421104\n",
      "Epoch 874: loss: 0.050391793\n",
      "Epoch 875: loss: 0.05036259\n",
      "Epoch 876: loss: 0.05033353\n",
      "Epoch 877: loss: 0.05030457\n",
      "Epoch 878: loss: 0.050275728\n",
      "Epoch 879: loss: 0.050247\n",
      "Epoch 880: loss: 0.05021839\n",
      "Epoch 881: loss: 0.050189886\n",
      "Epoch 882: loss: 0.05016148\n",
      "Epoch 883: loss: 0.050133247\n",
      "Epoch 884: loss: 0.05010507\n",
      "Epoch 885: loss: 0.050077006\n",
      "Epoch 886: loss: 0.050049067\n",
      "Epoch 887: loss: 0.050021224\n",
      "Epoch 888: loss: 0.04999352\n",
      "Epoch 889: loss: 0.049965892\n",
      "Epoch 890: loss: 0.049938392\n",
      "Epoch 891: loss: 0.049911\n",
      "Epoch 892: loss: 0.049883705\n",
      "Epoch 893: loss: 0.049856525\n",
      "Epoch 894: loss: 0.049829435\n",
      "Epoch 895: loss: 0.04980249\n",
      "Epoch 896: loss: 0.04977564\n",
      "Epoch 897: loss: 0.049748864\n",
      "Epoch 898: loss: 0.04972219\n",
      "Epoch 899: loss: 0.04969565\n",
      "Epoch 900: loss: 0.049669195\n",
      "Epoch 901: loss: 0.04964287\n",
      "Epoch 902: loss: 0.049616616\n",
      "Epoch 903: loss: 0.049590476\n",
      "Epoch 904: loss: 0.049564403\n",
      "Epoch 905: loss: 0.049538463\n",
      "Epoch 906: loss: 0.049512636\n",
      "Epoch 907: loss: 0.049486894\n",
      "Epoch 908: loss: 0.04946125\n",
      "Epoch 909: loss: 0.04943569\n",
      "Epoch 910: loss: 0.049410254\n",
      "Epoch 911: loss: 0.049384896\n",
      "Epoch 912: loss: 0.04935964\n",
      "Epoch 913: loss: 0.049334493\n",
      "Epoch 914: loss: 0.049309425\n",
      "Epoch 915: loss: 0.049284454\n",
      "Epoch 916: loss: 0.049259588\n",
      "Epoch 917: loss: 0.049234822\n",
      "Epoch 918: loss: 0.049210127\n",
      "Epoch 919: loss: 0.049185526\n",
      "Epoch 920: loss: 0.04916103\n",
      "Epoch 921: loss: 0.04913664\n",
      "Epoch 922: loss: 0.049112324\n",
      "Epoch 923: loss: 0.049088106\n",
      "Epoch 924: loss: 0.049063962\n",
      "Epoch 925: loss: 0.04903992\n",
      "Epoch 926: loss: 0.049015973\n",
      "Epoch 927: loss: 0.04899212\n",
      "Epoch 928: loss: 0.048968356\n",
      "Epoch 929: loss: 0.048944693\n",
      "Epoch 930: loss: 0.048921086\n",
      "Epoch 931: loss: 0.04889757\n",
      "Epoch 932: loss: 0.04887414\n",
      "Epoch 933: loss: 0.048850827\n",
      "Epoch 934: loss: 0.048827585\n",
      "Epoch 935: loss: 0.048804406\n",
      "Epoch 936: loss: 0.048781354\n",
      "Epoch 937: loss: 0.048758358\n",
      "Epoch 938: loss: 0.048735473\n",
      "Epoch 939: loss: 0.048712656\n",
      "Epoch 940: loss: 0.048689917\n",
      "Epoch 941: loss: 0.048667267\n",
      "Epoch 942: loss: 0.04864469\n",
      "Epoch 943: loss: 0.04862223\n",
      "Epoch 944: loss: 0.048599806\n",
      "Epoch 945: loss: 0.04857751\n",
      "Epoch 946: loss: 0.04855526\n",
      "Epoch 947: loss: 0.048533093\n",
      "Epoch 948: loss: 0.048511006\n",
      "Epoch 949: loss: 0.048489034\n",
      "Epoch 950: loss: 0.048467126\n",
      "Epoch 951: loss: 0.048445296\n",
      "Epoch 952: loss: 0.048423536\n",
      "Epoch 953: loss: 0.04840185\n",
      "Epoch 954: loss: 0.04838024\n",
      "Epoch 955: loss: 0.04835872\n",
      "Epoch 956: loss: 0.048337307\n",
      "Epoch 957: loss: 0.048315927\n",
      "Epoch 958: loss: 0.04829464\n",
      "Epoch 959: loss: 0.048273407\n",
      "Epoch 960: loss: 0.048252277\n",
      "Epoch 961: loss: 0.048231248\n",
      "Epoch 962: loss: 0.048210263\n",
      "Epoch 963: loss: 0.04818934\n",
      "Epoch 964: loss: 0.048168495\n",
      "Epoch 965: loss: 0.048147734\n",
      "Epoch 966: loss: 0.048127055\n",
      "Epoch 967: loss: 0.048106432\n",
      "Epoch 968: loss: 0.0480859\n",
      "Epoch 969: loss: 0.04806544\n",
      "Epoch 970: loss: 0.048045028\n",
      "Epoch 971: loss: 0.04802473\n",
      "Epoch 972: loss: 0.048004463\n",
      "Epoch 973: loss: 0.047984242\n",
      "Epoch 974: loss: 0.047964152\n",
      "Epoch 975: loss: 0.047944125\n",
      "Epoch 976: loss: 0.04792416\n",
      "Epoch 977: loss: 0.04790424\n",
      "Epoch 978: loss: 0.04788441\n",
      "Epoch 979: loss: 0.047864664\n",
      "Epoch 980: loss: 0.04784499\n",
      "Epoch 981: loss: 0.047825355\n",
      "Epoch 982: loss: 0.04780579\n",
      "Epoch 983: loss: 0.04778634\n",
      "Epoch 984: loss: 0.047766924\n",
      "Epoch 985: loss: 0.047747567\n",
      "Epoch 986: loss: 0.047728285\n",
      "Epoch 987: loss: 0.047709063\n",
      "Epoch 988: loss: 0.047689904\n",
      "Epoch 989: loss: 0.047670834\n",
      "Epoch 990: loss: 0.047651824\n",
      "Epoch 991: loss: 0.047632866\n",
      "Epoch 992: loss: 0.047613997\n",
      "Epoch 993: loss: 0.047595147\n",
      "Epoch 994: loss: 0.04757641\n",
      "Epoch 995: loss: 0.04755769\n",
      "Epoch 996: loss: 0.047539085\n",
      "Epoch 997: loss: 0.047520515\n",
      "Epoch 998: loss: 0.047502007\n",
      "Epoch 999: loss: 0.04748357\n",
      "Epoch 1000: loss: 0.047465205\n",
      "Accuracy: 98.66666666666667 %\n",
      "Training indices: [63, 56, 97, 31, 85, 43, 24, 47, 99, 86, 3, 70, 32, 41, 50, 95, 53, 55, 92, 48, 23, 93, 29, 60, 52, 15, 59, 82, 76, 65, 74, 54, 62, 1, 33, 30, 35, 34, 11, 81, 58, 84, 90, 68, 2, 72, 91, 83, 51, 18, 39, 42, 28, 14, 21, 22, 80, 77, 10, 78, 46, 6, 98, 75, 9, 88, 8, 67, 96, 73, 94, 17, 87, 27, 13, 7, 66, 38, 20, 37]\n",
      "Testing indices: [4, 5, 69, 26, 40, 100, 25, 19, 89, 49, 16, 57, 64, 45, 61, 12, 36, 71, 44, 79]\n",
      "Training indices: [12, 33, 63, 61, 14, 96, 69, 54, 3, 80, 26, 78, 29, 95, 34, 67, 66, 62, 30, 39, 31, 41, 28, 37, 36, 45, 52, 56, 5, 1, 23, 85, 32, 79, 89, 76, 70, 15, 71, 68, 21, 100, 47, 35, 81, 24, 43, 77, 4, 88, 16, 53, 57, 60, 91, 50, 22, 51, 9, 99, 72, 6, 82, 49, 13, 94, 93, 87, 10, 25]\n",
      "Validation indices: [98, 44, 84, 75, 92, 90, 20, 38, 40, 27]\n",
      "Testing indices: [11, 58, 64, 8, 59, 97, 55, 42, 46, 86, 48, 83, 17, 19, 65, 74, 73, 2, 7, 18]\n",
      "Training indices: [72, 15, 78, 26, 86, 38, 9, 47, 67, 76, 21, 100, 5, 23, 39, 55, 71, 95, 81, 75, 44, 27, 66, 87, 68, 45, 42, 31, 8, 91, 59, 14, 64, 49, 6, 3, 18, 63, 83, 93, 84, 92, 35, 74, 73, 40, 50, 62, 88, 79, 33, 12, 17, 96, 20, 36, 46, 16, 13, 34, 94, 56, 60, 25, 43, 80, 70, 85, 82, 98]\n",
      "Validation indices: [54, 30, 57, 37, 90, 29, 11, 22, 19, 52]\n",
      "Test indices: [69, 4, 24, 41, 99, 2, 1, 65, 28, 32, 61, 48, 10, 51, 89, 77, 58, 7, 97, 53]\n",
      "Vector{Int64}\n",
      "(105, 4)\n",
      "(105, 3)\n",
      "Epoch 0: Training Loss: 1.3211436 | Validation Loss: 1.3211436 | Test Loss: 1.1201717\n",
      "Epoch 911: Training Loss: 0.03382924 | Validation Loss: 0.03382924 | Test Loss: 0.14462422\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "Float32[0.9946957 0.004997472 0.00030693557; 0.9940996 0.0055703497 0.0003299997; 0.010011548 0.98718226 0.0028062498; 6.782504f-5 0.002017206 0.99791497; 0.994372 0.005308387 0.00031953305; 0.007453454 0.9890702 0.0034763494; 0.0068781837 0.98777956 0.0053423047; 3.8603077f-5 0.0011171418 0.99884427; 0.9946688 0.0050232154 0.0003079796; 0.9943831 0.0052977526 0.0003191094; 0.00012269882 0.0038760805 0.9960012; 0.9945821 0.0051065013 0.00031136954; 0.00946504 0.98765403 0.0028808971; 0.99475455 0.0049407594 0.00030459635; 2.7244503f-5 0.0007873856 0.9991854; 0.99432087 0.005357616 0.0003215148; 0.99481636 0.004881478 0.00030215862; 0.00013909207 0.004490313 0.99537057; 0.9945251 0.005161331 0.00031361383; 0.994639 0.0050518676 0.00030916187; 2.0991465f-5 0.0006157878 0.99936324; 0.9941274 0.0055437027 0.00032891927; 0.99455243 0.0051350268 0.0003125389; 3.530409f-5 0.0010172704 0.99894744; 4.1477484f-5 0.0011976275 0.99876094; 0.0046088933 0.30754322 0.6878479; 0.0074583814 0.988841 0.0037006887; 0.9951592 0.0045523443 0.0002883807; 0.007584306 0.9890126 0.003403038; 0.000101116915 0.0031456954 0.99675316; 0.9945821 0.0051065013 0.00031136954; 0.006851842 0.9860576 0.007090551; 3.236855f-5 0.00093394046 0.9990337; 0.994228 0.005446917 0.0003251087; 0.99215806 0.0074416962 0.00040023468; 0.0003281233 0.01218772 0.98748416; 0.99438393 0.00529691 0.00031907708; 0.00082706864 0.036405027 0.9627679; 0.008148693 0.9886778 0.003173514; 0.0068571568 0.98397875 0.009164039; 0.010238978 0.98692644 0.0028345706; 0.00013909207 0.004490313 0.99537057; 2.5901338f-5 0.00074962695 0.9992244; 0.9945634 0.0051244358 0.00031212892; 0.9947301 0.0049642487 0.000305572; 0.0054002227 0.39384273 0.6007571; 0.010450683 0.9867519 0.002797369; 0.006844882 0.9868101 0.006344987; 0.0024395634 0.14166942 0.855891; 0.011461303 0.9857268 0.0028118473; 0.012570531 0.98456025 0.0028691916; 0.0017577283 0.09072904 0.90751326; 0.006774483 0.98371834 0.009507221; 0.9945821 0.0051065013 0.00031136954; 0.99501145 0.004694134 0.00029435699; 0.0072870604 0.98880553 0.0039074034; 0.001661189 0.084695235 0.9136436; 0.0070710434 0.9878099 0.0051190434; 0.99484384 0.004855031 0.0003010762; 0.9945503 0.005137017 0.00031261038; 0.0068904557 0.98600185 0.007107684; 4.7384896f-5 0.0013728847 0.99857974; 0.9947325 0.0049619786 0.00030547017; 0.0034094008 0.21482782 0.7817627; 3.7703805f-5 0.0010903167 0.998872; 0.00034745922 0.012772088 0.9868804; 0.010002371 0.9872236 0.002774027; 0.0069084466 0.984503 0.008588637; 0.99515027 0.0045610266 0.000288754; 0.00013288818 0.004244553 0.9956226; 0.00024626093 0.008703428 0.99105036; 0.99480313 0.004894202 0.0003026772; 2.8442808f-5 0.0008229807 0.9991486; 0.99400735 0.005659196 0.00033348825; 3.2511318f-5 0.0009358341 0.99903166; 0.008154658 0.98870915 0.0031362; 0.0032979175 0.20096646 0.7957356; 0.99436474 0.0053154984 0.00031984382; 0.007099987 0.9881725 0.004727555; 0.0006965785 0.029606562 0.9696969; 0.00705921 0.9883161 0.0046246825; 2.726549f-5 0.00078785926 0.9991849; 0.99462897 0.00506142 0.00030956088; 0.008775705 0.9882762 0.0029480862; 4.0980103f-5 0.0011858834 0.99877316; 0.99453485 0.005151925 0.00031322267; 3.0693485f-5 0.0008859921 0.9990833; 0.007168156 0.9882768 0.0045550535; 0.9948933 0.0048075677 0.00029908956; 0.9948664 0.0048333975 0.00030016043; 0.007830959 0.9887051 0.0034639505; 2.6557485f-5 0.00076786085 0.99920565; 0.007250543 0.9888467 0.0039027652; 0.007863544 0.75624406 0.23589237; 0.99489105 0.004809701 0.0002991776; 0.99504095 0.004665789 0.0002931823; 0.007868442 0.6940728 0.29805884; 0.9948602 0.004839481 0.00030041626; 2.5694757f-5 0.0007447916 0.99922955; 0.007212758 0.9888465 0.0039408086; 0.008679162 0.9882918 0.003029042; 0.006880474 0.9830667 0.0100528775; 0.9934558 0.0061900285 0.00035409178; 0.009342652 0.98782223 0.0028351715; 0.036223967 0.9601898 0.0035862685]\n",
      "Bool[1 0 0; 1 0 0; 0 1 0; 0 0 1; 1 0 0; 0 1 0; 0 1 0; 0 0 1; 1 0 0; 1 0 0; 0 0 1; 1 0 0; 0 1 0; 1 0 0; 0 0 1; 1 0 0; 1 0 0; 0 0 1; 1 0 0; 1 0 0; 0 0 1; 1 0 0; 1 0 0; 0 0 1; 0 0 1; 0 0 1; 0 1 0; 1 0 0; 0 1 0; 0 0 1; 1 0 0; 0 1 0; 0 0 1; 1 0 0; 1 0 0; 0 0 1; 1 0 0; 0 0 1; 0 1 0; 0 1 0; 0 1 0; 0 0 1; 0 0 1; 1 0 0; 1 0 0; 0 1 0; 0 1 0; 0 1 0; 0 0 1; 0 1 0; 0 1 0; 0 0 1; 0 1 0; 1 0 0; 1 0 0; 0 1 0; 0 0 1; 0 1 0; 1 0 0; 1 0 0; 0 1 0; 0 0 1; 1 0 0; 0 0 1; 0 0 1; 0 0 1; 0 1 0; 0 1 0; 1 0 0; 0 0 1; 0 0 1; 1 0 0; 0 0 1; 1 0 0; 0 0 1; 0 1 0; 0 0 1; 1 0 0; 0 1 0; 0 0 1; 0 1 0; 0 0 1; 1 0 0; 0 1 0; 0 0 1; 1 0 0; 0 0 1; 0 1 0; 1 0 0; 1 0 0; 0 1 0; 0 0 1; 0 1 0; 0 1 0; 1 0 0; 1 0 0; 0 1 0; 1 0 0; 0 0 1; 0 1 0; 0 1 0; 0 1 0; 1 0 0; 0 1 0; 0 1 0]\n",
      "Accuracy: 99.04761904761905 %\n",
      "LinearAlgebra.Adjoint{Float32, Matrix{Float32}}\n",
      "Matrix{Float32}\n",
      "BitMatrix\n",
      "Matrix{Float32}\n",
      "BitMatrix\n",
      "Epoch 0: Training Loss: 1.1998857 | Validation Loss: 1.0243603 | Test Loss: 1.2213402\n",
      "Epoch 20: Training Loss: 1.0921404 | Validation Loss: 1.1002948 | Test Loss: 1.096634\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "Accuracy: 36.19047619047619 %\n",
      "Epoch 0: Training Loss: 1.1428121 | Validation Loss: 1.1845357 | Test Loss: 1.0978392\n",
      "Epoch 677: Training Loss: 0.06666402 | Validation Loss: 0.018529503 | Test Loss: 0.036580577\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "Accuracy: 98.09523809523809 %\n",
      "Epoch 0: Training Loss: 1.1789513 | Validation Loss: 1.3359278 | Test Loss: 1.1964877\n",
      "Epoch 587: Training Loss: 0.048553545 | Validation Loss: 0.01488047 | Test Loss: 0.11174233\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "Accuracy: 99.04761904761905 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vector: [-1.0, -1.0, -0.2]\n",
      "Output vector (softmax): [0.23665609135556676, 0.23665609135556676, 0.5266878172888664]\n",
      "num_elements50\n",
      "subsets1[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "subsets2[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "subsets3[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "class_indices[6, 8, 5, 9, 7, 2, 9, 5, 10, 4, 7, 1, 3, 2, 8, 6, 3, 6, 8, 4, 10, 4, 9, 9, 10, 1, 2, 10, 6, 4, 5, 8, 8, 7, 2, 9, 1, 3, 3, 7, 5, 3, 1, 2, 1, 6, 7, 4, 10, 5]\n",
      "(50,)\n",
      "indices[6, 8, 5, 9, 7, 2, 9, 5, 10, 4, 7, 1, 3, 2, 8, 6, 3, 6, 8, 4, 10, 4, 9, 9, 10, 1, 2, 10, 6, 4, 5, 8, 8, 7, 2, 9, 1, 3, 3, 7, 5, 3, 1, 2, 1, 6, 7, 4, 10, 5, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150]\n",
      "num_elements50\n",
      "subsets1[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "subsets2[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "subsets3[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "class_indices[7, 7, 3, 9, 9, 9, 9, 9, 2, 2, 1, 10, 4, 4, 7, 7, 8, 3, 3, 6, 5, 8, 1, 5, 10, 1, 5, 6, 7, 3, 4, 6, 6, 5, 1, 1, 10, 4, 10, 4, 8, 6, 2, 8, 3, 2, 10, 5, 2, 8]\n",
      "(50,)\n",
      "indices[6, 8, 5, 9, 7, 2, 9, 5, 10, 4, 7, 1, 3, 2, 8, 6, 3, 6, 8, 4, 10, 4, 9, 9, 10, 1, 2, 10, 6, 4, 5, 8, 8, 7, 2, 9, 1, 3, 3, 7, 5, 3, 1, 2, 1, 6, 7, 4, 10, 5, 7, 7, 3, 9, 9, 9, 9, 9, 2, 2, 1, 10, 4, 4, 7, 7, 8, 3, 3, 6, 5, 8, 1, 5, 10, 1, 5, 6, 7, 3, 4, 6, 6, 5, 1, 1, 10, 4, 10, 4, 8, 6, 2, 8, 3, 2, 10, 5, 2, 8, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150]\n",
      "num_elements50\n",
      "subsets1[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "subsets2[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "subsets3[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "class_indices[4, 4, 10, 5, 10, 8, 6, 3, 5, 6, 4, 6, 6, 5, 7, 3, 2, 10, 9, 10, 7, 9, 5, 2, 1, 7, 6, 8, 2, 2, 9, 8, 1, 4, 9, 1, 7, 1, 3, 7, 1, 9, 8, 5, 3, 8, 4, 2, 10, 3]\n",
      "(50,)\n",
      "indices[6, 8, 5, 9, 7, 2, 9, 5, 10, 4, 7, 1, 3, 2, 8, 6, 3, 6, 8, 4, 10, 4, 9, 9, 10, 1, 2, 10, 6, 4, 5, 8, 8, 7, 2, 9, 1, 3, 3, 7, 5, 3, 1, 2, 1, 6, 7, 4, 10, 5, 7, 7, 3, 9, 9, 9, 9, 9, 2, 2, 1, 10, 4, 4, 7, 7, 8, 3, 3, 6, 5, 8, 1, 5, 10, 1, 5, 6, 7, 3, 4, 6, 6, 5, 1, 1, 10, 4, 10, 4, 8, 6, 2, 8, 3, 2, 10, 5, 2, 8, 4, 4, 10, 5, 10, 8, 6, 3, 5, 6, 4, 6, 6, 5, 7, 3, 2, 10, 9, 10, 7, 9, 5, 2, 1, 7, 6, 8, 2, 2, 9, 8, 1, 4, 9, 1, 7, 1, 3, 7, 1, 9, 8, 5, 3, 8, 4, 2, 10, 3]\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.3083318 | Validation Loss: 1.3083318 | Test Loss: 1.1905841\n",
      "Epoch 756: Training Loss: 0.06060055 | Validation Loss: 0.06060055 | Test Loss: 0.01741102\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.2024398 | Validation Loss: 1.2024398 | Test Loss: 1.1414577\n",
      "Epoch 709: Training Loss: 0.058927078 | Validation Loss: 0.058927078 | Test Loss: 0.018196046\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.0968312 | Validation Loss: 1.0968312 | Test Loss: 1.0862504\n",
      "Epoch 570: Training Loss: 0.055045288 | Validation Loss: 0.055045288 | Test Loss: 0.011888542\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.2460363 | Validation Loss: 1.2460363 | Test Loss: 1.2460424\n",
      "Epoch 765: Training Loss: 0.056679077 | Validation Loss: 0.056679077 | Test Loss: 0.01568369\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1577435 | Validation Loss: 1.1577435 | Test Loss: 1.3000575\n",
      "Epoch 649: Training Loss: 0.056264993 | Validation Loss: 0.056264993 | Test Loss: 0.017688977\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.12697 | Validation Loss: 1.12697 | Test Loss: 1.227244\n",
      "Epoch 670: Training Loss: 0.055888634 | Validation Loss: 0.055888634 | Test Loss: 0.014880314\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.2043089 | Validation Loss: 1.2043089 | Test Loss: 1.2704238\n",
      "Epoch 633: Training Loss: 0.056879513 | Validation Loss: 0.056879513 | Test Loss: 0.016695613\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1692953 | Validation Loss: 1.1692953 | Test Loss: 1.2143714\n",
      "Epoch 54: Training Loss: 1.0970579 | Validation Loss: 1.0970579 | Test Loss: 1.1127018\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.5538667 | Validation Loss: 1.5538667 | Test Loss: 1.9331983\n",
      "Epoch 67: Training Loss: 1.097793 | Validation Loss: 1.097793 | Test Loss: 1.1031972\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1097217 | Validation Loss: 1.1097217 | Test Loss: 1.092509\n",
      "Epoch 625: Training Loss: 0.05378153 | Validation Loss: 0.05378153 | Test Loss: 0.0140798725\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.2318327 | Validation Loss: 1.2318327 | Test Loss: 1.2147331\n",
      "Epoch 795: Training Loss: 0.032347236 | Validation Loss: 0.032347236 | Test Loss: 0.2600411\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.2202879 | Validation Loss: 1.2202879 | Test Loss: 1.1375484\n",
      "Epoch 846: Training Loss: 0.032040335 | Validation Loss: 0.032040335 | Test Loss: 0.26985475\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.2328612 | Validation Loss: 1.2328612 | Test Loss: 1.1796546\n",
      "Epoch 373: Training Loss: 0.47008747 | Validation Loss: 0.47008747 | Test Loss: 0.47331005\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1741216 | Validation Loss: 1.1741216 | Test Loss: 1.2122707\n",
      "Epoch 717: Training Loss: 0.03295741 | Validation Loss: 0.03295741 | Test Loss: 0.2425363\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1165745 | Validation Loss: 1.1165745 | Test Loss: 1.1192671\n",
      "Epoch 676: Training Loss: 0.03164435 | Validation Loss: 0.03164435 | Test Loss: 0.23844363\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1101177 | Validation Loss: 1.1101177 | Test Loss: 1.0937659\n",
      "Epoch 690: Training Loss: 0.031746782 | Validation Loss: 0.031746782 | Test Loss: 0.22741215\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1510328 | Validation Loss: 1.1510328 | Test Loss: 1.2109684\n",
      "Epoch 782: Training Loss: 0.031856976 | Validation Loss: 0.031856976 | Test Loss: 0.26586494\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.2461716 | Validation Loss: 1.2461716 | Test Loss: 1.334551\n",
      "Epoch 35: Training Loss: 1.0996093 | Validation Loss: 1.0996093 | Test Loss: 1.094305\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.106792 | Validation Loss: 1.106792 | Test Loss: 1.0878941\n",
      "Epoch 671: Training Loss: 0.031517867 | Validation Loss: 0.031517867 | Test Loss: 0.23549977\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.324223 | Validation Loss: 1.324223 | Test Loss: 1.351188\n",
      "Epoch 851: Training Loss: 0.03229015 | Validation Loss: 0.03229015 | Test Loss: 0.27139992\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1461824 | Validation Loss: 1.1461824 | Test Loss: 1.2289379\n",
      "Epoch 611: Training Loss: 0.056937933 | Validation Loss: 0.056937933 | Test Loss: 0.03829668\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1184094 | Validation Loss: 1.1184094 | Test Loss: 1.2357119\n",
      "Epoch 624: Training Loss: 0.05422131 | Validation Loss: 0.05422131 | Test Loss: 0.045970105\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1071457 | Validation Loss: 1.1071457 | Test Loss: 1.0639598\n",
      "Epoch 574: Training Loss: 0.054009102 | Validation Loss: 0.054009102 | Test Loss: 0.044512913\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1471405 | Validation Loss: 1.1471405 | Test Loss: 1.3124919\n",
      "Epoch 593: Training Loss: 0.05587663 | Validation Loss: 0.05587663 | Test Loss: 0.040342614\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1727997 | Validation Loss: 1.1727997 | Test Loss: 1.1520317\n",
      "Epoch 36: Training Loss: 1.0966427 | Validation Loss: 1.0966427 | Test Loss: 1.1193721\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.249768 | Validation Loss: 1.249768 | Test Loss: 1.0058333\n",
      "Epoch 686: Training Loss: 0.05584439 | Validation Loss: 0.05584439 | Test Loss: 0.057137966\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1605026 | Validation Loss: 1.1605026 | Test Loss: 1.3025115\n",
      "Epoch 670: Training Loss: 0.055922482 | Validation Loss: 0.055922482 | Test Loss: 0.057795532\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.2677146 | Validation Loss: 1.2677146 | Test Loss: 1.443194\n",
      "Epoch 740: Training Loss: 0.05552132 | Validation Loss: 0.05552132 | Test Loss: 0.058224\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.2696717 | Validation Loss: 1.2696717 | Test Loss: 1.1604387\n",
      "Epoch 708: Training Loss: 0.05682232 | Validation Loss: 0.05682232 | Test Loss: 0.0560297\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1269877 | Validation Loss: 1.1269877 | Test Loss: 1.0528901\n",
      "Epoch 614: Training Loss: 0.055608522 | Validation Loss: 0.055608522 | Test Loss: 0.046944465\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.3896372 | Validation Loss: 1.3896372 | Test Loss: 1.2786129\n",
      "Epoch 51: Training Loss: 1.0976652 | Validation Loss: 1.0976652 | Test Loss: 1.1067864\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.121781 | Validation Loss: 1.121781 | Test Loss: 1.089868\n",
      "Epoch 601: Training Loss: 0.05329508 | Validation Loss: 0.05329508 | Test Loss: 0.06220891\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1472924 | Validation Loss: 1.1472924 | Test Loss: 1.1955196\n",
      "Epoch 32: Training Loss: 1.0986593 | Validation Loss: 1.0986593 | Test Loss: 1.0959204\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1601756 | Validation Loss: 1.1601756 | Test Loss: 1.1052055\n",
      "Epoch 675: Training Loss: 0.053007025 | Validation Loss: 0.053007025 | Test Loss: 0.065815955\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.346938 | Validation Loss: 1.346938 | Test Loss: 1.2147446\n",
      "Epoch 687: Training Loss: 0.05421638 | Validation Loss: 0.05421638 | Test Loss: 0.05682218\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1653986 | Validation Loss: 1.1653986 | Test Loss: 1.121479\n",
      "Epoch 629: Training Loss: 0.05430476 | Validation Loss: 0.05430476 | Test Loss: 0.056249674\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1007588 | Validation Loss: 1.1007588 | Test Loss: 1.1155744\n",
      "Epoch 579: Training Loss: 0.052253336 | Validation Loss: 0.052253336 | Test Loss: 0.059750624\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1288674 | Validation Loss: 1.1288674 | Test Loss: 1.129852\n",
      "Epoch 625: Training Loss: 0.05212822 | Validation Loss: 0.05212822 | Test Loss: 0.0585852\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.117116 | Validation Loss: 1.117116 | Test Loss: 1.156653\n",
      "Epoch 579: Training Loss: 0.05145894 | Validation Loss: 0.05145894 | Test Loss: 0.05430908\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.5797918 | Validation Loss: 1.5797918 | Test Loss: 1.4245871\n",
      "Epoch 990: Training Loss: 0.055407252 | Validation Loss: 0.055407252 | Test Loss: 0.07627151\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1746747 | Validation Loss: 1.1746747 | Test Loss: 1.2496163\n",
      "Epoch 967: Training Loss: 0.014514817 | Validation Loss: 0.014514817 | Test Loss: 0.31805533\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1178591 | Validation Loss: 1.1178591 | Test Loss: 1.0894574\n",
      "Epoch 727: Training Loss: 0.024910929 | Validation Loss: 0.024910929 | Test Loss: 0.319582\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1155962 | Validation Loss: 1.1155962 | Test Loss: 1.0880755\n",
      "Epoch 935: Training Loss: 0.016205875 | Validation Loss: 0.016205875 | Test Loss: 0.30558708\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.12571 | Validation Loss: 1.12571 | Test Loss: 1.168617\n",
      "Epoch 788: Training Loss: 0.025226418 | Validation Loss: 0.025226418 | Test Loss: 0.28430542\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.2573909 | Validation Loss: 1.2573909 | Test Loss: 1.3300308\n",
      "Epoch 1000: Training Loss: 0.013357884 | Validation Loss: 0.013357884 | Test Loss: 0.36955154\n",
      "Max Epochs\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.5621592 | Validation Loss: 1.5621592 | Test Loss: 1.4400088\n",
      "Epoch 1000: Training Loss: 0.025733622 | Validation Loss: 0.025733622 | Test Loss: 0.29814574\n",
      "Max Epochs\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1098657 | Validation Loss: 1.1098657 | Test Loss: 1.1303512\n",
      "Epoch 833: Training Loss: 0.020797178 | Validation Loss: 0.020797178 | Test Loss: 0.3015527\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.3229457 | Validation Loss: 1.3229457 | Test Loss: 1.2299434\n",
      "Epoch 1000: Training Loss: 0.019945843 | Validation Loss: 0.019945843 | Test Loss: 0.3043686\n",
      "Max Epochs\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1969051 | Validation Loss: 1.1969051 | Test Loss: 1.234272\n",
      "Epoch 903: Training Loss: 0.018007645 | Validation Loss: 0.018007645 | Test Loss: 0.30529454\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1532326 | Validation Loss: 1.1532326 | Test Loss: 1.2112976\n",
      "Epoch 819: Training Loss: 0.02375945 | Validation Loss: 0.02375945 | Test Loss: 0.27957866\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.2475126 | Validation Loss: 1.2475126 | Test Loss: 1.2027094\n",
      "Epoch 46: Training Loss: 1.0983067 | Validation Loss: 1.0983067 | Test Loss: 1.1223565\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.320095 | Validation Loss: 1.320095 | Test Loss: 1.0479773\n",
      "Epoch 48: Training Loss: 1.0976387 | Validation Loss: 1.0976387 | Test Loss: 1.1394105\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1339895 | Validation Loss: 1.1339895 | Test Loss: 1.079211\n",
      "Epoch 30: Training Loss: 1.0970184 | Validation Loss: 1.0970184 | Test Loss: 1.1155542\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.2536343 | Validation Loss: 1.2536343 | Test Loss: 1.283947\n",
      "Epoch 763: Training Loss: 0.055077597 | Validation Loss: 0.055077597 | Test Loss: 0.059651025\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.4938239 | Validation Loss: 1.4938239 | Test Loss: 1.1568763\n",
      "Epoch 58: Training Loss: 1.0962623 | Validation Loss: 1.0962623 | Test Loss: 1.1299608\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1276331 | Validation Loss: 1.1276331 | Test Loss: 1.0360034\n",
      "Epoch 650: Training Loss: 0.054643046 | Validation Loss: 0.054643046 | Test Loss: 0.0576079\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1021898 | Validation Loss: 1.1021898 | Test Loss: 1.1366578\n",
      "Epoch 603: Training Loss: 0.051936075 | Validation Loss: 0.051936075 | Test Loss: 0.05831363\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.4460363 | Validation Loss: 1.4460363 | Test Loss: 1.7001773\n",
      "Epoch 825: Training Loss: 0.058133457 | Validation Loss: 0.058133457 | Test Loss: 0.05646174\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1447787 | Validation Loss: 1.1447787 | Test Loss: 1.2783127\n",
      "Epoch 33: Training Loss: 1.096828 | Validation Loss: 1.096828 | Test Loss: 1.1168247\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1224416 | Validation Loss: 1.1224416 | Test Loss: 1.1835533\n",
      "Epoch 632: Training Loss: 0.05353596 | Validation Loss: 0.05353596 | Test Loss: 0.05948197\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.2053114 | Validation Loss: 1.2053114 | Test Loss: 1.1568414\n",
      "Epoch 40: Training Loss: 1.0984751 | Validation Loss: 1.0984751 | Test Loss: 1.1056435\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1849672 | Validation Loss: 1.1849672 | Test Loss: 1.2170064\n",
      "Epoch 636: Training Loss: 0.057568017 | Validation Loss: 0.057568017 | Test Loss: 0.027126838\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1241236 | Validation Loss: 1.1241236 | Test Loss: 1.1471429\n",
      "Epoch 613: Training Loss: 0.05605036 | Validation Loss: 0.05605036 | Test Loss: 0.027033122\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1734188 | Validation Loss: 1.1734188 | Test Loss: 1.1959742\n",
      "Epoch 613: Training Loss: 0.057802487 | Validation Loss: 0.057802487 | Test Loss: 0.027544001\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1932696 | Validation Loss: 1.1932696 | Test Loss: 1.1278619\n",
      "Epoch 667: Training Loss: 0.057280395 | Validation Loss: 0.057280395 | Test Loss: 0.026638966\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.2217082 | Validation Loss: 1.2217082 | Test Loss: 1.1432879\n",
      "Epoch 730: Training Loss: 0.05677381 | Validation Loss: 0.05677381 | Test Loss: 0.02824076\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.2061039 | Validation Loss: 1.2061039 | Test Loss: 1.1813428\n",
      "Epoch 671: Training Loss: 0.055874962 | Validation Loss: 0.055874962 | Test Loss: 0.029508587\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1846093 | Validation Loss: 1.1846093 | Test Loss: 1.1224135\n",
      "Epoch 633: Training Loss: 0.05552601 | Validation Loss: 0.05552601 | Test Loss: 0.025065413\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.2049016 | Validation Loss: 1.2049016 | Test Loss: 1.2326915\n",
      "Epoch 37: Training Loss: 1.0989169 | Validation Loss: 1.0989169 | Test Loss: 1.0969009\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1764861 | Validation Loss: 1.1764861 | Test Loss: 1.1701432\n",
      "Epoch 669: Training Loss: 0.05687046 | Validation Loss: 0.05687046 | Test Loss: 0.02362167\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1324792 | Validation Loss: 1.1324792 | Test Loss: 1.1777096\n",
      "Epoch 663: Training Loss: 0.057993468 | Validation Loss: 0.057993468 | Test Loss: 0.023075767\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.2106459 | Validation Loss: 1.2106459 | Test Loss: 1.1586736\n",
      "Epoch 748: Training Loss: 0.057851925 | Validation Loss: 0.057851925 | Test Loss: 0.022864839\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1921232 | Validation Loss: 1.1921232 | Test Loss: 1.174324\n",
      "Epoch 708: Training Loss: 0.05820109 | Validation Loss: 0.05820109 | Test Loss: 0.02303101\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1370965 | Validation Loss: 1.1370965 | Test Loss: 1.1829778\n",
      "Epoch 659: Training Loss: 0.056995094 | Validation Loss: 0.056995094 | Test Loss: 0.021849493\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1314119 | Validation Loss: 1.1314119 | Test Loss: 1.1759325\n",
      "Epoch 681: Training Loss: 0.056983035 | Validation Loss: 0.056983035 | Test Loss: 0.019598108\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1308182 | Validation Loss: 1.1308182 | Test Loss: 1.1043661\n",
      "Epoch 48: Training Loss: 1.0975683 | Validation Loss: 1.0975683 | Test Loss: 1.1008365\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1971006 | Validation Loss: 1.1971006 | Test Loss: 1.1545913\n",
      "Epoch 655: Training Loss: 0.057005584 | Validation Loss: 0.057005584 | Test Loss: 0.023541082\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.240834 | Validation Loss: 1.240834 | Test Loss: 1.3452924\n",
      "Epoch 390: Training Loss: 0.47568557 | Validation Loss: 0.47568557 | Test Loss: 0.4258105\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1799204 | Validation Loss: 1.1799204 | Test Loss: 1.1367759\n",
      "Epoch 690: Training Loss: 0.05763173 | Validation Loss: 0.05763173 | Test Loss: 0.01959485\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1695614 | Validation Loss: 1.1695614 | Test Loss: 1.1744407\n",
      "Epoch 657: Training Loss: 0.057772283 | Validation Loss: 0.057772283 | Test Loss: 0.023051612\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1578166 | Validation Loss: 1.1578166 | Test Loss: 1.2204236\n",
      "Epoch 634: Training Loss: 0.0558389 | Validation Loss: 0.0558389 | Test Loss: 0.016047962\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.101772 | Validation Loss: 1.101772 | Test Loss: 1.0950335\n",
      "Epoch 617: Training Loss: 0.05471363 | Validation Loss: 0.05471363 | Test Loss: 0.014476622\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1370584 | Validation Loss: 1.1370584 | Test Loss: 1.088722\n",
      "Epoch 31: Training Loss: 1.0968312 | Validation Loss: 1.0968312 | Test Loss: 1.128257\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1071401 | Validation Loss: 1.1071401 | Test Loss: 1.093866\n",
      "Epoch 614: Training Loss: 0.055156443 | Validation Loss: 0.055156443 | Test Loss: 0.014277125\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1139494 | Validation Loss: 1.1139494 | Test Loss: 1.2081298\n",
      "Epoch 574: Training Loss: 0.056244444 | Validation Loss: 0.056244444 | Test Loss: 0.01596247\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1171631 | Validation Loss: 1.1171631 | Test Loss: 1.1831989\n",
      "Epoch 652: Training Loss: 0.055305175 | Validation Loss: 0.055305175 | Test Loss: 0.015947644\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.2009599 | Validation Loss: 1.2009599 | Test Loss: 1.2889116\n",
      "Epoch 621: Training Loss: 0.05786334 | Validation Loss: 0.05786334 | Test Loss: 0.018653193\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.2537657 | Validation Loss: 1.2537657 | Test Loss: 1.0169525\n",
      "Epoch 37: Training Loss: 1.0981311 | Validation Loss: 1.0981311 | Test Loss: 1.1479411\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.3385078 | Validation Loss: 1.3385078 | Test Loss: 1.6742631\n",
      "Epoch 724: Training Loss: 0.057565242 | Validation Loss: 0.057565242 | Test Loss: 0.018882323\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1816676 | Validation Loss: 1.1816676 | Test Loss: 1.0292412\n",
      "Epoch 757: Training Loss: 0.059288364 | Validation Loss: 0.059288364 | Test Loss: 0.021260269\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1323127 | Validation Loss: 1.1323127 | Test Loss: 1.2022159\n",
      "Epoch 674: Training Loss: 0.04883039 | Validation Loss: 0.04883039 | Test Loss: 0.09767931\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1619048 | Validation Loss: 1.1619048 | Test Loss: 1.0447257\n",
      "Epoch 35: Training Loss: 1.0982745 | Validation Loss: 1.0982745 | Test Loss: 1.1259727\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1031833 | Validation Loss: 1.1031833 | Test Loss: 1.0749092\n",
      "Epoch 584: Training Loss: 0.048127472 | Validation Loss: 0.048127472 | Test Loss: 0.107979275\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1018311 | Validation Loss: 1.1018311 | Test Loss: 1.0852736\n",
      "Epoch 592: Training Loss: 0.047774825 | Validation Loss: 0.047774825 | Test Loss: 0.1087534\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1909612 | Validation Loss: 1.1909612 | Test Loss: 1.2016091\n",
      "Epoch 665: Training Loss: 0.051103063 | Validation Loss: 0.051103063 | Test Loss: 0.10143475\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1158215 | Validation Loss: 1.1158215 | Test Loss: 1.0687368\n",
      "Epoch 713: Training Loss: 0.04666777 | Validation Loss: 0.04666777 | Test Loss: 0.11098283\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1966525 | Validation Loss: 1.1966525 | Test Loss: 1.1244751\n",
      "Epoch 637: Training Loss: 0.049105763 | Validation Loss: 0.049105763 | Test Loss: 0.09876527\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.18879 | Validation Loss: 1.18879 | Test Loss: 1.1510422\n",
      "Epoch 670: Training Loss: 0.051824134 | Validation Loss: 0.051824134 | Test Loss: 0.10789565\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.1816512 | Validation Loss: 1.1816512 | Test Loss: 1.1077085\n",
      "Epoch 654: Training Loss: 0.050064325 | Validation Loss: 0.050064325 | Test Loss: 0.109184876\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "(135, 4)\n",
      "(135, 3)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(15, 4)\n",
      "(15, 3)\n",
      "Epoch 0: Training Loss: 1.3440533 | Validation Loss: 1.3440533 | Test Loss: 1.5745156\n",
      "Epoch 766: Training Loss: 0.051045157 | Validation Loss: 0.051045157 | Test Loss: 0.13177064\n",
      "Paramos entrenamiento\n",
      "Min Loss\n",
      "[10, 10, 2, 9, 6, 3, 6, 4, 4, 4, 4, 7, 8, 9, 5, 3, 8, 2, 5, 8, 3, 9, 8, 8, 3, 4, 8, 2, 9, 2, 9, 2, 1, 4, 6, 7, 10, 5, 4, 7, 3, 6, 1, 1, 6, 6, 4, 7, 9, 7, 7, 10, 7, 5, 3, 10, 4, 7, 8, 7, 5, 5, 3, 2, 10, 4, 6, 6, 7, 9, 1, 10, 6, 5, 4, 8, 1, 4, 7, 6, 8, 9, 2, 3, 5, 1, 3, 8, 2, 8, 5, 1, 5, 2, 5, 1, 6, 3, 10, 3, 6, 5, 10, 6, 7, 5, 1, 7, 1, 7, 5, 2, 9, 9, 10, 9, 10, 3, 1, 9, 5, 4, 2, 9, 10, 4, 6, 3, 3, 8, 1, 1, 10, 2, 9, 10, 10, 4, 1, 8, 6, 3, 2, 8, 1, 9, 2, 2, 7, 8]\n",
      "Accuracy: 98.74074074074073 %\n",
      "Error rate: 1.259259259259261 %\n",
      "Confusion matrix: [45.0 0.0 0.0; 0.0 44.1 0.8; 0.0 0.9 44.2]\n",
      "Standard deviation of accuracy: 0.4999618946004103 %\n",
      "Standard deviation of error rate: 0.49996189460041024 %\n",
      "Standard deviation of confusion matrix: [1.632993161855452 0.0 0.0; 0.0 1.5238839267549946 0.42163702135578396; 0.0 0.31622776601683794 1.6193277068654826]\n"
     ]
    }
   ],
   "source": [
    "using NBInclude\n",
    "#@nbinclude(\"/home/martin/Escritorio/ML1/MIA_ML1/Unit 2 - Multilayer Perceptron_SOLVED.ipynb\")\n",
    "@nbinclude(\"/home/martin/Escritorio/ML1/MIA_ML1/Unit 5 - Cross-validation.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17e6c492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelCrossValidation (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function modelCrossValidation(modelType::Symbol,\n",
    "        modelHyperparameters::Dict,\n",
    "        inputs::AbstractArray{<:Real,2},\n",
    "        targets::AbstractArray{Any,1},\n",
    "        crossValidationIndices::Array{Int64,1})\n",
    "\n",
    "    \n",
    "    #Check that the number of inputs is equal to the number of targets\n",
    "    @assert length(inputs[:,1])==length(targets) \n",
    "\n",
    "    #Check that the model type is valid\n",
    "    @assert modelType==:ann || modelType==:svm || modelType==:decision_tree || modelType==:knn\n",
    "\n",
    "    #Get the number of folds\n",
    "    numFolds=maximum(crossValidationIndices)\n",
    "    \n",
    "    #Check that the crossvalidation indices are valid\n",
    "    @assert all(1 .<= crossValidationIndices .<= numFolds)    \n",
    "\n",
    "    #Define the variables that will be returned\n",
    "    accuracy_list=[]\n",
    "    confusion_matrix_list=[]\n",
    "    \n",
    "    #If the model is not SVM, we need to one-hot encode the targets\n",
    "    if modelType!= :svm\n",
    "        targets=oneHotEncoding(targets)\n",
    "    end\n",
    "    \n",
    "    #For each fold, train the model and test it\n",
    "    for i = 1:numFolds\n",
    "        #Get the training and test inputs\n",
    "        trainInputs = inputs[crossValidationIndices.!=i,:]\n",
    "        testInputs = inputs[crossValidationIndices.==i,:]\n",
    "\n",
    "        #Get the training and test targets\n",
    "        if modelType!= :svm\n",
    "            trainTargets = targets[crossValidationIndices.!=i,:]\n",
    "            testTargets = targets[crossValidationIndices.==i,:]            \n",
    "\n",
    "            testTargets=BitMatrix(testTargets')\n",
    "        else\n",
    "            trainTargets = targets[crossValidationIndices.!=i]\n",
    "            testTargets = targets[crossValidationIndices.==i]\n",
    "        end\n",
    "\n",
    "\n",
    "        ################\n",
    "        #Train the model\n",
    "        ################\n",
    "\n",
    "\n",
    "        #Train ann model\n",
    "        if modelType == :ann\n",
    "            @assert haskey(modelHyperparameters, \"hidden_layer_sizes\") && haskey(modelHyperparameters, \"activation\") && haskey(modelHyperparameters, \"learning_rate_init\") && haskey(modelHyperparameters, \"validation_fraction\") && haskey(modelHyperparameters, \"max_iter\")\n",
    "            model = MLPClassifier(hidden_layer_sizes=modelHyperparameters[\"hidden_layer_sizes\"],\n",
    "                                  activation=modelHyperparameters[\"activation\"],\n",
    "                                  learning_rate_init=modelHyperparameters[\"learning_rate_init\"],\n",
    "                                  validation_fraction=modelHyperparameters[\"validation_fraction\"],\n",
    "                                  max_iter=modelHyperparameters[\"max_iter\"])\n",
    "            model.fit(trainInputs, trainTargets)\n",
    "            predictedTargets = model.predict(testInputs)\n",
    "\n",
    "        #Train svm model\n",
    "        elseif modelType == :svm\n",
    "            @assert haskey(modelHyperparameters, \"kernel\") && haskey(modelHyperparameters, \"C\") && haskey(modelHyperparameters, \"gamma\") && haskey(modelHyperparameters, \"degree\")\n",
    "            model = SVC(kernel=modelHyperparameters[\"kernel\"], C=modelHyperparameters[\"C\"], gamma=modelHyperparameters[\"gamma\"], degree=modelHyperparameters[\"degree\"])\n",
    "            model.fit(trainInputs, trainTargets)\n",
    "            predictedTargets = model.predict(testInputs)\n",
    "            predictedTargets=oneHotEncoding(predictedTargets)\n",
    "            testTargets=oneHotEncoding(testTargets)\n",
    "            testTargets=testTargets'\n",
    "\n",
    "        #Train decision tree model\n",
    "        elseif modelType == :decision_tree\n",
    "            @assert haskey(modelHyperparameters, \"max_depth\") && haskey(modelHyperparameters, \"criterion\") && haskey(modelHyperparameters, \"splitter\")\n",
    "            model = DecisionTreeClassifier(max_depth=modelHyperparameters[\"max_depth\"],criterion=modelHyperparameters[\"criterion\"],splitter=modelHyperparameters[\"splitter\"])\n",
    "            model.fit(trainInputs, trainTargets)\n",
    "            predictedTargets = model.predict(testInputs)\n",
    "        \n",
    "        #Train knn model\n",
    "        elseif modelType == :knn\n",
    "            @assert haskey(modelHyperparameters, \"n_neighbors\") && haskey(modelHyperparameters, \"weights\")\n",
    "            model = KNeighborsClassifier(n_neighbors=modelHyperparameters[\"n_neighbors\"],weights=modelHyperparameters[\"weights\"])\n",
    "            model.fit(trainInputs, trainTargets)\n",
    "            predictedTargets = model.predict(testInputs)\n",
    "            \n",
    "        end\n",
    "\n",
    "        #Check that the number of predicted targets is equal to the number of test targets\n",
    "        @assert length(predictedTargets)==length(testTargets)\n",
    "        println(\"Type target\",typeof(predictedTargets))\n",
    "        println(\"Size\",size(predictedTargets))\n",
    "        #Calculate the accuracy and confusion matrix\n",
    "        accuracy1, error_rate, sensitivity, specificity, positive_predictive_value, negative_predictive_value, f_score, confusion_matrix=confusionMatrix(predictedTargets,testTargets',weighted=false)\n",
    "        #Add the accuracy and confusion matrix to the lists\n",
    "        push!(accuracy_list,accuracy1)\n",
    "        push!(confusion_matrix_list,confusion_matrix)\n",
    "\n",
    "    end\n",
    "    #Return the accuracy and confusion matrix lists\n",
    "    return accuracy_list, confusion_matrix_list\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8235c40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Onehot\n",
      "num_elements50\n",
      "subsets1[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "subsets2[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "subsets3[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "class_indices[6, 3, 4, 4, 1, 5, 2, 3, 2, 5, 10, 10, 10, 3, 3, 1, 6, 5, 8, 6, 9, 10, 2, 9, 8, 7, 5, 4, 9, 1, 9, 4, 3, 8, 6, 10, 9, 1, 2, 7, 8, 1, 7, 2, 4, 7, 8, 7, 6, 5]\n",
      "(50,)\n",
      "indices[6, 3, 4, 4, 1, 5, 2, 3, 2, 5, 10, 10, 10, 3, 3, 1, 6, 5, 8, 6, 9, 10, 2, 9, 8, 7, 5, 4, 9, 1, 9, 4, 3, 8, 6, 10, 9, 1, 2, 7, 8, 1, 7, 2, 4, 7, 8, 7, 6, 5, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150]\n",
      "num_elements50\n",
      "subsets1[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "subsets2[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "subsets3[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "class_indices[1, 1, 10, 7, 4, 8, 3, 1, 5, 9, 7, 9, 6, 9, 1, 10, 6, 3, 7, 8, 7, 5, 4, 10, 5, 8, 3, 6, 4, 2, 8, 5, 5, 4, 9, 10, 3, 8, 2, 2, 6, 3, 4, 2, 2, 6, 1, 10, 9, 7]\n",
      "(50,)\n",
      "indices[6, 3, 4, 4, 1, 5, 2, 3, 2, 5, 10, 10, 10, 3, 3, 1, 6, 5, 8, 6, 9, 10, 2, 9, 8, 7, 5, 4, 9, 1, 9, 4, 3, 8, 6, 10, 9, 1, 2, 7, 8, 1, 7, 2, 4, 7, 8, 7, 6, 5, 1, 1, 10, 7, 4, 8, 3, 1, 5, 9, 7, 9, 6, 9, 1, 10, 6, 3, 7, 8, 7, 5, 4, 10, 5, 8, 3, 6, 4, 2, 8, 5, 5, 4, 9, 10, 3, 8, 2, 2, 6, 3, 4, 2, 2, 6, 1, 10, 9, 7, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150]\n",
      "num_elements50\n",
      "subsets1[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "subsets2[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "subsets3[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "class_indices[3, 8, 7, 6, 10, 6, 9, 5, 5, 3, 8, 10, 4, 1, 4, 4, 4, 2, 6, 9, 10, 2, 9, 4, 9, 2, 8, 3, 5, 2, 1, 1, 8, 6, 2, 10, 7, 3, 5, 1, 8, 6, 7, 5, 3, 7, 1, 7, 9, 10]\n",
      "(50,)\n",
      "indices[6, 3, 4, 4, 1, 5, 2, 3, 2, 5, 10, 10, 10, 3, 3, 1, 6, 5, 8, 6, 9, 10, 2, 9, 8, 7, 5, 4, 9, 1, 9, 4, 3, 8, 6, 10, 9, 1, 2, 7, 8, 1, 7, 2, 4, 7, 8, 7, 6, 5, 1, 1, 10, 7, 4, 8, 3, 1, 5, 9, 7, 9, 6, 9, 1, 10, 6, 3, 7, 8, 7, 5, 4, 10, 5, 8, 3, 6, 4, 2, 8, 5, 5, 4, 9, 10, 3, 8, 2, 2, 6, 3, 4, 2, 2, 6, 1, 10, 9, 7, 3, 8, 7, 6, 10, 6, 9, 5, 5, 3, 8, 10, 4, 1, 4, 4, 4, 2, 6, 9, 10, 2, 9, 4, 9, 2, 8, 3, 5, 2, 1, 1, 8, 6, 2, 10, 7, 3, 5, 1, 8, 6, 7, 5, 3, 7, 1, 7, 9, 10]\n",
      "Crossvalidation vector: [8, 4, 6, 6, 10, 8, 8, 7, 9, 6, 10, 3, 7, 10, 4, 2, 2, 9, 5, 7, 5, 5, 7, 7, 9, 6, 9, 4, 8, 6, 9, 4, 3, 9, 5, 8, 3, 7, 8, 5, 7, 7, 8, 5, 3, 6, 1, 1, 1, 1, 1, 2, 1, 9, 10, 7, 6, 4, 4, 4, 5, 1, 2, 1, 1, 6, 5, 3, 1, 1, 10, 2, 4, 7, 6, 10, 3, 7, 1, 6, 10, 3, 9, 10, 3, 8, 4, 3, 4, 4, 1, 8, 10, 8, 5, 2, 2, 8, 2, 4, 10, 9, 4, 5, 10, 8, 5, 1, 1, 5, 4, 9, 6, 7, 3, 10, 3, 5, 7, 2, 5, 7, 8, 2, 7, 3, 2, 8, 9, 10, 9, 9, 3, 9, 3, 3, 10, 9, 6, 2, 2, 5, 2, 6, 6, 10, 6, 8, 2, 4]\n",
      "Any[\"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-versicolor\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\"]\n",
      "Vector{Any}\n",
      "(150,)\n",
      "Type targetMatrix{Int64}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Int64}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Int64}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Int64}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Int64}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Int64}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Int64}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Int64}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Int64}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Int64}\n",
      "Size(15, 3)\n",
      "Type targetBitMatrix\n",
      "Size(15, 3)\n",
      "Type targetBitMatrix\n",
      "Size(15, 3)\n",
      "Type targetBitMatrix\n",
      "Size(15, 3)\n",
      "Type targetBitMatrix\n",
      "Size(15, 3)\n",
      "Type targetBitMatrix\n",
      "Size(15, 3)\n",
      "Type targetBitMatrix\n",
      "Size(15, 3)\n",
      "Type targetBitMatrix\n",
      "Size(15, 3)\n",
      "Type targetBitMatrix\n",
      "Size(15, 3)\n",
      "Type targetBitMatrix\n",
      "Size(15, 3)\n",
      "Type targetBitMatrix\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Bool}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Bool}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Bool}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Bool}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Bool}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Bool}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Bool}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Bool}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Bool}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Bool}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Bool}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Bool}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Bool}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Bool}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Bool}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Bool}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Bool}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Bool}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Bool}\n",
      "Size(15, 3)\n",
      "Type targetMatrix{Bool}\n",
      "Size(15, 3)\n",
      "ANN accuracy: 0.86 ANN confusion matrix: [5.0 1.4 0.1; 0.0 3.1 0.1; 0.0 0.5 4.8]\n",
      "SVM accuracy: 0.9666666666666668 SVM confusion matrix: [5.0 0.0 0.0; 0.0 4.7 0.2; 0.0 0.3 4.8]\n",
      "Decision tree accuracy: 0.9466666666666667 Decision tree confusion matrix: [5.0 0.0 0.0; 0.0 4.7 0.3; 0.0 0.3 4.5]\n",
      "KNN accuracy: 0.9533333333333334 KNN confusion matrix: [5.0 0.0 0.0; 0.0 4.7 0.4; 0.0 0.3 4.6]\n",
      "Bool[1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 1 0 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 1 0; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1; 0 0 1]\n",
      "(150, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = readdlm(\"../iris/iris.data\",',');\n",
    "# Prepare the data\n",
    "inputs = convert(Array{Float32,2}, dataset[:,1:4]);\n",
    "targets=dataset[:,5];\n",
    "# Normalize the inputs\n",
    "inputs = normalizeMinMax!(inputs);\n",
    "\n",
    "# Check that the inputs are normalized\n",
    "@assert(all(minimum(inputs, dims=1) .== 0));\n",
    "@assert(all(maximum(inputs, dims=1) .== 1));\n",
    "\n",
    "# Call the crossvalidation function\n",
    "vector=crossvalidation(targets,10)\n",
    "\n",
    "println(\"Crossvalidation vector: \", vector);\n",
    "println(targets);\n",
    "println(typeof(targets));\n",
    "println(size(targets))\n",
    "# Define the hyperparameters\n",
    "ANNhyperparameters=Dict(\"hidden_layer_sizes\"=>(10,3),\"activation\"=>\"relu\",\"learning_rate_init\"=>0.01,\"validation_fraction\"=>0.1,\"max_iter\"=>2000);\n",
    "SVMhyperparameters=Dict(\"kernel\"=>\"rbf\",\"degree\"=>3,\"gamma\"=>2,\"C\"=>1);\n",
    "DThyperparameters=Dict(\"max_depth\"=>3,\"criterion\"=>\"gini\",\"splitter\"=>\"best\");\n",
    "KNNhyperparameters=Dict(\"n_neighbors\"=>3,\"weights\"=>\"uniform\");\n",
    "\n",
    "\n",
    "# Call the modelCrossValidation function\n",
    "accuracyANN,confusionMatrixANN = modelCrossValidation(:ann, ANNhyperparameters, inputs, targets, vector);\n",
    "accuracySVM,confusionMatrixSVM = modelCrossValidation(:svm, SVMhyperparameters, inputs, targets, vector);\n",
    "accuracyDT,confusionMatrixDT = modelCrossValidation(:decision_tree, DThyperparameters, inputs, targets, vector);\n",
    "accuracyKNN,confusionMatrixKNN = modelCrossValidation(:knn, KNNhyperparameters, inputs, targets, vector);\n",
    "\n",
    "\n",
    "# Print the results\n",
    "println(\"ANN accuracy: \", mean(accuracyANN),\" ANN confusion matrix: \", mean(confusionMatrixANN));\n",
    "println(\"SVM accuracy: \", mean(accuracySVM),\" SVM confusion matrix: \", mean(confusionMatrixSVM));\n",
    "println(\"Decision tree accuracy: \", mean(accuracyDT),\" Decision tree confusion matrix: \", mean(confusionMatrixDT));\n",
    "println(\"KNN accuracy: \", mean(accuracyKNN),\" KNN confusion matrix: \", mean(confusionMatrixKNN));\n",
    "\n",
    "\n",
    "encoded_targets = oneHotEncoding(targets)\n",
    "println(encoded_targets)\n",
    "println(size(encoded_targets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b760ec1a",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c935a3bd",
   "metadata": {},
   "source": [
    "## Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ef40f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN accuracy: 0.6933333333333334 ANN confusion matrix: [5.0 3.4 1.1; 0.0 1.5 0.0; 0.0 0.1 3.9]\n",
      "ANN accuracy: 0.6266666666666667 ANN confusion matrix: [4.3 2.4 1.1; 0.7 1.4 0.2; 0.0 1.2 3.7]\n",
      "ANN accuracy: 0.8066666666666666 ANN confusion matrix: [5.0 0.8 1.1; 0.0 4.0 0.8; 0.0 0.2 3.1]\n",
      "ANN accuracy: 0.7 ANN confusion matrix: [5.0 2.5 1.0; 0.0 2.3 0.8; 0.0 0.2 3.2]\n",
      "ANN accuracy: 0.5266666666666666 ANN confusion matrix: [5.0 2.8 3.1; 0.0 1.1 0.1; 0.0 1.1 1.8]\n",
      "ANN accuracy: 0.9533333333333334 ANN confusion matrix: [5.0 0.0 0.0; 0.0 4.6 0.3; 0.0 0.4 4.7]\n",
      "ANN accuracy: 0.9733333333333334 ANN confusion matrix: [5.0 0.0 0.0; 0.0 4.7 0.1; 0.0 0.3 4.9]\n",
      "ANN accuracy: 0.9133333333333333 ANN confusion matrix: [5.0 0.1 0.0; 0.0 4.7 1.0; 0.0 0.2 4.0]\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "##ANN experiments##\n",
    "###################\n",
    "\n",
    "# Define the hyperparameters\n",
    "ANNhyperparameters1=Dict(\"hidden_layer_sizes\"=>(10,3),\"activation\"=>\"relu\",\"learning_rate_init\"=>0.01,\"validation_fraction\"=>0.1,\"max_iter\"=>5000);\n",
    "ANNhyperparameters2=Dict(\"hidden_layer_sizes\"=>(4,3),\"activation\"=>\"relu\",\"learning_rate_init\"=>0.001,\"validation_fraction\"=>0.1,\"max_iter\"=>5000);\n",
    "ANNhyperparameters3=Dict(\"hidden_layer_sizes\"=>(20,3),\"activation\"=>\"relu\",\"learning_rate_init\"=>0.01,\"validation_fraction\"=>0.1,\"max_iter\"=>5000);\n",
    "ANNhyperparameters4=Dict(\"hidden_layer_sizes\"=>(6,10,3),\"activation\"=>\"relu\",\"learning_rate_init\"=>0.01,\"validation_fraction\"=>0.1,\"max_iter\"=>5000);\n",
    "ANNhyperparameters5=Dict(\"hidden_layer_sizes\"=>(20,10,3),\"activation\"=>\"logistic\",\"learning_rate_init\"=>0.01,\"validation_fraction\"=>0.1,\"max_iter\"=>5000);\n",
    "ANNhyperparameters6=Dict(\"hidden_layer_sizes\"=>(20,10,3),\"activation\"=>\"tanh\",\"learning_rate_init\"=>0.01,\"validation_fraction\"=>0.1,\"max_iter\"=>5000);\n",
    "ANNhyperparameters7=Dict(\"hidden_layer_sizes\"=>(10,3),\"activation\"=>\"tanh\",\"learning_rate_init\"=>0.01,\"validation_fraction\"=>0.1,\"max_iter\"=>5000);\n",
    "ANNhyperparameters8=Dict(\"hidden_layer_sizes\"=>(10,3),\"activation\"=>\"tanh\",\"learning_rate_init\"=>0.001,\"validation_fraction\"=>0.1,\"max_iter\"=>5000);\n",
    "\n",
    "\n",
    "# Call the modelCrossValidation function\n",
    "accuracyANN1,confusionMatrixANN1 = modelCrossValidation(:ann, ANNhyperparameters1, inputs, targets, vector);\n",
    "accuracyANN2,confusionMatrixANN2 = modelCrossValidation(:ann, ANNhyperparameters2, inputs, targets, vector);\n",
    "accuracyANN3,confusionMatrixANN3 = modelCrossValidation(:ann, ANNhyperparameters3, inputs, targets, vector);\n",
    "accuracyANN4,confusionMatrixANN4 = modelCrossValidation(:ann, ANNhyperparameters4, inputs, targets, vector);\n",
    "accuracyANN5,confusionMatrixANN5 = modelCrossValidation(:ann, ANNhyperparameters5, inputs, targets, vector);\n",
    "accuracyANN6,confusionMatrixANN6 = modelCrossValidation(:ann, ANNhyperparameters6, inputs, targets, vector);\n",
    "accuracyANN7,confusionMatrixANN7 = modelCrossValidation(:ann, ANNhyperparameters7, inputs, targets, vector);\n",
    "accuracyANN8,confusionMatrixANN8 = modelCrossValidation(:ann, ANNhyperparameters8, inputs, targets, vector);\n",
    "\n",
    "\n",
    "# Print the results\n",
    "println(\"ANN accuracy: \", mean(accuracyANN1),\" ANN confusion matrix: \", mean(confusionMatrixANN1));\n",
    "println(\"ANN accuracy: \", mean(accuracyANN2),\" ANN confusion matrix: \", mean(confusionMatrixANN2));\n",
    "println(\"ANN accuracy: \", mean(accuracyANN3),\" ANN confusion matrix: \", mean(confusionMatrixANN3));\n",
    "println(\"ANN accuracy: \", mean(accuracyANN4),\" ANN confusion matrix: \", mean(confusionMatrixANN4));\n",
    "println(\"ANN accuracy: \", mean(accuracyANN5),\" ANN confusion matrix: \", mean(confusionMatrixANN5));\n",
    "println(\"ANN accuracy: \", mean(accuracyANN6),\" ANN confusion matrix: \", mean(confusionMatrixANN6));\n",
    "println(\"ANN accuracy: \", mean(accuracyANN7),\" ANN confusion matrix: \", mean(confusionMatrixANN7));\n",
    "println(\"ANN accuracy: \", mean(accuracyANN8),\" ANN confusion matrix: \", mean(confusionMatrixANN8));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d24255",
   "metadata": {},
   "source": [
    "After the result, it seems that the best option is the last model, where the activation was changed from \"relu\" to \"tanh\". We also viewed that if the number of layers and neurons is very high, the results are not good. In our experiments, the best results is the topology of (10,3). Other changes made is to modify the value of the learning rate, but the change we made was small and has hardly affected the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c049116",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "AssertionError: length(predictedTargets) == length(testTargets)",
     "output_type": "error",
     "traceback": [
      "AssertionError: length(predictedTargets) == length(testTargets)",
      "",
      "Stacktrace:",
      " [1] modelCrossValidation(modelType::Symbol, modelHyperparameters::Dict{String, Any}, inputs::Matrix{Float32}, targets::Vector{Any}, crossValidationIndices::Vector{Int64})",
      "   @ Main ./In[4]:90",
      " [2] top-level scope",
      "   @ In[7]:21"
     ]
    }
   ],
   "source": [
    "###################\n",
    "##SVM experiments##\n",
    "###################\n",
    "\n",
    "# Define the hyperparameters\n",
    "SVMhyperparameters1=Dict(\"kernel\"=>\"rbf\",\"degree\"=>3,\"gamma\"=>2,\"C\"=>1);\n",
    "SVMhyperparameters2=Dict(\"kernel\"=>\"poly\",\"degree\"=>3,\"gamma\"=>2,\"C\"=>1);\n",
    "SVMhyperparameters3=Dict(\"kernel\"=>\"sigmoid\",\"degree\"=>3,\"gamma\"=>2,\"C\"=>1);\n",
    "SVMhyperparameters4=Dict(\"kernel\"=>\"linear\",\"degree\"=>3,\"gamma\"=>2,\"C\"=>1);\n",
    "SVMhyperparameters5=Dict(\"kernel\"=>\"rbf\",\"degree\"=>5,\"gamma\"=>2,\"C\"=>1);\n",
    "SVMhyperparameters6=Dict(\"kernel\"=>\"poly\",\"degree\"=>5,\"gamma\"=>2,\"C\"=>1);\n",
    "SVMhyperparameters7=Dict(\"kernel\"=>\"linear\",\"degree\"=>5,\"gamma\"=>2,\"C\"=>1);\n",
    "SVMhyperparameters8=Dict(\"kernel\"=>\"rbf\",\"degree\"=>5,\"gamma\"=>\"scale\",\"C\"=>0.1);\n",
    "SVMhyperparameters9=Dict(\"kernel\"=>\"poly\",\"degree\"=>5,\"gamma\"=>\"scale\",\"C\"=>0.1);\n",
    "SVMhyperparameters10=Dict(\"kernel\"=>\"linear\",\"degree\"=>5,\"gamma\"=>2,\"C\"=>0.1);\n",
    "\n",
    "\n",
    "# Call the modelCrossValidation function\n",
    "accuracySVM1,confusionMatrixSVM1 = modelCrossValidation(:svm, SVMhyperparameters1, inputs, targets, vector);\n",
    "accuracySVM2,confusionMatrixSVM2 = modelCrossValidation(:svm, SVMhyperparameters2, inputs, targets, vector);\n",
    "accuracySVM3,confusionMatrixSVM3 = modelCrossValidation(:svm, SVMhyperparameters3, inputs, targets, vector);\n",
    "accuracySVM4,confusionMatrixSVM4 = modelCrossValidation(:svm, SVMhyperparameters4, inputs, targets, vector);\n",
    "accuracySVM5,confusionMatrixSVM5 = modelCrossValidation(:svm, SVMhyperparameters5, inputs, targets, vector);\n",
    "accuracySVM6,confusionMatrixSVM6 = modelCrossValidation(:svm, SVMhyperparameters6, inputs, targets, vector);\n",
    "accuracySVM7,confusionMatrixSVM7 = modelCrossValidation(:svm, SVMhyperparameters7, inputs, targets, vector);\n",
    "accuracySVM8,confusionMatrixSVM8 = modelCrossValidation(:svm, SVMhyperparameters8, inputs, targets, vector);\n",
    "accuracySVM9,confusionMatrixSVM9 = modelCrossValidation(:svm, SVMhyperparameters9, inputs, targets, vector);\n",
    "accuracySVM10,confusionMatrixSVM10 = modelCrossValidation(:svm, SVMhyperparameters10, inputs, targets, vector);\n",
    "\n",
    "\n",
    "# Print the results\n",
    "println(\"SVM accuracy: \", mean(accuracySVM1),\" SVM confusion matrix: \", mean(confusionMatrixSVM1));\n",
    "println(\"SVM accuracy: \", mean(accuracySVM2),\" SVM confusion matrix: \", mean(confusionMatrixSVM2));\n",
    "println(\"SVM accuracy: \", mean(accuracySVM3),\" SVM confusion matrix: \", mean(confusionMatrixSVM3));\n",
    "println(\"SVM accuracy: \", mean(accuracySVM4),\" SVM confusion matrix: \", mean(confusionMatrixSVM4));\n",
    "println(\"SVM accuracy: \", mean(accuracySVM5),\" SVM confusion matrix: \", mean(confusionMatrixSVM5));\n",
    "println(\"SVM accuracy: \", mean(accuracySVM6),\" SVM confusion matrix: \", mean(confusionMatrixSVM6));\n",
    "println(\"SVM accuracy: \", mean(accuracySVM7),\" SVM confusion matrix: \", mean(confusionMatrixSVM7));\n",
    "println(\"SVM accuracy: \", mean(accuracySVM8),\" SVM confusion matrix: \", mean(confusionMatrixSVM8));\n",
    "println(\"SVM accuracy: \", mean(accuracySVM9),\" SVM confusion matrix: \", mean(confusionMatrixSVM9));\n",
    "println(\"SVM accuracy: \", mean(accuracySVM10),\" SVM confusion matrix: \", mean(confusionMatrixSVM10));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c233a5",
   "metadata": {},
   "source": [
    "After the results, the best options for the kernel are \"rbf\" and \"poly\", we also made changes in the values of gamma, C and degree, but the final result was very similar. In this case the best model is the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596aa4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT accuracy: 0.9533333333333334 DT confusion matrix: [5.0 0.0 0.0; 0.0 4.6 0.2; 0.0 0.3 4.7]\n",
      "DT accuracy: 0.9466666666666667 DT confusion matrix: [5.0 0.0 0.0; 0.0 4.5 0.3; 0.0 0.5 4.7]\n",
      "DT accuracy: 0.9333333333333333 DT confusion matrix: [5.0 0.0 0.0; 0.0 4.5 0.5; 0.0 0.5 4.5]\n",
      "DT accuracy: 0.9400000000000001 DT confusion matrix: [5.0 0.0 0.0; 0.0 4.8 0.7; 0.0 0.2 4.3]\n",
      "DT accuracy: 0.9266666666666667 DT confusion matrix: [5.0 0.0 0.0; 0.0 4.3 0.4; 0.0 0.6 4.6]\n",
      "DT accuracy: 0.9266666666666667 DT confusion matrix: [5.0 0.0 0.0; 0.0 4.5 0.5; 0.0 0.4 4.4]\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "##DT experiments##\n",
    "##################\n",
    "\n",
    "\n",
    "# Define the hyperparameters\n",
    "DThyperparameters1=Dict(\"max_depth\"=>3,\"criterion\"=>\"gini\",\"splitter\"=>\"best\");\n",
    "DThyperparameters2=Dict(\"max_depth\"=>5,\"criterion\"=>\"entropy\",\"splitter\"=>\"best\");\n",
    "DThyperparameters3=Dict(\"max_depth\"=>7,\"criterion\"=>\"log_loss\",\"splitter\"=>\"best\");\n",
    "DThyperparameters4=Dict(\"max_depth\"=>3,\"criterion\"=>\"gini\",\"splitter\"=>\"random\");\n",
    "DThyperparameters5=Dict(\"max_depth\"=>5,\"criterion\"=>\"entropy\",\"splitter\"=>\"random\");\n",
    "DThyperparameters6=Dict(\"max_depth\"=>7,\"criterion\"=>\"log_loss\",\"splitter\"=>\"random\");\n",
    "\n",
    "\n",
    "# Call the modelCrossValidation function\n",
    "accuracyDT1,confusionMatrixDT1 = modelCrossValidation(:decision_tree, DThyperparameters1, inputs, targets, vector);\n",
    "accuracyDT2,confusionMatrixDT2 = modelCrossValidation(:decision_tree, DThyperparameters2, inputs, targets, vector);\n",
    "accuracyDT3,confusionMatrixDT3 = modelCrossValidation(:decision_tree, DThyperparameters3, inputs, targets, vector);\n",
    "accuracyDT4,confusionMatrixDT4 = modelCrossValidation(:decision_tree, DThyperparameters4, inputs, targets, vector);\n",
    "accuracyDT5,confusionMatrixDT5 = modelCrossValidation(:decision_tree, DThyperparameters5, inputs, targets, vector);\n",
    "accuracyDT6,confusionMatrixDT6 = modelCrossValidation(:decision_tree, DThyperparameters6, inputs, targets, vector);\n",
    "\n",
    "\n",
    "# Print the results\n",
    "println(\"DT accuracy: \", mean(accuracyDT1),\" DT confusion matrix: \", mean(confusionMatrixDT1));\n",
    "println(\"DT accuracy: \", mean(accuracyDT2),\" DT confusion matrix: \", mean(confusionMatrixDT2));\n",
    "println(\"DT accuracy: \", mean(accuracyDT3),\" DT confusion matrix: \", mean(confusionMatrixDT3));\n",
    "println(\"DT accuracy: \", mean(accuracyDT4),\" DT confusion matrix: \", mean(confusionMatrixDT4));\n",
    "println(\"DT accuracy: \", mean(accuracyDT5),\" DT confusion matrix: \", mean(confusionMatrixDT5));\n",
    "println(\"DT accuracy: \", mean(accuracyDT6),\" DT confusion matrix: \", mean(confusionMatrixDT6));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c540c786",
   "metadata": {},
   "source": [
    "If we compare the accuracies obtaniend in these experiment, we can find that DT models seems to have a nice performance for the given problem. In this manner, we find that the 3 criteria seems to have similar accuracy values, but the one who performs better is \"gini\", with a small differece. Analyzing the splitter, at the begining the model seems to work better using weights=\"uniform\"; but there are times when weights=\"random\" have a better performance. \n",
    "\n",
    "With all of this, we have seen that there are no so much different in the accuracy of the model between the parameters used in this experimentation in the given problem, but the one to seems to perform better is the first one with \"max_depth=3\",\"criterion=gini\" and \"splitter=best\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ae857c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN accuracy: 0.9533333333333334 KNN confusion matrix: [5.0 0.0 0.0; 0.0 4.7 0.4; 0.0 0.3 4.6]\n",
      "KNN accuracy: 0.9533333333333334 KNN confusion matrix: [5.0 0.0 0.0; 0.0 4.7 0.4; 0.0 0.3 4.6]\n",
      "KNN accuracy: 0.9733333333333334 KNN confusion matrix: [5.0 0.0 0.0; 0.0 4.9 0.3; 0.0 0.1 4.7]\n",
      "KNN accuracy: 0.9533333333333334 KNN confusion matrix: [5.0 0.0 0.0; 0.0 4.7 0.4; 0.0 0.3 4.6]\n",
      "KNN accuracy: 0.9533333333333334 KNN confusion matrix: [5.0 0.0 0.0; 0.0 4.7 0.4; 0.0 0.3 4.6]\n",
      "KNN accuracy: 0.9666666666666668 KNN confusion matrix: [5.0 0.0 0.0; 0.0 4.8 0.3; 0.0 0.2 4.7]\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "##KNN experiments##\n",
    "###################\n",
    "\n",
    "# Define the hyperparameters\n",
    "KNNhyperparameters1=Dict(\"n_neighbors\"=>3,\"weights\"=>\"uniform\");\n",
    "KNNhyperparameters2=Dict(\"n_neighbors\"=>5,\"weights\"=>\"uniform\");\n",
    "KNNhyperparameters3=Dict(\"n_neighbors\"=>7,\"weights\"=>\"uniform\");\n",
    "KNNhyperparameters4=Dict(\"n_neighbors\"=>3,\"weights\"=>\"distance\");\n",
    "KNNhyperparameters5=Dict(\"n_neighbors\"=>5,\"weights\"=>\"distance\");\n",
    "KNNhyperparameters6=Dict(\"n_neighbors\"=>7,\"weights\"=>\"distance\");\n",
    "\n",
    "\n",
    "# Call the modelCrossValidation function\n",
    "accuracyKNN1,confusionMatrixKNN1 = modelCrossValidation(:knn, KNNhyperparameters1, inputs, targets, vector);\n",
    "accuracyKNN2,confusionMatrixKNN2 = modelCrossValidation(:knn, KNNhyperparameters2, inputs, targets, vector);\n",
    "accuracyKNN3,confusionMatrixKNN3 = modelCrossValidation(:knn, KNNhyperparameters3, inputs, targets, vector);\n",
    "accuracyKNN4,confusionMatrixKNN4 = modelCrossValidation(:knn, KNNhyperparameters4, inputs, targets, vector);\n",
    "accuracyKNN5,confusionMatrixKNN5 = modelCrossValidation(:knn, KNNhyperparameters5, inputs, targets, vector);\n",
    "accuracyKNN6,confusionMatrixKNN6 = modelCrossValidation(:knn, KNNhyperparameters6, inputs, targets, vector);\n",
    "\n",
    "\n",
    "# Print the results\n",
    "println(\"KNN accuracy: \", mean(accuracyKNN1),\" KNN confusion matrix: \", mean(confusionMatrixKNN1));\n",
    "println(\"KNN accuracy: \", mean(accuracyKNN2),\" KNN confusion matrix: \", mean(confusionMatrixKNN2));\n",
    "println(\"KNN accuracy: \", mean(accuracyKNN3),\" KNN confusion matrix: \", mean(confusionMatrixKNN3));\n",
    "println(\"KNN accuracy: \", mean(accuracyKNN4),\" KNN confusion matrix: \", mean(confusionMatrixKNN4));\n",
    "println(\"KNN accuracy: \", mean(accuracyKNN5),\" KNN confusion matrix: \", mean(confusionMatrixKNN5));\n",
    "println(\"KNN accuracy: \", mean(accuracyKNN6),\" KNN confusion matrix: \", mean(confusionMatrixKNN6));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a5cd94",
   "metadata": {},
   "source": [
    "Comparing the results obtained in the experimentation, we found that kNN has proved to be one of the best fitting model for the given problem. It can be seen in the accuracies, which are above 0.95. With this model we really do not have so much parameters to adapt to the problem, so in all the experiment we get similar results. At first sight, the model works better when we use weights=\"uniform\", but the difference is minimal. The model also seems to have better results when we increase \"n_neighbors\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0b1d0f",
   "metadata": {},
   "source": [
    "### Final Conclusion\n",
    "Analyzing all the experiment we have performed, we can conclude that the best performing models for the given problem are SVMs and kNN, which both have accuracies between 0.95 - 0.98.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34af644c",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a7ded7",
   "metadata": {},
   "source": [
    "### Learn Julia\n",
    "\n",
    "In this assignment, it is necessary to pass parameters which are dependent on the model. To do this, the simplest way is to create a variable of type Dictionary (actually the type is `Dict`) which works in a similar way to Python. For example, to specify the parameters of an SVM, you could create a variable as follows:\n",
    "\n",
    "```Julia\n",
    "parameters = Dict(\"kernel\" => \"rbf\", \"degree\" => 3, \"gamma\" => 2, \"C\" => 1);\n",
    "```\n",
    "\n",
    "Another way of defining such a variable could be the following:\n",
    "\n",
    "```Julia\n",
    "parameters = Dict();\n",
    "\n",
    "parameters[\"kernel\"] = \"rbf\";\n",
    "parameters[\"kernelDegree\"] = 3;\n",
    "parameters[\"kernelGamma\"] = 2;\n",
    "parameters[\"C\"] = 1;\n",
    "```\n",
    "\n",
    "Once inside the function to be developed, the model parameters can be used to create the model objet as follows:\n",
    "\n",
    "```Julia\n",
    "model = SVC(kernel=parameters[\"kernel\"], \n",
    "    degree=parameters[\"kernelDegree\"], \n",
    "    gamma=parameters[\"kernelGamma\"], \n",
    "    C=parameters[\"C\"]);\n",
    "```\n",
    "\n",
    "In the same way, something similar could be done for decision trees and kNN.\n",
    "\n",
    "Another type of Julia that may be interesting for this assignment is the `Symbol` type. An object of this type can be any symbol you want, simply by typing its name after a colon (\":\"). In this practice, you can use it to indicate which model you want to train, for example `:ANN`, `:SVM`, `:DecisionTree` or `:kNN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7c8d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
