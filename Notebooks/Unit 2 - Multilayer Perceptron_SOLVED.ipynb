{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0c2c02",
   "metadata": {},
   "source": [
    "# Training Multilayer Perceptrons\n",
    "\n",
    "The aim of this tutorial is to learn how to train multilayer perceptrons in Julia. To do so, we will make use of the `Flux` library, whose documentation can be consulted at https://fluxml.ai/Flux.jl/.\n",
    "\n",
    "If not previously, the packege has to be installed on the system by executing the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a530ceb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg; Pkg.add(\"Flux\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe81995",
   "metadata": {},
   "source": [
    "Flux is a library which provides a set of functions to create neural networks with an arbitrary number of layers. This is a library designed to develop Deep Learning projects, whose ANNs usually have a large number of layers of different types, for example, convolutional or maxpooling layers.  In these exercises, only multilayer perceptrons will be developed, with fully-connected (dense) layers, with a maximum of two hidden layers. \n",
    "\n",
    "In order to implement an ANN in Julia, there is a function called `Chain`. This function receives as parameters the layers that the network will have (excluding the input layer, which does not perform any processing), which can be of different types.This function receives as parameters the layers that the network will have (excluding the input layer, which does not perform any processing), which can be of different types. Therefore, it is a function with a variable number of parameters.  Depending on the type of layer desired, there are different functions to create each one of them. A couple of examples are the functions `Conv`, which allows for the creation of convolutional layers, or `MaxPool`, which allows for the creation of MaxPooling layers. These layers are used in more advanced models that will be seen in other subjects of the programme and that are beyond the syllabus of this subject.  In this subject, as only multilayer perceptrons will be covered, the layers will always be created fully connected with the function `Dense`. This function accepts as parameters the number of inputs, outputs, and the transfer function of the neurons in the layer. In this sense, two different cases can be distinguished when creating ANNs, depending on the problem to be solved: \n",
    "\n",
    "- ***Regression problems***. In this type of problem, the output layer usually has a linear transfer function, while the hidden layers have a non-linear transfer function.  In the following examples, a sigmoidal transfer function is used as the transfer function in the hidden layers; for further information on other supported transfer functions, please refer to the library documentation.  \n",
    "\n",
    "  In the first example, an ANN is implemented with 10 inputs, a hidden layer with 5 neurons, and an output layer of 1 neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "013ea6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(10 => 5, σ),                    \u001b[90m# 55 parameters\u001b[39m\n",
       "  Dense(5 => 1),                        \u001b[90m# 6 parameters\u001b[39m\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m61 parameters, 500 bytes."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux;\n",
    "\n",
    "ann = Chain(    \n",
    "    Dense(10, 5, σ),    \n",
    "    Dense(5, 1, identity) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c7de9",
   "metadata": {},
   "source": [
    "* The second example builds an ANN with 15 inputs, two hidden layers with 12 and 5 neurons, and an output layer with 2 neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66502f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(15 => 12, σ),                   \u001b[90m# 192 parameters\u001b[39m\n",
       "  Dense(12 => 5, σ),                    \u001b[90m# 65 parameters\u001b[39m\n",
       "  Dense(5 => 2),                        \u001b[90m# 12 parameters\u001b[39m\n",
       ") \u001b[90m                  # Total: 6 arrays, \u001b[39m269 parameters, 1.426 KiB."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = Chain(    \n",
    "    Dense(15, 12, σ),    \n",
    "    Dense(12, 5, σ),    \n",
    "    Dense(5, 2, identity) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6a9bb0",
   "metadata": {},
   "source": [
    "**Warning:** Be aware that the number of neurons between the different layers has to match\n",
    "\n",
    "### Question\n",
    "What would happen if all the layers of the ANN have a linear transfer function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d892b3",
   "metadata": {},
   "source": [
    "***ANSWER***\n",
    "\n",
    "If all the layers of an artificial neural network (ANN) have a linear transfer function, the network would essentially become a linear model, regardless of its depth or the number of layers. This means that the network would not be able to capture complex non-linear patterns in the data, and its expressiveness and modeling capabilities would be severely limited. \n",
    "\n",
    "This is the mathematical proof: Being a neuron the aplkication of  $ f_1(x) = m_1 x + b_1 $, and similarly, for a second on the second layer $ f_2(x) = m_2 x + b_2 $, then if the result from the first neuron is transfered to the second one without a non-lineal conversion:\n",
    "\n",
    "$f_2(f_1(x)) = m_2 (m_1 x + b_1) + b_2 = m_2m_1x + m_2b_1 + b_2 = m_3x + b_3$\n",
    "\n",
    "the result is another expression that multiplies the input by a weight $m_3$ and adds a bias $b_3$, as a simple neuron does.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976afc9b",
   "metadata": {},
   "source": [
    "- ***Classification problems***.  In this case, as explained in theory class, two different situations are considered, depending on whether there are two classes or more than two classes, whereas not belonging to any class is treated as another class:\n",
    "1. When there are only two classes, this is usually referred to as positives and negatives. In this case, the desired outputs will be either 1 or 0, and a single hidden neuron is, therefore, present.  Thus, such neuron is desired to return values between 0 and 1, which will be interpreted as the degree of certainty the system has that the output is positive.To ensure it returns a bounded value between 0 and 1, a sigmoidal function is applied to the output layer.In the following example, an ANN is defined with a hidden layer and an output neuron; in this example, the sigmoid function has also been used in the hidden layer, but this could be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f813d6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(8 => 4, σ),                     \u001b[90m# 36 parameters\u001b[39m\n",
       "  Dense(4 => 1, σ),                     \u001b[90m# 5 parameters\u001b[39m\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m41 parameters, 420 bytes."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = Chain(    \n",
    "    Dense(8, 4, σ),    \n",
    "    Dense(4, 1, σ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5660d303",
   "metadata": {},
   "source": [
    "2. With more than two classes, you have one output neuron per class. The desired output of a pattern is 1 for the neuron of the class it belongs to, and 0 for the rest. This kind of encoding is known as one-hot-encoding.  In this way, the output of a neuron for a pattern can be interpreted as the degree of certainty that the pattern belongs to the class corresponding to that neuron.  Unlike the previous case, a sigmoidal transfer function is not applied to the outputs of each neuron in the output layer to bound the output between 0 and 1, but no function (identity function) is applied.  Instead, a `softmax` function is applied to the outputs of all the neurons, which takes unbounded numeric values and returns numeric values between 0 and 1 such that the sum of all values is 1.  Even though this function does not constitute a layer of neurons, sometimes this last `softmax` function is considered as an additional layer, and in fact, from the point of view of programming with the `Flux` library, it is effectively performed as if it were a last layer, as can be seen in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f162550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(4 => 5, σ),                     \u001b[90m# 25 parameters\u001b[39m\n",
       "  Dense(5 => 3),                        \u001b[90m# 18 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m43 parameters, 428 bytes."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = Chain(    \n",
    "    Dense(4, 5, σ),    \n",
    "    Dense(5, 3, identity),    \n",
    "    softmax )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fe51cb",
   "metadata": {},
   "source": [
    "Another possibility to use the `Chain` function is by successive calls, adding layers to an already created network.  To do this, the ellipsis operator is used when specifying arguments to a function. In the following example, an equivalent ANN is created by first creating an empty ANN and successively adding layers to it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1d83bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(4 => 5, σ),                     \u001b[90m# 25 parameters\u001b[39m\n",
       "  Dense(5 => 3),                        \u001b[90m# 18 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m43 parameters, 428 bytes."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = Chain(); \n",
    "ann = Chain(ann...,  Dense(4, 5, σ) );\n",
    "ann = Chain(ann...,  Dense(5, 3, identity) ); \n",
    "ann = Chain(ann...,  softmax )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c810a352",
   "metadata": {},
   "source": [
    "This variable, obtained in either form, can be used as a function.  For example, the matrix inputs created in the previous tutorial can be taken and passed to the ANN resulting in the outputs of the network by simply writing the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c627c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×150 Matrix{Float32}:\n",
       " 0.421241  0.418478  0.417456  0.417602  …  0.482924  0.496907  0.478473\n",
       " 0.253602  0.266094  0.25731   0.259605     0.254751  0.232275  0.252998\n",
       " 0.325156  0.315428  0.325235  0.322793     0.262325  0.270818  0.268529"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "using DelimitedFiles;\n",
    "dataset = readdlm(\"../iris/iris.data\",',');\n",
    "\n",
    "# Prepare the data\n",
    "inputs = convert(Array{Float32,2}, dataset[:,1:4]);\n",
    "\n",
    "targets = dataset[:,5];\n",
    "classes = unique(targets);\n",
    "numClasses = length(classes);\n",
    "oneHot = Array{Bool,2}(undef, length(targets), numClasses);\n",
    "for numClass = 1:numClasses\n",
    "    oneHot[:,numClass] .= (targets.==classes[numClass]);\n",
    "end;\n",
    "targets = oneHot;\n",
    "\n",
    "outputs = ann(inputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d85b7c",
   "metadata": {},
   "source": [
    "***Warning***: due to the formulation used in the world of ANNs, the input and output matrices for the ANN have each pattern in each column, not in each row. Therefore, each row of the input matrix will represent one of the input attributes, while in the output matrix, each row will correspond to one of the desired outputs of the ANN. Consequently, a transpose of the matrices created in the previous practice is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1758a307",
   "metadata": {},
   "source": [
    "Thus, even if the ANN is not properly trained, it may be interesting to make a call similar to this one to verify that the ANN has been created correctly. Once it has been verified that the ANN has been created correctly, it is time to train it. \n",
    "\n",
    "To train an ANN, following the workflow described in the theory class, the patterns are presented to the network, then the output is compared with the desired output, and finally a loss value is calculated.  This loss value will be used to modify the weights of the connections and bias. Therefore, a key point is to define this loss function, which will be different for regression and classification problems. The Flux library includes the Losses module with a large number of loss functions used to train NRs. In this subject only the most common ones will be used and, therefore, the first step is to load this module with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee788074",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux.Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c53217a",
   "metadata": {},
   "source": [
    "For any kind of problem, the usage of the `loss` functions is the same. The first argument is the outputs of the model, the second argument is the desired outputs (in both cases with a pattern in each column), and the third argument is the optional keyword `agg`, which indicates how to aggregate the loss values for each pattern. If no value is specified for this keyword, by default an average of all loss values for all patterns will be performed.\n",
    "\n",
    "* For a regression problem, the most commonly loss function is the **Mean Square Error** (MSE) between the model outputs and the desired targets.  This MSE function is already defined in the Losses module of Flux, although it is very simple to define. It can be used as follows, where `x` are the model inputs and `y` are the targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "275883f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(m, x, y) = Losses.mse(m(x), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fa6eac",
   "metadata": {},
   "source": [
    "  Other loss functions that may be of interest for regression problems are `Flux.Loss.mae` or `Flux.Loss.msle`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e93cb7d",
   "metadata": {},
   "source": [
    "* For a classification problem the loss function is different. As it was appointed during theory sessions, the binary cross-entropy function is the common choice for a  2-classes problem (only one output neuron)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "def3bcb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(m, x, y) = Losses.binarycrossentropy(m(x), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7266f73",
   "metadata": {},
   "source": [
    "  Whereas, the cross-entropy function is main used for problems with more than 2  classes (one output neuron per class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9c1d20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(m,x, y) = Losses.crossentropy(m,(x), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df78f212",
   "metadata": {},
   "source": [
    "Although, each of these functions could be declared in the various branches of an if statement, Julia has problems when making such function declarations. For this reason, these two declarations can be merged into one, with the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c79bcd17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(m, x, y) = (size(y,1) == 1) ? Losses.binarycrossentropy(m(x),y) : Losses.crossentropy(m(x),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1333f6f",
   "metadata": {},
   "source": [
    "### Question\n",
    "As it can be seen, first of all the number of rows of the desired output matrix (`y`) is checked, why is the number of rows checked and not the number of columns?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95b6608",
   "metadata": {},
   "source": [
    "***ANSWER***\n",
    "\n",
    "According to the convention used in Julia, input samples are store in columns so each column represent a sample of the data set. However, for the output when we have more than two classes such as this one, each partiticular sample process outputs as much outputs as clases in columns and the result for each sample is store in a row.\n",
    "Therefore, we need to chech if the number of input samples aligns to the number of outputs of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6dd73c",
   "metadata": {},
   "source": [
    "It is important to remember that in both `x` (inputs) and `y` (targets) each pattern must be in a column, contrary to the usual practice. For this reason, as will be shown below, the matrices of inputs and desired outputs will be transposed.\n",
    "\n",
    "These functions use the variable `ann`, which is used as a function as it was previously described.  Therefore, it needs to be defined within the environment in which the function is defined.\n",
    "\n",
    "Once the loss function has been defined, it is necessary to indicate the optimizer to be used during training. The optimizer is nothing more than a specific implementation of one of the alternative backpropagation algorithms. Flux has a large number of those implementations, from the classical one based on gradient descent (`Descent`) or adding also the momentum (`Momentum`) to more advanced ones: `Adam`, `RAdam`, `AdaMax`, `AdaGrad`, `AdaDelta`, `AMSGrad`, `RMSProp`, etc.  Possibly, nowadays, the most widely used optimizer is ADAM, to which you have to indicate the learning rate. This value is usualy a small amount, however, you can find more information in the documentation of the library. Therefore, we have to setup the optimizer we want to use, for example a definitiion of an `Adam` would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c08e278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(layers = ((weight = \u001b[32mLeaf(Adam(0.1, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; … ; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0], Float32[0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; … ; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(0.1, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam(0.1, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(0.1, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), ()),)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learningRate = 0.1;\n",
    "opt_state = Flux.setup(Adam(learningRate), ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b2fa9",
   "metadata": {},
   "source": [
    "For `learningRate`parameter, a value is defined beforehand in the range $(0.01, 0.1)$ which would control the updates of the mdoel. A common value is 0.01, although you should try different values until you find one that gives good results for the specific problem. Additionaly, the model is required in this case represented by `ann`.  \n",
    "\n",
    "Thus, once the optimizer is defined, training **an epoch** of the previous ANN can be done with the `train!` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce614f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.train!(loss, ann, [(inputs', targets')], opt_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bdc91f",
   "metadata": {},
   "source": [
    "**Important**. It is worth mentioning that in Julia, by convention, when a function is defined by adding `!` (bang) as the last symbol, it is understood taht it modifies the contents of one or more of its arguments, which has therefore been passed by reference.\n",
    "\n",
    "That is the case of the function `train!`, which has four arguments:\n",
    "\n",
    "1. The `loss` function, which has been previously defined\n",
    "2. The `ann` parameter contains the model to be trained. \n",
    "3. A set of patterns, inputs and targets. As you can see in the example, an array with only one element is being passed. This element is a tuple with two elements: the arrays of inputs and desired outputs.  This way of passing the patterns, which may seem cumbersome, has its motivation, since when the set of patterns is very large, calculating the modifications to the weights with all the patterns can be very costly.  For this reason, the patterns are usually divided into batches so that each update is done with only one of these batches. If this were done, the array passed as a parameter, instead of having one tuple, would have several, one per batch.  However, in the exercises to be carried out in this subject, this will not be done, and all the patterns will be passed together. \n",
    "  * **Important**: As indicated above, these matrices of inputs and targets have each pattern in a column, contrary to what is usual.  For this reason, the input and target matrices passed as parameters are transposed, i.e. instead of passing `inputs` and `targets`, `inputs'` and `targets'` are passed. If the input and/or target matrices already had a pattern in each column, there would be no need to transpose the corresponding matrix.\n",
    "  * **Important**: The matrices given in this parameter are used for training the ANN. Therefore, they have to be completely different from those used for testing.\n",
    "4. Optimizer.  Defined previously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10f9c4f",
   "metadata": {},
   "source": [
    "In this way only one loop is trained.  Therefore, to train an ANN it is necessary to create a loop that executes this function as long as some stop criterion is not met. Some of the most common criteria can be:\n",
    "\n",
    "* The loss in training is good enough.\n",
    "* The number of training cycles has reached a predefined maximum. \n",
    "* The change in training error is less than a predefined value. \n",
    "* etc.\n",
    "\n",
    "#### Question\n",
    "Could a similar stopping criterion to first one be made but with the test error? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bef25d",
   "metadata": {},
   "source": [
    "***ANSWER***\n",
    "\n",
    "The main reason for not using a similar stopping criterion based on the test error is that the test set must remain entirely independent of the training process. Introducing the test error as a stopping criterion could lead to biasing the training process toward the specific characteristics of the test set, making it less effective in evaluating the model's generalization capability to new, unseen data. To accurately assess a model's ability to generalize, it's essential to maintain the complete independence of the test set from the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d5c172",
   "metadata": {},
   "source": [
    "This way, it is possible to train an ANN so that the error or loss in the training set is minimised.   However, the training of ANNs is not deterministic, but has a random component, which is the random initialisation of the weights.  When this happens, to minimise the random component, the ANN is created and trained several times and the results are averaged.  If you want to train several ANNs, it will be necessary to nest two loops, where the outer loop will iterate through the different networks, and the inner loop will execute the different training cycles of each network.\n",
    "\n",
    "#### Question\n",
    "Where in the code (outside both loops, inside the first loop or inside the second loop) will it be necessary to put the call to the Chain function to create each network? Why shouldn't it be put elsewhere?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520901a",
   "metadata": {},
   "source": [
    "***ANSWER***\n",
    "\n",
    "The call to the Chain function to create each network should be placed inside the first loop. This is because each iteration of the outer loop corresponds to a specific network configuration or setup. Placing it outside both loops would mean that the same network would be trained in all iterations, essentially using a single network for all experiments. Conversely, putting the Chain function inside the second loop would lead to the network changing with every cycle of the inner loop, making it challenging to compare or analyze the performance of different network configurations. By placing it inside the first loop, you ensure that the second loop trains a specific network configuration, while the outer loop allows you to train and evaluate multiple networks with different settings or architectures. This structured approach facilitates systematic experimentation and comparison of various network setups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9389a2b",
   "metadata": {},
   "source": [
    "Through this process, the weights and biases, starting from initially random values, will take on different values until one of the stop criteria is met. \n",
    "\n",
    "In this sense, it is necessary to bear in mind that the weights corresponding to the connection of inputs with a very high absolute value will take on a low absolute value; on the other hand, the weights of those connections that connect inputs with a low absolute value will take on a high absolute value.  In this way, the ANN is able to combine inputs that are in very different ranges, passing them to values of similar scale.  similar scale values. In other words, the ANN is able to \"learn\" the relationship between the different scales on which the inputs move.\n",
    "\n",
    "When dealing with classification problems, the value of loss is often not easy to interpret.  For classification problems there are different metrics that will be the focus of later sessions.  For now, to assess the goodness of output of ANN, we will only use classification accuracy, defined as the ratio of well-classified patterns (number of well-classified patterns divided by the total number of patterns). Therefore, Two different cases can be identified:\n",
    "1. When there are two classes, the ANN has a single output neuron. As described above, a sigmoidal function, which returns a value between 0 and 1, is typically used as a transfer function. By simply passing a threshold (usually 0.5), the pattern can be classified as \"positive\" or \"negative\" depending on whether the output is greater or less than the threshold respectively.\n",
    "2. When there are more than two classes, applying the softmax function will result in different values of certainty, certainty or probability of belonging to each class.  Therefore, a pattern is classified as the class with the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084554cc",
   "metadata": {},
   "source": [
    "**Important**: This process of normalisation, although it is applied here to ANNs, is common to the rest of the Machine Learning techniques. Therefore, the code to be developed regarding normalisation will also be used in the other models.\n",
    "\n",
    "On the following paragraph the requirements of the different exercises to devop are explained with the signature of the methods that has to be developed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a324e1",
   "metadata": {},
   "source": [
    "1. Develop a function called oneHotEncoding, containing the code developed in the previous session regarding the encoding of a categorical input or output.  That is, to receive a vector of values and encode it as explained in the previous tutorial.  This function will receive two parameters, called `feature` and `classes`, both of type `AbstractArray{<:Any,1}`, that is, they are vectors containing any type of value. The first one has the values of that attribute or desired output for each pattern, and the second one has the values of the categories. This function should perform the following tasks:\n",
    "  * When the number of values in the class vector is equal to 2, the attribute vector is compared with one of the two classes by broadcasting the `==` operator to generate a vector of Boolean values.   This vector is then transformed into a two-dimensional matrix of one column and returned. To do the latter, see the reshape function.\n",
    "  * When the number of classes is greater than 2, first a matrix of boolean values is created (of type Array{Bool,2} or BitArray{2}) with as many rows as patterns and as many columns as categories (one column per category). Subsequently, iterate over each column/category, and assign the values of that column as the result of comparing the vector `feature` with the corresponding category by performing a broadcast as in the previous point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9c757c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "function oneHotEncoding(feature::AbstractArray{<:Any,1},      \n",
    "        classes::AbstractArray{<:Any,1})\n",
    "    # First we are going to set a line as defensive to check values\n",
    "    @assert(all([in(value, classes) for value in feature]));\n",
    "    \n",
    "    # Second defensive statement, check the number of classes\n",
    "    numClasses = length(classes);\n",
    "    @assert(numClasses>1)\n",
    "    \n",
    "    if (numClasses==2)\n",
    "        # Case with only two classes\n",
    "        oneHot = reshape(feature.==classes[1], :, 1);\n",
    "    else\n",
    "        #Case with more than two clases\n",
    "        oneHot =  BitArray{2}(undef, length(feature), numClasses);\n",
    "        for numClass = 1:numClasses\n",
    "            oneHot[:,numClass] .= (feature.==classes[numClass]);\n",
    "        end;\n",
    "    end;\n",
    "    return oneHot;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45faa4ff",
   "metadata": {},
   "source": [
    "2. Overload this function called oneHotEncoding in these two ways: \n",
    "  * One that receives a single parameter called `feature`, of type AbstractArray{<:Any,1}, which takes the categories and makes a call to the previous function. The unique function can be used to extract the categories. Develop this function without explicitly declaring it using the word function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ff8cb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function equivalent to \n",
    "# function oneHotEncoding(feature::AbstractArray{<:Any,1})\n",
    "\n",
    "oneHotEncoding(feature::AbstractArray{<:Any,1}) = oneHotEncoding(feature, unique(feature));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1b3f83",
   "metadata": {},
   "source": [
    "  * Another function that receives a single parameter called `feature`, of type AbstractArray{Bool,1}, and therefore it will be a vector that contains for each pattern only two possibilities. As there are two categories, the output array must have a single column, which has the same elements. A simple way to do this function is to use the reshape function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "613ffa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function oneHotEncoding(feature::AbstractArray{Bool,1})\n",
    "#    return reshape(feature, (length(feature), 1))\n",
    "#end;\n",
    "\n",
    "# It is prefirable an overload of the method\n",
    "oneHotEncoding(feature::AbstractArray{Bool,1}) = reshape(feature, :, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7a436a",
   "metadata": {},
   "source": [
    "3. Develop a set of functions to normalise the data using the code developed in the previous practice.  For this, develop the following functions: \n",
    "  * Two functions called `calculateMinMaxNormalizationParameters` and `calculateZeroMeanNormalizationParameters` that receive a parameter of type `AbstractArray{<:Real,2}` and return a tuple with two values, each of them being a matrix with one row with the minimum and maximum values for each column (first function) or means and standard deviations for each column (second function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "537abc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function calculateMinMaxNormalizationParameters(dataset::AbstractArray{<:Real,2})\n",
    "    return minimum(dataset, dims=1), maximum(dataset, dims=1)\n",
    "end;\n",
    "\n",
    "# Alternative more compact definition\n",
    "#calculateMinMaxNormalizationParameters(dataset::AbstractArray{<:Real,2}) = ( minimum(dataset, dims=1), maximum(dataset, dims=1) );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5cc0f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "function calculateZeroMeanNormalizationParameters(dataset::AbstractArray{<:Real,2})\n",
    "    return mean(dataset, dims=1), std(dataset, dims=1)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29062af",
   "metadata": {},
   "source": [
    "  * Develop four functions to normalise between maximum and minimum. \n",
    "    * The first one, called normalizeMinMax! receives two parameters, an array of values to normalise (of type `AbstractArray{<:Real,2})` and the normalisation parameters (of type `NTuple{2, AbstractArray{<:Real,2}}`), executes the code of the previous tutorial referred to normalise between maximum and minimum, and returns the same array with the normalised data. Subsequently, make another function with the same name (overloaded) but with a single parameter that is the data matrix, and what it will do is to calculate the normalisation parameters with the function developed in the previous point and call the function normalizeMinMax! These two functions end in `!` because the array of values passed as a parameter is modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7376c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function normalizeMinMax!(dataset::AbstractArray{<:Real,2},      \n",
    "        normalizationParameters::NTuple{2, AbstractArray{<:Real,2}})\n",
    "    minValues = normalizationParameters[1];\n",
    "    maxValues = normalizationParameters[2];\n",
    "    dataset .-= minValues;\n",
    "    dataset ./= (maxValues .- minValues);\n",
    "    # eliminate any atribute that do not add information\n",
    "    dataset[:, vec(minValues.==maxValues)] .= 0;\n",
    "    return dataset;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74c52803",
   "metadata": {},
   "outputs": [],
   "source": [
    "function normalizeMinMax!(dataset::AbstractArray{<:Real,2})\n",
    "    normalizeMinMax!(dataset , calculateMinMaxNormalizationParameters(dataset));\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63822b75",
   "metadata": {},
   "source": [
    "  Make two other functions with the name normalizeMinMax that do the same, but do not modify the data matrix, i.e. create a new one, modify it and return it.  To do this, see the copy function.  These two functions should make calls to the previous functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "090c6221",
   "metadata": {},
   "outputs": [],
   "source": [
    "function normalizeMinMax( dataset::AbstractArray{<:Real,2},      \n",
    "                normalizationParameters::NTuple{2, AbstractArray{<:Real,2}}) \n",
    "    normalizeMinMax!(copy(dataset), normalizationParameters);\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "faa814c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "function normalizeMinMax( dataset::AbstractArray{<:Real,2})\n",
    "      normalizeMinMax!(copy(dataset), calculateMinMaxNormalizationParameters(dataset));\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25f1866",
   "metadata": {},
   "source": [
    "- Develop four similar functions for the case of performing a 0-mean normalisation, whose names are `normalizeZeroMean!` and  `normalizeZeroMean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca6e6c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function normalizeZeroMean!(dataset::AbstractArray{<:Real,2},      \n",
    "                        normalizationParameters::NTuple{2, AbstractArray{<:Real,2}}) \n",
    "    avgValues = normalizationParameters[1];\n",
    "    stdValues = normalizationParameters[2];\n",
    "    dataset .-= avgValues;\n",
    "    dataset ./= stdValues;\n",
    "    # Remove any atribute that do not have information\n",
    "    dataset[:, vec(stdValues.==0)] .= 0;\n",
    "    return dataset; \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bbe49ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "function normalizeZeroMean!(dataset::AbstractArray{<:Real,2})\n",
    "    normalizeZeroMean!(dataset , calculateZeroMeanNormalizationParameters(dataset));   \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbdf7502",
   "metadata": {},
   "outputs": [],
   "source": [
    "function normalizeZeroMean( dataset::AbstractArray{<:Real,2},      \n",
    "                            normalizationParameters::NTuple{2, AbstractArray{<:Real,2}})\n",
    "    normalizeZeroMean!(copy(dataset), normalizationParameters);\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c164721",
   "metadata": {},
   "outputs": [],
   "source": [
    "function normalizeZeroMean( dataset::AbstractArray{<:Real,2}) \n",
    "    normalizeZeroMean!(copy(dataset), calculateZeroMeanNormalizationParameters(dataset));\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50d4550",
   "metadata": {},
   "source": [
    "4. Develop a function called `classifyOutputs`, which receives a parameter called outputs of type `AbstractArray{<:Real,2}`. It will contain the outputs of a model (not necessarily for an ANN) with `a single sample/patten in each row` and convert it to an array of boolean values that each row only has a value of true, indicating the class to which that pattern is classified.  To do this, first look at the number of columns the outputs matrix has, and then do the following: \n",
    "\n",
    "   * If you have one column, compare the matrix with a threshold value by broadcasting the >= operator to generate a matrix of boolean values of a column to be returned. To do this, the function needs to receive the threshold value as an optional parameter, with a default value of 0.5. \n",
    "\n",
    "   * If you have more than one column, you must create an array of boolean values of the same size, and, for each row, set the column with a larger value to true. This can be done without writing any loops. \n",
    "\n",
    "  This aim of this exercise, together with the next one, is to develop skills in  vector programming. Below this lines are the steps that could be taken to write the code for this second scenario, when you have more than one column (assuming that each pattern is in each row):\n",
    "  \n",
    "   * First, for each row, it needs to be found out in which column is the maximum output value for each pattern. This can be done with the `findmax` function, which returns a tuple with two values: the maximum in each row or column, and the coordinates in the matrix in which that maximum was found, which is what we are really interested in. With the keyword `dims` you can indicate whether you want to search for the maximums in the rows or in the columns.  The line of code would be as follows:\n",
    "   ```julia\n",
    "        (_,indicesMaxEachInstance) = findmax(outputs, dims=2);\n",
    "    ```\n",
    "  * Once you have it, you can create a boolean array of the same dimensionality as the output array, where each value indicates the membership of the corresponding class of that pattern. This matrix is initialised to false, so it can be easily created with the function `false`. Such as:\n",
    " ```julia\n",
    "        outputs = falses(size(outputs));\n",
    "    ``` \n",
    "  * Finally, the values of the indices containing the largest values of each row, which are collected in the `indicesMaxEachInstance` variable created earlier, are set to true in this array.  This can be done by comparing with the `outputs` array as follows:\n",
    " ```julia\n",
    "        outputs[indicesMaxEachInstance] .= true;\n",
    "    ``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b85e30ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "function classifyOutputs(outputs::AbstractArray{<:Real,2}; \n",
    "                        threshold::Real=0.5) \n",
    "   numOutputs = size(outputs, 2);\n",
    "    @assert(numOutputs!=2)\n",
    "    if numOutputs==1\n",
    "        return outputs.>=threshold;\n",
    "    else\n",
    "        # Look for the maximum value using the findmax funtion\n",
    "        (_,indicesMaxEachInstance) = findmax(outputs, dims=2);\n",
    "        # Set up then boolean matrix to everything false while max values aretrue.\n",
    "        outputs = falses(size(outputs));\n",
    "        outputs[indicesMaxEachInstance] .= true;\n",
    "        # Defensive check if all patterns are in a single class\n",
    "        @assert(all(sum(outputs, dims=2).==1));\n",
    "        return outputs;\n",
    "    end;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3bce71",
   "metadata": {},
   "source": [
    "5.  Develop a function called `accuracy` given a matrix of desired outputs (`targets`) and a matrix of outputs emitted by a model (not necessarily for an ANN), calculate the accuracy in a classification problem.  Both matrices should have a number of rows equal to the number of patterns, i.e. each pattern will be placed in each row. Develop this function in such a way that it works for the cases of having 2 classes (one output neuron) as well as more than two classes (one output neuron per class).\n",
    "\n",
    "    To develop this function, four functions with the same name must be carried out:\n",
    "    \n",
    "    * In the first one, `targets` and `outputs` are of type `AbstractArray{Bool,1}`, i.e. vectors of boolean values.  The precision will simply be the average value of the comparison of both vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "388a808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics;\n",
    "\n",
    "function accuracy(outputs::AbstractArray{Bool,1}, targets::AbstractArray{Bool,1}) \n",
    "    mean(outputs.==targets);\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e942f1e",
   "metadata": {},
   "source": [
    "  * In the second of the functions, `targets` and `outputs` are of type `AbstractArray{Bool,2}`, i.e. two-dimensional arrays of Boolean values.  In this case, it will be necessary to examine the number of columns.  If they have only one column, a call to the above function will be made taking the first column of targets and outputs as vectors. If the number of columns is greater than 2, it will be necessary to compare both matrices to see in which rows the values do not match.\n",
    "  \n",
    "### Question\n",
    " What would happen if the number of columns is equal to 2? (put your answer into a comment in the code below)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88daf403",
   "metadata": {},
   "outputs": [],
   "source": [
    "function accuracy(outputs::AbstractArray{Bool,2}, targets::AbstractArray{Bool,2}) \n",
    "    @assert(all(size(outputs).==size(targets)));\n",
    "    if (size(targets,2)==1)\n",
    "        return accuracy(outputs[:,1], targets[:,1]);\n",
    "    else\n",
    "        return mean(all(targets .== outputs, dims=2));\n",
    "    end;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff28ee46",
   "metadata": {},
   "source": [
    "  * In the third of the functions, `targets` will be of type `AbstractArray{Bool,1}` (a vector of boolean values) while `outputs` will be of type `AbstractArray{<:Real,1}`, i.e. real outputs that have not yet been interpreted as \"positive\"/\"negative\" class membership values.  In this case, the function could accept an optional `threshold` parameter with a default value of 0.5, and what it would do would be to pass the threshold to the outputs vector, and call the first `accuracy` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2642d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "function accuracy(outputs::AbstractArray{<:Real,1}, targets::AbstractArray{Bool,1};      \n",
    "                threshold::Real=0.5)\n",
    "    accuracy(outputs.>=threshold, targets);\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3079a0a5",
   "metadata": {},
   "source": [
    "  * Finally, in the last of the functions, `targets` will be of type `AbstractArray{Bool,2}` (an array of boolean values) while `outputs` will be of type `AbstractArray{<:Real,2}`, i.e. be real outputs that have not yet been interpreted as values belonging to N classes (N: number of columns). In this case, it is again necessary to distinguish whether you have 1 or more than 2 columns. In the first case, a call to `accuracy` should be made with the vectors corresponding to the first column of outputs and targets. In the second case, a call should be made to the function classifyOutputs to convert outputs into a variable of type AbstractArray{Bool,2}, and then make a call to `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84b032c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "function accuracy(outputs::AbstractArray{<:Real,2}, targets::AbstractArray{Bool,2};\n",
    "                threshold::Real=0.5)\n",
    "    @assert(all(size(outputs).==size(targets)));\n",
    "    if (size(targets,2)==1)\n",
    "        return accuracy(outputs[:,1], targets[:,1]);\n",
    "    else\n",
    "        return accuracy(classifyOutputs(outputs; threshold=threshold), targets);\n",
    "    end;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e728ae87",
   "metadata": {},
   "source": [
    "As in the previous exercise, due to the aim of this one is to develop the vectorial programming skills of the students, the main steps will be pointed out considering that we want ot calculate the accurry of more than 2 classes, the samples are in the rows of the matrices, and both outputs and targgets matrices are `AbstractArray{Bool, 2}`.\n",
    "\n",
    "  * Once the outputs variable has the desired form (array of Boolean values), it is compared with the targets array as follows: \n",
    "   ```julia\n",
    "        classComparison = targets .== outputs\n",
    "    ```\n",
    "  * In this new array, for each pattern, when the class matches, all elements of that row will be true. On the other hand, when the class does not match, more than one element in that row will be false. Therefore, one way to know for a pattern if it is well sorted is to look if in its row all elements are true.  This can be checked with the function `all`, which receives an array and returns true if all elements are true, but also accepts the keyword `dims`, with which it applies this same function across the given dimension. To do this on rows, you would have to do like this:\n",
    "   ```julia\n",
    "        correctClassifications = all(classComparison, dims=2)\n",
    "    ```\n",
    "  * Finally, the only thing left to do is to average this matrix. Remember that you can operate with boolean values in the same way as with real values, in this case they will be treated as 0 or 1. \n",
    "     ```julia\n",
    "        accuracy = mean(correctClassifications)\n",
    "    ```\n",
    "  * These last steps could have been performed by looking, instead of the matches between the two arrays, at the patterns where there are no matches.  For that you can use the function `any`, which receives an array of boolean values and returns true if there is any value equal to true, and also accepts the keyword `dims`. This would calculate the error rate instead of the accuracy, but the accuracy can be calculated from the error rate by simply subtracting it from 1. The code would look like:\n",
    "      ```julia\n",
    "        classComparison = targets .!= outputs \n",
    "        incorrectClassifications = any(classComparison, dims=2)\n",
    "        accuracy = 1 - mean(incorrectClassifications)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c517c72",
   "metadata": {},
   "source": [
    "6. Develop a function to create ANNs to solve classification problems.  This function must receive the `topology` (number of hidden layers and neurons in each, and optionally activation functions in each hidden layer, the number of input neurons and the number of output neurons).\n",
    "   **Important** It is worth pointing out that the transfer function of the output layer is not given by the user but by the problem itself (regression/classification).  Similarly, the number of neurons in the input and output layers is given by the problem to be solved.\n",
    "   A straightforward way to create this ANN is to receive the topology as a parameter called topology of type AbstractArray{<:Int,1}, which contains the number of neurons in each hidden layer (empty for networks without hidden layers), and create the ANN as follows:\n",
    "   * Create an empty ANN\n",
    "   ```julia\n",
    "            ann = Chain();\n",
    "   ```\n",
    "   * Create a variable called numInputsLayer, initialized to the number of inputs of the ANN.\n",
    "   * If there are hidden layers, i.e. if the `topology` vector is not empty, iterate through this vector (the value of the loop will be equal to the number of neurons in each layer) and in each iteration create a hidden layer where the number of inputs will be equal to the value of the numInputsLayer variable and the number of outputs equal to the current value of the loop. If the transfer function of each hidden layer has not been specified, use the same transfer function σ in all hidden layers. After this, update the value of numInputsLayer to the value used in that iteration.\n",
    "   ```julia\n",
    "            for numOutputsLayer = topology      \n",
    "                ann = Chain(ann..., Dense(numInputsLayer, numOutputsLayer, σ) );      \n",
    "                numInputsLayer = numOutputsLayer; \n",
    "            end;\n",
    "   ```\n",
    "   If these lines are written in the script without being inside a loop or function, compiling the code will give several warnings and the code will not properly work. This is automatically corrected by using this code inside a function.  To use it in the main body, you should write `global ann, numInputsLayer;` at the beginning of the loop.\n",
    "   * Finally, add the final layer, with the number of neurons and transfer function appropriate to the number of classes as described above, adding the `softmax` function if there are more than two output classes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "686ad280",
   "metadata": {},
   "outputs": [],
   "source": [
    "function buildClassANN(numInputs::Int, topology::AbstractArray{<:Int,1}, numOutputs::Int;\n",
    "                    transferFunctions::AbstractArray{<:Function,1}=fill(σ, length(topology))) \n",
    "    ann=Chain();\n",
    "    numInputsLayer = numInputs;\n",
    "    for numHiddenLayer in 1:length(topology)\n",
    "        numNeurons = topology[numHiddenLayer];\n",
    "        ann = Chain(ann..., Dense(numInputsLayer, numNeurons, transferFunctions[numHiddenLayer]));\n",
    "        numInputsLayer = numNeurons;\n",
    "    end;\n",
    "    if (numOutputs == 1)\n",
    "        ann = Chain(ann..., Dense(numInputsLayer, 1, σ));\n",
    "    else\n",
    "        ann = Chain(ann..., Dense(numInputsLayer, numOutputs, identity));\n",
    "        ann = Chain(ann..., softmax);\n",
    "    end;\n",
    "    return ann;\n",
    "end;                                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d4e6a2",
   "metadata": {},
   "source": [
    "7. Develop a function that creates an ANN to perform classification (via a call to the above function) and trains it.   To do this, this function should implement a loop in which the ANN is trained with the training set passed as a parameter until one of the stop criteria passed as parameters is met.  The function should return the trained ANN.  The argumentes shopuld be:\n",
    "  * **topology**, of type `AbstractArray{<:Int,1}` with the topology (hidden layers) of the ANN. \n",
    "  * **dataset**, of type `Tuple{AbstractArray{<:Real,2},  AbstractArray{Bool,2}} with the  matrix of inputs and targets.  From these matrices, the number of input and output neurons necessary to call the previous function can be obtained by means of the size function.\n",
    "  * Optional parameters controlling other aspects of the algorithm such as the stopping criteria of the algorithm, with default values:\n",
    "    * **maxEpochs**, of type Int, with a default value of 1000\n",
    "    * **minLoss**, of type Real, with a default value of 0\n",
    "    * **learningRate**, of type Real, with a default value of 0.01.\n",
    "    \n",
    "  It is important to highlight that the set of patterns used here will have each pattern in a row, while the Flux library (in general, most Genetic Algorithms libraries do) expect matrices in which each pattern is in a column.  This is not a problem, matrices only need to be transposed on certain occasions, such as when supplying the function to train an ANN cycle, when calculating the loss value, or when taking the output matrices obtained by running the ANN.\n",
    "  \n",
    "  Bear in mind that a call to the train! function trains the ANN for one loop only. Therefore, it is necessary, first, to create the ANN by calling the above function, and then to run a loop that calls the `train!` function only once in each iteration.  That is, if you want to train for n cycles, this loop will have to perform `n` iterations.\n",
    "  \n",
    "  The output of this function should be at least the trained ANN and a vector with the loss values for each training cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ed65c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function trainClassANN(topology::AbstractArray{<:Int,1},      \n",
    "                    dataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,2}};\n",
    "                    transferFunctions::AbstractArray{<:Function,1}=fill(σ, length(topology)),\n",
    "                    maxEpochs::Int=1000, minLoss::Real=0.0, learningRate::Real=0.01) \n",
    "\n",
    "    (inputs, targets) = dataset;\n",
    "    \n",
    "    # This function assumes that each sumple is in a row\n",
    "    # we are going to check the numeber of samples to have same inputs and targets\n",
    "    @assert(size(inputs,1)==size(targets,1));\n",
    "\n",
    "    # We define the ANN\n",
    "    ann = buildClassANN(size(inputs,2), topology, size(targets,2));\n",
    "\n",
    "    # Setting up the loss funtion to reduce the error\n",
    "    loss(model,x,y) = (size(y,1) == 1) ? Losses.binarycrossentropy(model(x),y) : Losses.crossentropy(model(x),y);\n",
    "\n",
    "    # This vectos is going to contain the losses and precission on each training epoch\n",
    "    trainingLosses = Float32[];\n",
    "\n",
    "    # Inicialize the counter to 0\n",
    "    numEpoch = 0;\n",
    "    # Calcualte the loss without training\n",
    "    trainingLoss = loss(ann, inputs', targets');\n",
    "    #  Store this one for checking the evolution.\n",
    "    push!(trainingLosses, trainingLoss);\n",
    "    #  and give some feedback on the screen\n",
    "    println(\"Epoch \", numEpoch, \": loss: \", trainingLoss);\n",
    "\n",
    "    # Define the optimazer for the network\n",
    "    opt_state = Flux.setup(Adam(learningRate), ann);\n",
    "\n",
    "    # Start the training until it reaches one of the stop critteria\n",
    "    while (numEpoch<maxEpochs) && (trainingLoss>minLoss)\n",
    "\n",
    "        # For each epoch, we habve to train and consequently traspose the pattern to have then in columns\n",
    "        Flux.train!(loss, ann, [(inputs', targets')], opt_state);\n",
    "\n",
    "        numEpoch += 1;\n",
    "        # calculate the loss for this epoch\n",
    "        trainingLoss = loss(ann, inputs', targets');\n",
    "        # store it\n",
    "        push!(trainingLosses, trainingLoss);\n",
    "        # shown it\n",
    "        println(\"Epoch \", numEpoch, \": loss: \", trainingLoss);\n",
    "\n",
    "    end;\n",
    "\n",
    "    # return the network and the evolution of the error\n",
    "    return (ann, trainingLosses);\n",
    "end;                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867622f0",
   "metadata": {},
   "source": [
    "8. Occasionally, when the classification problem is of two classes, instead of having the desired outputs as a single column matrix, it will be as a vector.  To deal with these cases, a function of the same name as above and accepting the same arguments is requested, except that the second argument, dataset, is of type `Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,1}}`. This function should only convert the desired output vector into an array with one column and call the above function.  To do this conversion, see the reshape function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eef7b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "function trainClassANN(topology::AbstractArray{<:Int,1},      \n",
    "                    (inputs, targets)::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,1}};      \n",
    "                    transferFunctions::AbstractArray{<:Function,1}=fill(σ, length(topology)),      \n",
    "                    maxEpochs::Int=1000, minLoss::Real=0.0, learningRate::Real=0.01)\n",
    "    \n",
    "     trainClassANN(topology, (inputs, reshape(targets, length(targets), 1)); \n",
    "        maxEpochs=maxEpochs, minLoss=minLoss, learningRate=learningRate);\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ffa64",
   "metadata": {},
   "source": [
    "#### Warning\n",
    "Remember to integrate this functions with the code developed on the previous tutorial in a separate file.\n",
    "\n",
    "Once the training function returns a trained ANN, it can be used to simulated on different problems by passing it a set of inputs (with the patterns in columns, i.e. transposed) and will return the outputs for that set (again, the patterns will be in columns, so the outputs of the ANN will have to be transposed).  These outputs, along with the desired outputs for that set of inputs, can be applied to the accuracy function to calculate the accuracy on that set. \n",
    "\n",
    "Run multiple times to train different networks with different architectures - which one will give the highest accuracy on the training set? Also test different learning rate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c2977e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix{Float32}\n",
      "Epoch 0: loss: 1.2880982\n",
      "Epoch 1: loss: 1.2707098\n",
      "Epoch 2: loss: 1.2541871\n",
      "Epoch 3: loss: 1.2385417\n",
      "Epoch 4: loss: 1.2237797\n",
      "Epoch 5: loss: 1.2098997\n",
      "Epoch 6: loss: 1.1968985\n",
      "Epoch 7: loss: 1.1847674\n",
      "Epoch 8: loss: 1.1734939\n",
      "Epoch 9: loss: 1.1630623\n",
      "Epoch 10: loss: 1.1534518\n",
      "Epoch 11: loss: 1.1446364\n",
      "Epoch 12: loss: 1.1365874\n",
      "Epoch 13: loss: 1.1292697\n",
      "Epoch 14: loss: 1.1226461\n",
      "Epoch 15: loss: 1.1166753\n",
      "Epoch 16: loss: 1.1113144\n",
      "Epoch 17: loss: 1.1065176\n",
      "Epoch 18: loss: 1.1022384\n",
      "Epoch 19: loss: 1.0984305\n",
      "Epoch 20: loss: 1.0950477\n",
      "Epoch 21: loss: 1.0920432\n",
      "Epoch 22: loss: 1.0893741\n",
      "Epoch 23: loss: 1.0869968\n",
      "Epoch 24: loss: 1.0848718\n",
      "Epoch 25: loss: 1.0829599\n",
      "Epoch 26: loss: 1.0812258\n",
      "Epoch 27: loss: 1.0796355\n",
      "Epoch 28: loss: 1.0781592\n",
      "Epoch 29: loss: 1.0767676\n",
      "Epoch 30: loss: 1.0754349\n",
      "Epoch 31: loss: 1.0741392\n",
      "Epoch 32: loss: 1.0728583\n",
      "Epoch 33: loss: 1.0715756\n",
      "Epoch 34: loss: 1.0702745\n",
      "Epoch 35: loss: 1.0689416\n",
      "Epoch 36: loss: 1.0675656\n",
      "Epoch 37: loss: 1.0661356\n",
      "Epoch 38: loss: 1.0646449\n",
      "Epoch 39: loss: 1.0630859\n",
      "Epoch 40: loss: 1.0614531\n",
      "Epoch 41: loss: 1.059741\n",
      "Epoch 42: loss: 1.0579462\n",
      "Epoch 43: loss: 1.0560652\n",
      "Epoch 44: loss: 1.0540946\n",
      "Epoch 45: loss: 1.0520314\n",
      "Epoch 46: loss: 1.0498735\n",
      "Epoch 47: loss: 1.0476176\n",
      "Epoch 48: loss: 1.0452609\n",
      "Epoch 49: loss: 1.0428016\n",
      "Epoch 50: loss: 1.0402359\n",
      "Epoch 51: loss: 1.0375618\n",
      "Epoch 52: loss: 1.0347759\n",
      "Epoch 53: loss: 1.0318753\n",
      "Epoch 54: loss: 1.0288568\n",
      "Epoch 55: loss: 1.0257179\n",
      "Epoch 56: loss: 1.0224556\n",
      "Epoch 57: loss: 1.0190662\n",
      "Epoch 58: loss: 1.0155475\n",
      "Epoch 59: loss: 1.0118961\n",
      "Epoch 60: loss: 1.0081092\n",
      "Epoch 61: loss: 1.0041845\n",
      "Epoch 62: loss: 1.000119\n",
      "Epoch 63: loss: 0.99590933\n",
      "Epoch 64: loss: 0.99155426\n",
      "Epoch 65: loss: 0.9870512\n",
      "Epoch 66: loss: 0.98239756\n",
      "Epoch 67: loss: 0.9775922\n",
      "Epoch 68: loss: 0.9726335\n",
      "Epoch 69: loss: 0.9675198\n",
      "Epoch 70: loss: 0.96225065\n",
      "Epoch 71: loss: 0.95682627\n",
      "Epoch 72: loss: 0.95124644\n",
      "Epoch 73: loss: 0.945512\n",
      "Epoch 74: loss: 0.939624\n",
      "Epoch 75: loss: 0.9335851\n",
      "Epoch 76: loss: 0.92739725\n",
      "Epoch 77: loss: 0.92106366\n",
      "Epoch 78: loss: 0.9145885\n",
      "Epoch 79: loss: 0.90797627\n",
      "Epoch 80: loss: 0.90123177\n",
      "Epoch 81: loss: 0.89436126\n",
      "Epoch 82: loss: 0.8873709\n",
      "Epoch 83: loss: 0.8802679\n",
      "Epoch 84: loss: 0.8730598\n",
      "Epoch 85: loss: 0.86575437\n",
      "Epoch 86: loss: 0.8583606\n",
      "Epoch 87: loss: 0.8508877\n",
      "Epoch 88: loss: 0.84334475\n",
      "Epoch 89: loss: 0.83574176\n",
      "Epoch 90: loss: 0.82808876\n",
      "Epoch 91: loss: 0.8203962\n",
      "Epoch 92: loss: 0.81267476\n",
      "Epoch 93: loss: 0.8049349\n",
      "Epoch 94: loss: 0.79718775\n",
      "Epoch 95: loss: 0.789444\n",
      "Epoch 96: loss: 0.7817142\n",
      "Epoch 97: loss: 0.7740093\n",
      "Epoch 98: loss: 0.7663397\n",
      "Epoch 99: loss: 0.7587156\n",
      "Epoch 100: loss: 0.7511472\n",
      "Epoch 101: loss: 0.7436438\n",
      "Epoch 102: loss: 0.73621523\n",
      "Epoch 103: loss: 0.7288695\n",
      "Epoch 104: loss: 0.7216156\n",
      "Epoch 105: loss: 0.7144614\n",
      "Epoch 106: loss: 0.70741385\n",
      "Epoch 107: loss: 0.7004802\n",
      "Epoch 108: loss: 0.6936663\n",
      "Epoch 109: loss: 0.6869777\n",
      "Epoch 110: loss: 0.6804197\n",
      "Epoch 111: loss: 0.67399645\n",
      "Epoch 112: loss: 0.6677117\n",
      "Epoch 113: loss: 0.6615689\n",
      "Epoch 114: loss: 0.6555705\n",
      "Epoch 115: loss: 0.6497184\n",
      "Epoch 116: loss: 0.64401436\n",
      "Epoch 117: loss: 0.63845915\n",
      "Epoch 118: loss: 0.63305324\n",
      "Epoch 119: loss: 0.6277967\n",
      "Epoch 120: loss: 0.622689\n",
      "Epoch 121: loss: 0.6177291\n",
      "Epoch 122: loss: 0.6129161\n",
      "Epoch 123: loss: 0.608248\n",
      "Epoch 124: loss: 0.603723\n",
      "Epoch 125: loss: 0.5993387\n",
      "Epoch 126: loss: 0.59509295\n",
      "Epoch 127: loss: 0.59098274\n",
      "Epoch 128: loss: 0.58700514\n",
      "Epoch 129: loss: 0.5831571\n",
      "Epoch 130: loss: 0.57943565\n",
      "Epoch 131: loss: 0.57583696\n",
      "Epoch 132: loss: 0.57235783\n",
      "Epoch 133: loss: 0.5689947\n",
      "Epoch 134: loss: 0.5657443\n",
      "Epoch 135: loss: 0.56260246\n",
      "Epoch 136: loss: 0.5595659\n",
      "Epoch 137: loss: 0.5566311\n",
      "Epoch 138: loss: 0.5537942\n",
      "Epoch 139: loss: 0.5510518\n",
      "Epoch 140: loss: 0.5484003\n",
      "Epoch 141: loss: 0.54583585\n",
      "Epoch 142: loss: 0.54335594\n",
      "Epoch 143: loss: 0.5409565\n",
      "Epoch 144: loss: 0.53863436\n",
      "Epoch 145: loss: 0.53638625\n",
      "Epoch 146: loss: 0.5342092\n",
      "Epoch 147: loss: 0.53209996\n",
      "Epoch 148: loss: 0.53005594\n",
      "Epoch 149: loss: 0.5280737\n",
      "Epoch 150: loss: 0.52615094\n",
      "Epoch 151: loss: 0.52428454\n",
      "Epoch 152: loss: 0.522472\n",
      "Epoch 153: loss: 0.5207109\n",
      "Epoch 154: loss: 0.51899874\n",
      "Epoch 155: loss: 0.5173333\n",
      "Epoch 156: loss: 0.5157118\n",
      "Epoch 157: loss: 0.51413244\n",
      "Epoch 158: loss: 0.51259285\n",
      "Epoch 159: loss: 0.51109105\n",
      "Epoch 160: loss: 0.5096253\n",
      "Epoch 161: loss: 0.50819314\n",
      "Epoch 162: loss: 0.5067929\n",
      "Epoch 163: loss: 0.50542325\n",
      "Epoch 164: loss: 0.5040818\n",
      "Epoch 165: loss: 0.5027672\n",
      "Epoch 166: loss: 0.5014777\n",
      "Epoch 167: loss: 0.5002117\n",
      "Epoch 168: loss: 0.4989678\n",
      "Epoch 169: loss: 0.49774438\n",
      "Epoch 170: loss: 0.49653992\n",
      "Epoch 171: loss: 0.49535328\n",
      "Epoch 172: loss: 0.49418288\n",
      "Epoch 173: loss: 0.4930275\n",
      "Epoch 174: loss: 0.49188593\n",
      "Epoch 175: loss: 0.49075663\n",
      "Epoch 176: loss: 0.4896385\n",
      "Epoch 177: loss: 0.48853073\n",
      "Epoch 178: loss: 0.48743138\n",
      "Epoch 179: loss: 0.48634008\n",
      "Epoch 180: loss: 0.48525527\n",
      "Epoch 181: loss: 0.48417604\n",
      "Epoch 182: loss: 0.4831013\n",
      "Epoch 183: loss: 0.48202989\n",
      "Epoch 184: loss: 0.4809611\n",
      "Epoch 185: loss: 0.47989354\n",
      "Epoch 186: loss: 0.47882676\n",
      "Epoch 187: loss: 0.4777594\n",
      "Epoch 188: loss: 0.47669077\n",
      "Epoch 189: loss: 0.4756199\n",
      "Epoch 190: loss: 0.4745461\n",
      "Epoch 191: loss: 0.47346848\n",
      "Epoch 192: loss: 0.47238633\n",
      "Epoch 193: loss: 0.47129864\n",
      "Epoch 194: loss: 0.47020513\n",
      "Epoch 195: loss: 0.4691047\n",
      "Epoch 196: loss: 0.46799707\n",
      "Epoch 197: loss: 0.46688145\n",
      "Epoch 198: loss: 0.4657572\n",
      "Epoch 199: loss: 0.46462396\n",
      "Epoch 200: loss: 0.46348113\n",
      "Epoch 201: loss: 0.46232823\n",
      "Epoch 202: loss: 0.4611648\n",
      "Epoch 203: loss: 0.45999065\n",
      "Epoch 204: loss: 0.45880538\n",
      "Epoch 205: loss: 0.4576085\n",
      "Epoch 206: loss: 0.4564\n",
      "Epoch 207: loss: 0.45517924\n",
      "Epoch 208: loss: 0.45394644\n",
      "Epoch 209: loss: 0.45270115\n",
      "Epoch 210: loss: 0.45144317\n",
      "Epoch 211: loss: 0.45017257\n",
      "Epoch 212: loss: 0.4488891\n",
      "Epoch 213: loss: 0.44759253\n",
      "Epoch 214: loss: 0.44628286\n",
      "Epoch 215: loss: 0.4449601\n",
      "Epoch 216: loss: 0.4436241\n",
      "Epoch 217: loss: 0.44227505\n",
      "Epoch 218: loss: 0.4409125\n",
      "Epoch 219: loss: 0.43953663\n",
      "Epoch 220: loss: 0.43814757\n",
      "Epoch 221: loss: 0.4367452\n",
      "Epoch 222: loss: 0.43532938\n",
      "Epoch 223: loss: 0.4339002\n",
      "Epoch 224: loss: 0.43245775\n",
      "Epoch 225: loss: 0.431002\n",
      "Epoch 226: loss: 0.42953292\n",
      "Epoch 227: loss: 0.4280504\n",
      "Epoch 228: loss: 0.42655468\n",
      "Epoch 229: loss: 0.42504546\n",
      "Epoch 230: loss: 0.42352304\n",
      "Epoch 231: loss: 0.42198718\n",
      "Epoch 232: loss: 0.42043808\n",
      "Epoch 233: loss: 0.41887555\n",
      "Epoch 234: loss: 0.4172996\n",
      "Epoch 235: loss: 0.41571036\n",
      "Epoch 236: loss: 0.41410768\n",
      "Epoch 237: loss: 0.41249168\n",
      "Epoch 238: loss: 0.4108624\n",
      "Epoch 239: loss: 0.40921974\n",
      "Epoch 240: loss: 0.40756384\n",
      "Epoch 241: loss: 0.4058949\n",
      "Epoch 242: loss: 0.4042128\n",
      "Epoch 243: loss: 0.40251774\n",
      "Epoch 244: loss: 0.4008098\n",
      "Epoch 245: loss: 0.39908922\n",
      "Epoch 246: loss: 0.39735615\n",
      "Epoch 247: loss: 0.39561072\n",
      "Epoch 248: loss: 0.3938533\n",
      "Epoch 249: loss: 0.39208385\n",
      "Epoch 250: loss: 0.3903028\n",
      "Epoch 251: loss: 0.3885104\n",
      "Epoch 252: loss: 0.38670692\n",
      "Epoch 253: loss: 0.38489264\n",
      "Epoch 254: loss: 0.38306797\n",
      "Epoch 255: loss: 0.38123322\n",
      "Epoch 256: loss: 0.37938857\n",
      "Epoch 257: loss: 0.3775346\n",
      "Epoch 258: loss: 0.37567157\n",
      "Epoch 259: loss: 0.37379992\n",
      "Epoch 260: loss: 0.3719199\n",
      "Epoch 261: loss: 0.37003192\n",
      "Epoch 262: loss: 0.36813638\n",
      "Epoch 263: loss: 0.36623368\n",
      "Epoch 264: loss: 0.36432415\n",
      "Epoch 265: loss: 0.3624082\n",
      "Epoch 266: loss: 0.3604862\n",
      "Epoch 267: loss: 0.35855857\n",
      "Epoch 268: loss: 0.35662556\n",
      "Epoch 269: loss: 0.3546877\n",
      "Epoch 270: loss: 0.35274526\n",
      "Epoch 271: loss: 0.3507986\n",
      "Epoch 272: loss: 0.34884802\n",
      "Epoch 273: loss: 0.34689412\n",
      "Epoch 274: loss: 0.344937\n",
      "Epoch 275: loss: 0.3429772\n",
      "Epoch 276: loss: 0.34101495\n",
      "Epoch 277: loss: 0.33905065\n",
      "Epoch 278: loss: 0.33708462\n",
      "Epoch 279: loss: 0.3351172\n",
      "Epoch 280: loss: 0.33314878\n",
      "Epoch 281: loss: 0.33117977\n",
      "Epoch 282: loss: 0.32921037\n",
      "Epoch 283: loss: 0.32724103\n",
      "Epoch 284: loss: 0.32527196\n",
      "Epoch 285: loss: 0.3233036\n",
      "Epoch 286: loss: 0.32133618\n",
      "Epoch 287: loss: 0.31937003\n",
      "Epoch 288: loss: 0.31740564\n",
      "Epoch 289: loss: 0.3154431\n",
      "Epoch 290: loss: 0.31348297\n",
      "Epoch 291: loss: 0.31152534\n",
      "Epoch 292: loss: 0.30957073\n",
      "Epoch 293: loss: 0.30761915\n",
      "Epoch 294: loss: 0.30567124\n",
      "Epoch 295: loss: 0.30372697\n",
      "Epoch 296: loss: 0.30178696\n",
      "Epoch 297: loss: 0.29985127\n",
      "Epoch 298: loss: 0.29792026\n",
      "Epoch 299: loss: 0.29599428\n",
      "Epoch 300: loss: 0.29407346\n",
      "Epoch 301: loss: 0.2921582\n",
      "Epoch 302: loss: 0.29024872\n",
      "Epoch 303: loss: 0.28834522\n",
      "Epoch 304: loss: 0.28644797\n",
      "Epoch 305: loss: 0.2845574\n",
      "Epoch 306: loss: 0.2826735\n",
      "Epoch 307: loss: 0.28079665\n",
      "Epoch 308: loss: 0.27892703\n",
      "Epoch 309: loss: 0.27706495\n",
      "Epoch 310: loss: 0.27521053\n",
      "Epoch 311: loss: 0.273364\n",
      "Epoch 312: loss: 0.27152556\n",
      "Epoch 313: loss: 0.2696955\n",
      "Epoch 314: loss: 0.26787385\n",
      "Epoch 315: loss: 0.266061\n",
      "Epoch 316: loss: 0.26425698\n",
      "Epoch 317: loss: 0.262462\n",
      "Epoch 318: loss: 0.26067618\n",
      "Epoch 319: loss: 0.2588998\n",
      "Epoch 320: loss: 0.25713295\n",
      "Epoch 321: loss: 0.25537583\n",
      "Epoch 322: loss: 0.25362852\n",
      "Epoch 323: loss: 0.2518911\n",
      "Epoch 324: loss: 0.25016388\n",
      "Epoch 325: loss: 0.24844685\n",
      "Epoch 326: loss: 0.24674019\n",
      "Epoch 327: loss: 0.24504398\n",
      "Epoch 328: loss: 0.24335833\n",
      "Epoch 329: loss: 0.2416834\n",
      "Epoch 330: loss: 0.24001917\n",
      "Epoch 331: loss: 0.23836586\n",
      "Epoch 332: loss: 0.23672348\n",
      "Epoch 333: loss: 0.2350922\n",
      "Epoch 334: loss: 0.233472\n",
      "Epoch 335: loss: 0.23186292\n",
      "Epoch 336: loss: 0.23026513\n",
      "Epoch 337: loss: 0.22867869\n",
      "Epoch 338: loss: 0.2271036\n",
      "Epoch 339: loss: 0.22553994\n",
      "Epoch 340: loss: 0.22398768\n",
      "Epoch 341: loss: 0.22244702\n",
      "Epoch 342: loss: 0.22091779\n",
      "Epoch 343: loss: 0.21940026\n",
      "Epoch 344: loss: 0.21789429\n",
      "Epoch 345: loss: 0.21639994\n",
      "Epoch 346: loss: 0.2149172\n",
      "Epoch 347: loss: 0.2134462\n",
      "Epoch 348: loss: 0.21198684\n",
      "Epoch 349: loss: 0.21053918\n",
      "Epoch 350: loss: 0.20910318\n",
      "Epoch 351: loss: 0.20767891\n",
      "Epoch 352: loss: 0.20626636\n",
      "Epoch 353: loss: 0.20486544\n",
      "Epoch 354: loss: 0.20347619\n",
      "Epoch 355: loss: 0.20209858\n",
      "Epoch 356: loss: 0.20073262\n",
      "Epoch 357: loss: 0.19937834\n",
      "Epoch 358: loss: 0.1980356\n",
      "Epoch 359: loss: 0.19670443\n",
      "Epoch 360: loss: 0.19538479\n",
      "Epoch 361: loss: 0.19407672\n",
      "Epoch 362: loss: 0.19278006\n",
      "Epoch 363: loss: 0.19149488\n",
      "Epoch 364: loss: 0.1902211\n",
      "Epoch 365: loss: 0.18895864\n",
      "Epoch 366: loss: 0.1877075\n",
      "Epoch 367: loss: 0.18646766\n",
      "Epoch 368: loss: 0.18523906\n",
      "Epoch 369: loss: 0.18402158\n",
      "Epoch 370: loss: 0.18281521\n",
      "Epoch 371: loss: 0.18161996\n",
      "Epoch 372: loss: 0.1804357\n",
      "Epoch 373: loss: 0.17926238\n",
      "Epoch 374: loss: 0.17809992\n",
      "Epoch 375: loss: 0.17694832\n",
      "Epoch 376: loss: 0.17580752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 377: loss: 0.17467736\n",
      "Epoch 378: loss: 0.17355788\n",
      "Epoch 379: loss: 0.17244898\n",
      "Epoch 380: loss: 0.17135054\n",
      "Epoch 381: loss: 0.17026259\n",
      "Epoch 382: loss: 0.16918495\n",
      "Epoch 383: loss: 0.16811766\n",
      "Epoch 384: loss: 0.1670606\n",
      "Epoch 385: loss: 0.16601367\n",
      "Epoch 386: loss: 0.16497682\n",
      "Epoch 387: loss: 0.16395001\n",
      "Epoch 388: loss: 0.16293307\n",
      "Epoch 389: loss: 0.16192605\n",
      "Epoch 390: loss: 0.1609288\n",
      "Epoch 391: loss: 0.15994127\n",
      "Epoch 392: loss: 0.15896334\n",
      "Epoch 393: loss: 0.15799499\n",
      "Epoch 394: loss: 0.15703611\n",
      "Epoch 395: loss: 0.1560866\n",
      "Epoch 396: loss: 0.15514646\n",
      "Epoch 397: loss: 0.15421559\n",
      "Epoch 398: loss: 0.15329379\n",
      "Epoch 399: loss: 0.15238112\n",
      "Epoch 400: loss: 0.15147752\n",
      "Epoch 401: loss: 0.15058276\n",
      "Epoch 402: loss: 0.14969692\n",
      "Epoch 403: loss: 0.14881988\n",
      "Epoch 404: loss: 0.14795142\n",
      "Epoch 405: loss: 0.14709164\n",
      "Epoch 406: loss: 0.14624038\n",
      "Epoch 407: loss: 0.14539763\n",
      "Epoch 408: loss: 0.14456324\n",
      "Epoch 409: loss: 0.1437371\n",
      "Epoch 410: loss: 0.14291926\n",
      "Epoch 411: loss: 0.1421095\n",
      "Epoch 412: loss: 0.14130783\n",
      "Epoch 413: loss: 0.14051415\n",
      "Epoch 414: loss: 0.13972841\n",
      "Epoch 415: loss: 0.13895053\n",
      "Epoch 416: loss: 0.13818033\n",
      "Epoch 417: loss: 0.13741787\n",
      "Epoch 418: loss: 0.13666302\n",
      "Epoch 419: loss: 0.13591571\n",
      "Epoch 420: loss: 0.13517581\n",
      "Epoch 421: loss: 0.13444334\n",
      "Epoch 422: loss: 0.13371818\n",
      "Epoch 423: loss: 0.1330002\n",
      "Epoch 424: loss: 0.13228941\n",
      "Epoch 425: loss: 0.13158576\n",
      "Epoch 426: loss: 0.13088907\n",
      "Epoch 427: loss: 0.13019933\n",
      "Epoch 428: loss: 0.12951651\n",
      "Epoch 429: loss: 0.12884046\n",
      "Epoch 430: loss: 0.12817112\n",
      "Epoch 431: loss: 0.12750849\n",
      "Epoch 432: loss: 0.1268524\n",
      "Epoch 433: loss: 0.12620284\n",
      "Epoch 434: loss: 0.12555978\n",
      "Epoch 435: loss: 0.12492306\n",
      "Epoch 436: loss: 0.12429268\n",
      "Epoch 437: loss: 0.12366853\n",
      "Epoch 438: loss: 0.12305062\n",
      "Epoch 439: loss: 0.12243876\n",
      "Epoch 440: loss: 0.12183305\n",
      "Epoch 441: loss: 0.12123324\n",
      "Epoch 442: loss: 0.120639354\n",
      "Epoch 443: loss: 0.12005136\n",
      "Epoch 444: loss: 0.11946917\n",
      "Epoch 445: loss: 0.11889271\n",
      "Epoch 446: loss: 0.118321925\n",
      "Epoch 447: loss: 0.11775674\n",
      "Epoch 448: loss: 0.117197126\n",
      "Epoch 449: loss: 0.116643004\n",
      "Epoch 450: loss: 0.116094284\n",
      "Epoch 451: loss: 0.11555097\n",
      "Epoch 452: loss: 0.11501298\n",
      "Epoch 453: loss: 0.1144802\n",
      "Epoch 454: loss: 0.11395264\n",
      "Epoch 455: loss: 0.113430224\n",
      "Epoch 456: loss: 0.1129129\n",
      "Epoch 457: loss: 0.1124006\n",
      "Epoch 458: loss: 0.111893274\n",
      "Epoch 459: loss: 0.11139087\n",
      "Epoch 460: loss: 0.11089334\n",
      "Epoch 461: loss: 0.11040056\n",
      "Epoch 462: loss: 0.10991262\n",
      "Epoch 463: loss: 0.10942936\n",
      "Epoch 464: loss: 0.10895074\n",
      "Epoch 465: loss: 0.10847673\n",
      "Epoch 466: loss: 0.10800728\n",
      "Epoch 467: loss: 0.107542306\n",
      "Epoch 468: loss: 0.10708182\n",
      "Epoch 469: loss: 0.10662574\n",
      "Epoch 470: loss: 0.10617401\n",
      "Epoch 471: loss: 0.10572656\n",
      "Epoch 472: loss: 0.10528339\n",
      "Epoch 473: loss: 0.10484442\n",
      "Epoch 474: loss: 0.10440961\n",
      "Epoch 475: loss: 0.10397891\n",
      "Epoch 476: loss: 0.10355228\n",
      "Epoch 477: loss: 0.103129715\n",
      "Epoch 478: loss: 0.102711104\n",
      "Epoch 479: loss: 0.10229642\n",
      "Epoch 480: loss: 0.10188567\n",
      "Epoch 481: loss: 0.10147874\n",
      "Epoch 482: loss: 0.10107563\n",
      "Epoch 483: loss: 0.10067627\n",
      "Epoch 484: loss: 0.10028065\n",
      "Epoch 485: loss: 0.099888705\n",
      "Epoch 486: loss: 0.09950041\n",
      "Epoch 487: loss: 0.099115685\n",
      "Epoch 488: loss: 0.09873455\n",
      "Epoch 489: loss: 0.09835693\n",
      "Epoch 490: loss: 0.097982794\n",
      "Epoch 491: loss: 0.09761209\n",
      "Epoch 492: loss: 0.09724478\n",
      "Epoch 493: loss: 0.09688085\n",
      "Epoch 494: loss: 0.096520245\n",
      "Epoch 495: loss: 0.09616294\n",
      "Epoch 496: loss: 0.095808856\n",
      "Epoch 497: loss: 0.09545803\n",
      "Epoch 498: loss: 0.09511037\n",
      "Epoch 499: loss: 0.09476586\n",
      "Epoch 500: loss: 0.09442445\n",
      "Epoch 501: loss: 0.094086125\n",
      "Epoch 502: loss: 0.093750864\n",
      "Epoch 503: loss: 0.09341859\n",
      "Epoch 504: loss: 0.09308927\n",
      "Epoch 505: loss: 0.09276291\n",
      "Epoch 506: loss: 0.092439465\n",
      "Epoch 507: loss: 0.092118874\n",
      "Epoch 508: loss: 0.091801144\n",
      "Epoch 509: loss: 0.091486216\n",
      "Epoch 510: loss: 0.09117409\n",
      "Epoch 511: loss: 0.09086468\n",
      "Epoch 512: loss: 0.09055803\n",
      "Epoch 513: loss: 0.09025402\n",
      "Epoch 514: loss: 0.0899527\n",
      "Epoch 515: loss: 0.089654\n",
      "Epoch 516: loss: 0.08935789\n",
      "Epoch 517: loss: 0.089064375\n",
      "Epoch 518: loss: 0.08877336\n",
      "Epoch 519: loss: 0.088484906\n",
      "Epoch 520: loss: 0.08819894\n",
      "Epoch 521: loss: 0.08791538\n",
      "Epoch 522: loss: 0.08763427\n",
      "Epoch 523: loss: 0.08735558\n",
      "Epoch 524: loss: 0.08707924\n",
      "Epoch 525: loss: 0.086805254\n",
      "Epoch 526: loss: 0.08653362\n",
      "Epoch 527: loss: 0.08626425\n",
      "Epoch 528: loss: 0.085997194\n",
      "Epoch 529: loss: 0.085732356\n",
      "Epoch 530: loss: 0.08546975\n",
      "Epoch 531: loss: 0.085209325\n",
      "Epoch 532: loss: 0.0849511\n",
      "Epoch 533: loss: 0.08469498\n",
      "Epoch 534: loss: 0.084441006\n",
      "Epoch 535: loss: 0.08418917\n",
      "Epoch 536: loss: 0.08393938\n",
      "Epoch 537: loss: 0.083691664\n",
      "Epoch 538: loss: 0.083445914\n",
      "Epoch 539: loss: 0.08320225\n",
      "Epoch 540: loss: 0.082960546\n",
      "Epoch 541: loss: 0.082720816\n",
      "Epoch 542: loss: 0.08248304\n",
      "Epoch 543: loss: 0.08224715\n",
      "Epoch 544: loss: 0.0820132\n",
      "Epoch 545: loss: 0.08178115\n",
      "Epoch 546: loss: 0.08155092\n",
      "Epoch 547: loss: 0.08132256\n",
      "Epoch 548: loss: 0.08109597\n",
      "Epoch 549: loss: 0.080871224\n",
      "Epoch 550: loss: 0.080648236\n",
      "Epoch 551: loss: 0.080427036\n",
      "Epoch 552: loss: 0.08020757\n",
      "Epoch 553: loss: 0.07998983\n",
      "Epoch 554: loss: 0.079773806\n",
      "Epoch 555: loss: 0.07955945\n",
      "Epoch 556: loss: 0.07934677\n",
      "Epoch 557: loss: 0.07913575\n",
      "Epoch 558: loss: 0.07892636\n",
      "Epoch 559: loss: 0.07871855\n",
      "Epoch 560: loss: 0.07851241\n",
      "Epoch 561: loss: 0.0783078\n",
      "Epoch 562: loss: 0.07810475\n",
      "Epoch 563: loss: 0.077903256\n",
      "Epoch 564: loss: 0.07770333\n",
      "Epoch 565: loss: 0.07750488\n",
      "Epoch 566: loss: 0.07730793\n",
      "Epoch 567: loss: 0.07711251\n",
      "Epoch 568: loss: 0.07691852\n",
      "Epoch 569: loss: 0.07672598\n",
      "Epoch 570: loss: 0.07653489\n",
      "Epoch 571: loss: 0.07634522\n",
      "Epoch 572: loss: 0.076156944\n",
      "Epoch 573: loss: 0.075970106\n",
      "Epoch 574: loss: 0.07578462\n",
      "Epoch 575: loss: 0.07560051\n",
      "Epoch 576: loss: 0.07541771\n",
      "Epoch 577: loss: 0.07523625\n",
      "Epoch 578: loss: 0.07505615\n",
      "Epoch 579: loss: 0.07487735\n",
      "Epoch 580: loss: 0.074699834\n",
      "Epoch 581: loss: 0.074523605\n",
      "Epoch 582: loss: 0.07434866\n",
      "Epoch 583: loss: 0.07417496\n",
      "Epoch 584: loss: 0.07400251\n",
      "Epoch 585: loss: 0.07383129\n",
      "Epoch 586: loss: 0.07366129\n",
      "Epoch 587: loss: 0.07349249\n",
      "Epoch 588: loss: 0.07332492\n",
      "Epoch 589: loss: 0.07315844\n",
      "Epoch 590: loss: 0.072993234\n",
      "Epoch 591: loss: 0.07282914\n",
      "Epoch 592: loss: 0.0726662\n",
      "Epoch 593: loss: 0.07250441\n",
      "Epoch 594: loss: 0.07234374\n",
      "Epoch 595: loss: 0.07218415\n",
      "Epoch 596: loss: 0.07202572\n",
      "Epoch 597: loss: 0.07186834\n",
      "Epoch 598: loss: 0.07171207\n",
      "Epoch 599: loss: 0.071556844\n",
      "Epoch 600: loss: 0.0714027\n",
      "Epoch 601: loss: 0.071249604\n",
      "Epoch 602: loss: 0.071097545\n",
      "Epoch 603: loss: 0.07094652\n",
      "Epoch 604: loss: 0.070796564\n",
      "Epoch 605: loss: 0.07064757\n",
      "Epoch 606: loss: 0.07049957\n",
      "Epoch 607: loss: 0.07035261\n",
      "Epoch 608: loss: 0.07020657\n",
      "Epoch 609: loss: 0.07006154\n",
      "Epoch 610: loss: 0.06991748\n",
      "Epoch 611: loss: 0.06977439\n",
      "Epoch 612: loss: 0.06963222\n",
      "Epoch 613: loss: 0.06949098\n",
      "Epoch 614: loss: 0.06935067\n",
      "Epoch 615: loss: 0.069211304\n",
      "Epoch 616: loss: 0.069072865\n",
      "Epoch 617: loss: 0.06893531\n",
      "Epoch 618: loss: 0.06879866\n",
      "Epoch 619: loss: 0.0686629\n",
      "Epoch 620: loss: 0.06852801\n",
      "Epoch 621: loss: 0.06839399\n",
      "Epoch 622: loss: 0.06826083\n",
      "Epoch 623: loss: 0.06812852\n",
      "Epoch 624: loss: 0.06799711\n",
      "Epoch 625: loss: 0.06786651\n",
      "Epoch 626: loss: 0.067736745\n",
      "Epoch 627: loss: 0.0676078\n",
      "Epoch 628: loss: 0.06747969\n",
      "Epoch 629: loss: 0.06735238\n",
      "Epoch 630: loss: 0.06722587\n",
      "Epoch 631: loss: 0.06710019\n",
      "Epoch 632: loss: 0.066975266\n",
      "Epoch 633: loss: 0.06685116\n",
      "Epoch 634: loss: 0.06672781\n",
      "Epoch 635: loss: 0.06660523\n",
      "Epoch 636: loss: 0.06648343\n",
      "Epoch 637: loss: 0.066362366\n",
      "Epoch 638: loss: 0.066242084\n",
      "Epoch 639: loss: 0.06612252\n",
      "Epoch 640: loss: 0.06600372\n",
      "Epoch 641: loss: 0.06588568\n",
      "Epoch 642: loss: 0.0657683\n",
      "Epoch 643: loss: 0.06565167\n",
      "Epoch 644: loss: 0.06553577\n",
      "Epoch 645: loss: 0.06542056\n",
      "Epoch 646: loss: 0.065306045\n",
      "Epoch 647: loss: 0.06519225\n",
      "Epoch 648: loss: 0.06507913\n",
      "Epoch 649: loss: 0.06496671\n",
      "Epoch 650: loss: 0.06485497\n",
      "Epoch 651: loss: 0.064743884\n",
      "Epoch 652: loss: 0.06463349\n",
      "Epoch 653: loss: 0.064523764\n",
      "Epoch 654: loss: 0.064414695\n",
      "Epoch 655: loss: 0.06430625\n",
      "Epoch 656: loss: 0.06419849\n",
      "Epoch 657: loss: 0.06409136\n",
      "Epoch 658: loss: 0.06398487\n",
      "Epoch 659: loss: 0.063879006\n",
      "Epoch 660: loss: 0.06377378\n",
      "Epoch 661: loss: 0.06366918\n",
      "Epoch 662: loss: 0.06356519\n",
      "Epoch 663: loss: 0.06346182\n",
      "Epoch 664: loss: 0.06335903\n",
      "Epoch 665: loss: 0.06325687\n",
      "Epoch 666: loss: 0.06315532\n",
      "Epoch 667: loss: 0.06305433\n",
      "Epoch 668: loss: 0.062953964\n",
      "Epoch 669: loss: 0.06285418\n",
      "Epoch 670: loss: 0.062754944\n",
      "Epoch 671: loss: 0.0626563\n",
      "Epoch 672: loss: 0.06255824\n",
      "Epoch 673: loss: 0.062460758\n",
      "Epoch 674: loss: 0.062363822\n",
      "Epoch 675: loss: 0.062267445\n",
      "Epoch 676: loss: 0.06217163\n",
      "Epoch 677: loss: 0.062076364\n",
      "Epoch 678: loss: 0.06198166\n",
      "Epoch 679: loss: 0.061887473\n",
      "Epoch 680: loss: 0.061793838\n",
      "Epoch 681: loss: 0.06170074\n",
      "Epoch 682: loss: 0.06160817\n",
      "Epoch 683: loss: 0.061516125\n",
      "Epoch 684: loss: 0.061424617\n",
      "Epoch 685: loss: 0.06133361\n",
      "Epoch 686: loss: 0.06124312\n",
      "Epoch 687: loss: 0.061153162\n",
      "Epoch 688: loss: 0.061063692\n",
      "Epoch 689: loss: 0.060974732\n",
      "Epoch 690: loss: 0.060886268\n",
      "Epoch 691: loss: 0.060798302\n",
      "Epoch 692: loss: 0.060710832\n",
      "Epoch 693: loss: 0.06062385\n",
      "Epoch 694: loss: 0.060537346\n",
      "Epoch 695: loss: 0.060451355\n",
      "Epoch 696: loss: 0.06036581\n",
      "Epoch 697: loss: 0.060280766\n",
      "Epoch 698: loss: 0.06019619\n",
      "Epoch 699: loss: 0.060112074\n",
      "Epoch 700: loss: 0.060028426\n",
      "Epoch 701: loss: 0.059945248\n",
      "Epoch 702: loss: 0.059862513\n",
      "Epoch 703: loss: 0.059780255\n",
      "Epoch 704: loss: 0.05969844\n",
      "Epoch 705: loss: 0.0596171\n",
      "Epoch 706: loss: 0.05953617\n",
      "Epoch 707: loss: 0.059455708\n",
      "Epoch 708: loss: 0.059375674\n",
      "Epoch 709: loss: 0.059296075\n",
      "Epoch 710: loss: 0.059216913\n",
      "Epoch 711: loss: 0.0591382\n",
      "Epoch 712: loss: 0.05905988\n",
      "Epoch 713: loss: 0.05898203\n",
      "Epoch 714: loss: 0.058904573\n",
      "Epoch 715: loss: 0.058827545\n",
      "Epoch 716: loss: 0.058750924\n",
      "Epoch 717: loss: 0.058674738\n",
      "Epoch 718: loss: 0.058598943\n",
      "Epoch 719: loss: 0.058523573\n",
      "Epoch 720: loss: 0.058448594\n",
      "Epoch 721: loss: 0.05837403\n",
      "Epoch 722: loss: 0.058299877\n",
      "Epoch 723: loss: 0.05822611\n",
      "Epoch 724: loss: 0.058152728\n",
      "Epoch 725: loss: 0.05807974\n",
      "Epoch 726: loss: 0.05800715\n",
      "Epoch 727: loss: 0.05793495\n",
      "Epoch 728: loss: 0.057863127\n",
      "Epoch 729: loss: 0.0577917\n",
      "Epoch 730: loss: 0.057720635\n",
      "Epoch 731: loss: 0.057649963\n",
      "Epoch 732: loss: 0.05757965\n",
      "Epoch 733: loss: 0.057509728\n",
      "Epoch 734: loss: 0.05744018\n",
      "Epoch 735: loss: 0.05737097\n",
      "Epoch 736: loss: 0.05730215\n",
      "Epoch 737: loss: 0.05723369\n",
      "Epoch 738: loss: 0.057165578\n",
      "Epoch 739: loss: 0.05709783\n",
      "Epoch 740: loss: 0.057030436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 741: loss: 0.05696341\n",
      "Epoch 742: loss: 0.056896713\n",
      "Epoch 743: loss: 0.05683037\n",
      "Epoch 744: loss: 0.05676438\n",
      "Epoch 745: loss: 0.056698736\n",
      "Epoch 746: loss: 0.05663342\n",
      "Epoch 747: loss: 0.056568462\n",
      "Epoch 748: loss: 0.05650383\n",
      "Epoch 749: loss: 0.05643954\n",
      "Epoch 750: loss: 0.056375593\n",
      "Epoch 751: loss: 0.056311958\n",
      "Epoch 752: loss: 0.056248665\n",
      "Epoch 753: loss: 0.05618569\n",
      "Epoch 754: loss: 0.05612305\n",
      "Epoch 755: loss: 0.05606072\n",
      "Epoch 756: loss: 0.0559987\n",
      "Epoch 757: loss: 0.055937055\n",
      "Epoch 758: loss: 0.055875678\n",
      "Epoch 759: loss: 0.055814635\n",
      "Epoch 760: loss: 0.055753887\n",
      "Epoch 761: loss: 0.05569348\n",
      "Epoch 762: loss: 0.055633347\n",
      "Epoch 763: loss: 0.055573553\n",
      "Epoch 764: loss: 0.05551404\n",
      "Epoch 765: loss: 0.055454828\n",
      "Epoch 766: loss: 0.055395946\n",
      "Epoch 767: loss: 0.055337355\n",
      "Epoch 768: loss: 0.05527904\n",
      "Epoch 769: loss: 0.055221025\n",
      "Epoch 770: loss: 0.05516331\n",
      "Epoch 771: loss: 0.055105895\n",
      "Epoch 772: loss: 0.05504878\n",
      "Epoch 773: loss: 0.05499195\n",
      "Epoch 774: loss: 0.054935366\n",
      "Epoch 775: loss: 0.054879107\n",
      "Epoch 776: loss: 0.054823145\n",
      "Epoch 777: loss: 0.05476741\n",
      "Epoch 778: loss: 0.054711998\n",
      "Epoch 779: loss: 0.054656837\n",
      "Epoch 780: loss: 0.054601986\n",
      "Epoch 781: loss: 0.05454739\n",
      "Epoch 782: loss: 0.05449307\n",
      "Epoch 783: loss: 0.054439012\n",
      "Epoch 784: loss: 0.05438525\n",
      "Epoch 785: loss: 0.054331742\n",
      "Epoch 786: loss: 0.054278474\n",
      "Epoch 787: loss: 0.054225497\n",
      "Epoch 788: loss: 0.054172784\n",
      "Epoch 789: loss: 0.054120336\n",
      "Epoch 790: loss: 0.054068144\n",
      "Epoch 791: loss: 0.05401622\n",
      "Epoch 792: loss: 0.053964533\n",
      "Epoch 793: loss: 0.053913098\n",
      "Epoch 794: loss: 0.053861912\n",
      "Epoch 795: loss: 0.053811017\n",
      "Epoch 796: loss: 0.053760346\n",
      "Epoch 797: loss: 0.05370994\n",
      "Epoch 798: loss: 0.053659774\n",
      "Epoch 799: loss: 0.053609855\n",
      "Epoch 800: loss: 0.05356015\n",
      "Epoch 801: loss: 0.053510737\n",
      "Epoch 802: loss: 0.053461544\n",
      "Epoch 803: loss: 0.053412598\n",
      "Epoch 804: loss: 0.05336389\n",
      "Epoch 805: loss: 0.053315412\n",
      "Epoch 806: loss: 0.053267166\n",
      "Epoch 807: loss: 0.05321915\n",
      "Epoch 808: loss: 0.053171422\n",
      "Epoch 809: loss: 0.05312387\n",
      "Epoch 810: loss: 0.053076554\n",
      "Epoch 811: loss: 0.053029496\n",
      "Epoch 812: loss: 0.05298264\n",
      "Epoch 813: loss: 0.05293601\n",
      "Epoch 814: loss: 0.05288965\n",
      "Epoch 815: loss: 0.05284345\n",
      "Epoch 816: loss: 0.052797507\n",
      "Epoch 817: loss: 0.052751794\n",
      "Epoch 818: loss: 0.052706297\n",
      "Epoch 819: loss: 0.052661013\n",
      "Epoch 820: loss: 0.052615926\n",
      "Epoch 821: loss: 0.052571073\n",
      "Epoch 822: loss: 0.052526455\n",
      "Epoch 823: loss: 0.052482028\n",
      "Epoch 824: loss: 0.052437816\n",
      "Epoch 825: loss: 0.052393816\n",
      "Epoch 826: loss: 0.05235001\n",
      "Epoch 827: loss: 0.052306432\n",
      "Epoch 828: loss: 0.052263077\n",
      "Epoch 829: loss: 0.05221991\n",
      "Epoch 830: loss: 0.052176964\n",
      "Epoch 831: loss: 0.0521342\n",
      "Epoch 832: loss: 0.05209165\n",
      "Epoch 833: loss: 0.05204929\n",
      "Epoch 834: loss: 0.052007142\n",
      "Epoch 835: loss: 0.051965207\n",
      "Epoch 836: loss: 0.05192344\n",
      "Epoch 837: loss: 0.051881902\n",
      "Epoch 838: loss: 0.051840555\n",
      "Epoch 839: loss: 0.05179938\n",
      "Epoch 840: loss: 0.05175842\n",
      "Epoch 841: loss: 0.051717617\n",
      "Epoch 842: loss: 0.051677044\n",
      "Epoch 843: loss: 0.05163666\n",
      "Epoch 844: loss: 0.051596437\n",
      "Epoch 845: loss: 0.051556457\n",
      "Epoch 846: loss: 0.05151662\n",
      "Epoch 847: loss: 0.05147697\n",
      "Epoch 848: loss: 0.05143752\n",
      "Epoch 849: loss: 0.051398244\n",
      "Epoch 850: loss: 0.051359143\n",
      "Epoch 851: loss: 0.051320262\n",
      "Epoch 852: loss: 0.051281527\n",
      "Epoch 853: loss: 0.05124298\n",
      "Epoch 854: loss: 0.051204626\n",
      "Epoch 855: loss: 0.051166445\n",
      "Epoch 856: loss: 0.05112842\n",
      "Epoch 857: loss: 0.051090598\n",
      "Epoch 858: loss: 0.051052928\n",
      "Epoch 859: loss: 0.05101545\n",
      "Epoch 860: loss: 0.05097813\n",
      "Epoch 861: loss: 0.050940998\n",
      "Epoch 862: loss: 0.050904036\n",
      "Epoch 863: loss: 0.050867252\n",
      "Epoch 864: loss: 0.0508306\n",
      "Epoch 865: loss: 0.050794158\n",
      "Epoch 866: loss: 0.05075784\n",
      "Epoch 867: loss: 0.05072172\n",
      "Epoch 868: loss: 0.05068576\n",
      "Epoch 869: loss: 0.05064998\n",
      "Epoch 870: loss: 0.05061434\n",
      "Epoch 871: loss: 0.0505789\n",
      "Epoch 872: loss: 0.050543595\n",
      "Epoch 873: loss: 0.050508432\n",
      "Epoch 874: loss: 0.05047344\n",
      "Epoch 875: loss: 0.050438613\n",
      "Epoch 876: loss: 0.05040396\n",
      "Epoch 877: loss: 0.05036944\n",
      "Epoch 878: loss: 0.05033509\n",
      "Epoch 879: loss: 0.050300878\n",
      "Epoch 880: loss: 0.050266866\n",
      "Epoch 881: loss: 0.05023295\n",
      "Epoch 882: loss: 0.050199226\n",
      "Epoch 883: loss: 0.050165642\n",
      "Epoch 884: loss: 0.05013223\n",
      "Epoch 885: loss: 0.05009893\n",
      "Epoch 886: loss: 0.050065804\n",
      "Epoch 887: loss: 0.05003282\n",
      "Epoch 888: loss: 0.049999963\n",
      "Epoch 889: loss: 0.049967304\n",
      "Epoch 890: loss: 0.049934767\n",
      "Epoch 891: loss: 0.04990237\n",
      "Epoch 892: loss: 0.049870126\n",
      "Epoch 893: loss: 0.04983801\n",
      "Epoch 894: loss: 0.049806062\n",
      "Epoch 895: loss: 0.049774244\n",
      "Epoch 896: loss: 0.04974255\n",
      "Epoch 897: loss: 0.04971104\n",
      "Epoch 898: loss: 0.049679637\n",
      "Epoch 899: loss: 0.04964838\n",
      "Epoch 900: loss: 0.049617253\n",
      "Epoch 901: loss: 0.049586285\n",
      "Epoch 902: loss: 0.04955544\n",
      "Epoch 903: loss: 0.04952474\n",
      "Epoch 904: loss: 0.04949419\n",
      "Epoch 905: loss: 0.049463715\n",
      "Epoch 906: loss: 0.04943345\n",
      "Epoch 907: loss: 0.049403265\n",
      "Epoch 908: loss: 0.049373243\n",
      "Epoch 909: loss: 0.049343344\n",
      "Epoch 910: loss: 0.04931358\n",
      "Epoch 911: loss: 0.049283955\n",
      "Epoch 912: loss: 0.049254432\n",
      "Epoch 913: loss: 0.04922506\n",
      "Epoch 914: loss: 0.049195785\n",
      "Epoch 915: loss: 0.04916669\n",
      "Epoch 916: loss: 0.04913769\n",
      "Epoch 917: loss: 0.04910882\n",
      "Epoch 918: loss: 0.049080078\n",
      "Epoch 919: loss: 0.049051475\n",
      "Epoch 920: loss: 0.04902297\n",
      "Epoch 921: loss: 0.04899461\n",
      "Epoch 922: loss: 0.048966367\n",
      "Epoch 923: loss: 0.048938256\n",
      "Epoch 924: loss: 0.04891024\n",
      "Epoch 925: loss: 0.04888237\n",
      "Epoch 926: loss: 0.048854634\n",
      "Epoch 927: loss: 0.04882697\n",
      "Epoch 928: loss: 0.048799444\n",
      "Epoch 929: loss: 0.04877206\n",
      "Epoch 930: loss: 0.04874478\n",
      "Epoch 931: loss: 0.04871763\n",
      "Epoch 932: loss: 0.048690565\n",
      "Epoch 933: loss: 0.04866364\n",
      "Epoch 934: loss: 0.048636854\n",
      "Epoch 935: loss: 0.04861013\n",
      "Epoch 936: loss: 0.048583552\n",
      "Epoch 937: loss: 0.048557077\n",
      "Epoch 938: loss: 0.04853072\n",
      "Epoch 939: loss: 0.048504476\n",
      "Epoch 940: loss: 0.048478335\n",
      "Epoch 941: loss: 0.04845231\n",
      "Epoch 942: loss: 0.0484264\n",
      "Epoch 943: loss: 0.048400596\n",
      "Epoch 944: loss: 0.04837494\n",
      "Epoch 945: loss: 0.048349343\n",
      "Epoch 946: loss: 0.048323866\n",
      "Epoch 947: loss: 0.04829849\n",
      "Epoch 948: loss: 0.048273243\n",
      "Epoch 949: loss: 0.0482481\n",
      "Epoch 950: loss: 0.048223056\n",
      "Epoch 951: loss: 0.048198115\n",
      "Epoch 952: loss: 0.048173267\n",
      "Epoch 953: loss: 0.048148546\n",
      "Epoch 954: loss: 0.04812394\n",
      "Epoch 955: loss: 0.04809942\n",
      "Epoch 956: loss: 0.048074994\n",
      "Epoch 957: loss: 0.04805069\n",
      "Epoch 958: loss: 0.048026476\n",
      "Epoch 959: loss: 0.048002366\n",
      "Epoch 960: loss: 0.047978334\n",
      "Epoch 961: loss: 0.04795444\n",
      "Epoch 962: loss: 0.047930636\n",
      "Epoch 963: loss: 0.04790692\n",
      "Epoch 964: loss: 0.047883306\n",
      "Epoch 965: loss: 0.047859784\n",
      "Epoch 966: loss: 0.047836374\n",
      "Epoch 967: loss: 0.04781308\n",
      "Epoch 968: loss: 0.047789842\n",
      "Epoch 969: loss: 0.047766704\n",
      "Epoch 970: loss: 0.047743708\n",
      "Epoch 971: loss: 0.047720745\n",
      "Epoch 972: loss: 0.047697905\n",
      "Epoch 973: loss: 0.047675163\n",
      "Epoch 974: loss: 0.047652528\n",
      "Epoch 975: loss: 0.047629975\n",
      "Epoch 976: loss: 0.047607493\n",
      "Epoch 977: loss: 0.047585122\n",
      "Epoch 978: loss: 0.04756285\n",
      "Epoch 979: loss: 0.04754065\n",
      "Epoch 980: loss: 0.04751856\n",
      "Epoch 981: loss: 0.047496557\n",
      "Epoch 982: loss: 0.047474638\n",
      "Epoch 983: loss: 0.047452804\n",
      "Epoch 984: loss: 0.047431078\n",
      "Epoch 985: loss: 0.047409426\n",
      "Epoch 986: loss: 0.04738786\n",
      "Epoch 987: loss: 0.04736639\n",
      "Epoch 988: loss: 0.04734501\n",
      "Epoch 989: loss: 0.04732372\n",
      "Epoch 990: loss: 0.0473025\n",
      "Epoch 991: loss: 0.047281384\n",
      "Epoch 992: loss: 0.047260337\n",
      "Epoch 993: loss: 0.047239378\n",
      "Epoch 994: loss: 0.047218505\n",
      "Epoch 995: loss: 0.047197722\n",
      "Epoch 996: loss: 0.047177013\n",
      "Epoch 997: loss: 0.04715639\n",
      "Epoch 998: loss: 0.047135875\n",
      "Epoch 999: loss: 0.047115404\n",
      "Epoch 1000: loss: 0.04709506\n",
      "Accuracy: 98.66666666666667 %\n"
     ]
    }
   ],
   "source": [
    "using DelimitedFiles;\n",
    "using Flux;\n",
    "\n",
    "#Define the parameters\n",
    "topology = [4, 3]; \n",
    "learningRate = 0.01;\n",
    "numMaxEpochs = 1000; \n",
    "\n",
    "# Load the dataset\n",
    "dataset = readdlm(\"../iris/iris.data\",',');\n",
    "# Prepare the data\n",
    "inputs = convert(Array{Float32,2}, dataset[:,1:4]);\n",
    "targets = oneHotEncoding(dataset[:,5]);\n",
    "\n",
    "# Normalization \n",
    "newInputs = normalizeMinMax(inputs);\n",
    "@assert(all(minimum(newInputs, dims=1) .== 0));\n",
    "@assert(all(maximum(newInputs, dims=1) .== 1));\n",
    "# Standarization. Beware of the error done for using the meand and std they are not going to hit perfect 0 or 1\n",
    "#newInputs = normalizeZeroMean(inputs);\n",
    "#@assert(all(abs.(mean(newInputs, dims=1))    .<= 1e-4));\n",
    "#@assert(all(abs.(std( newInputs, dims=1)).-1 .<= 1e-4));\n",
    "\n",
    "# In this example, we use the MAX-MIN normalization \n",
    "normalizeMinMax!(inputs);\n",
    "println(typeof(inputs))\n",
    "(ann, trainingLosses) = trainClassANN(topology, (inputs, targets); maxEpochs=numMaxEpochs, learningRate=learningRate);\n",
    "outputs = ann(inputs')';\n",
    "trainingAccuracy = accuracy(outputs, targets);\n",
    "\n",
    "println(\"Accuracy: \", 100*trainingAccuracy, \" %\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a675f36",
   "metadata": {},
   "source": [
    "***ANSWER***\n",
    "\n",
    "repeat above code several times in order to perform an study with at least 3 architectures and 3 learning rates, this can be done with a functiona an a couple of loops in a very easy way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8659529c",
   "metadata": {},
   "source": [
    "Repeat the experiments performed above, with the unstandardised data, and compare the results with the standardised data. Are there important differences when using standardised data? Which one is the best topology?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f2d622",
   "metadata": {},
   "source": [
    "***ANSWER***\n",
    "\n",
    "\n",
    "same code execute multiple times but without normalization the instructionn to normalize\n",
    "\n",
    "This test should get a poorer performance than the normalized one due to the difference in the scales which makes the them generalize less than the normalized counterpart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b1a6e1",
   "metadata": {},
   "source": [
    "# Julia Notes\n",
    "\n",
    "As any other language, Julia allows the creation of functions. There are several ways to create functions, the most common are these two:\n",
    "\n",
    "1. When the operation to be performed is simple, the function can be declared on one line without the need for the reserved word function.  This is the case for operations that can be performed in one or a few lines of code.  In the latter case, parentheses can be used to enclose the actions to be performed, and `;` to separate them.  The value to be returned by the function will be the last thing to be evaluated, although the reserved word `return` can also be used.  Here are a couple of examples of this way of declaring functions:\n",
    "    ```julia\n",
    "         add(x::Float32, y::Float32) = x+x; \n",
    "         mse(outputs::Array{Float32,1}, targets:: Array{Float32,1}) = mean((targets.-outputs).^2);\n",
    "         avgGreaterThan0(valores::Array{Float32,1}) = mean(values[values.>0]); \n",
    "         avgGreaterThan0(valores::Array{Float32,1}) = ( positives=values.>0; mean(values[positives]); )   \n",
    "    ```\n",
    "2. In many other cases, a function may perform many complex operations that are impractical to write on a single line.  In this case, you can declare the function with the reserved word `function`, and return the result with the reserved word `return`.  As is common in programming languages, evaluating `return` immediately exits the function.  If `return` is not used, the result returned by function will be the last one evaluated.  The example above is shown below: \n",
    "   ```julia\n",
    "         function avgGreaterThan0(Values::Array{Float32,1})    \n",
    "            positives=values.>0;    \n",
    "            return mean(Values[positives]); \n",
    "         end;  \n",
    "    ```\n",
    "  When passing parameters, it is not mandatory, but recommended, to indicate the type of the parameters.  If this is not done, they are assumed to be of type `Any`.  However, a common practice in Julia is to overload functions, i.e. to define functions with the same name but different parameters or of different types.  When calling a function, the correct function will be executed, if any of the ones defined match the parameters passed. This allows different behaviours to be defined. When a function call is made, Julia checks that there exists in memory a definition with that name in which the types of the passed arguments match the defined ones and, if it exists, it will execute it with those parameters.\n",
    "  \n",
    "  In the examples above, the parameters have been defined as `Float32` or `Array{Float32,2}`, which means that the functions are defined for those specific types, and therefore, if calls are made with parameters of type `Float64` or `Array{Float64,2}`, a suitable function definition will not be found, and an error will be raised.  To solve this, it is worthy to apply the subtyping properties as seen in the previous tutorial to define the most generic types that the function can work with. For example, instead of using `Float32`, you can use `AbstractFloat`, `Real` or `Number` types (be aware that the latter type includes complex numbers). Instead of using `Array{Float32,2}`, you could use `Array{<:AbstractFloat}`, `Array{<:Real}` or `Array{<:Number}`. Additionaly, it is needed to take into remember that there are other types that behave like arrays but are not arrays. An important example that will be used in this subject is when transposing arrays, which creates an object that is not of type `Array`, but of type `LinearAlgebra.Adjoint`, but that can be used as if it were an array because its operations are defined as such.  Both `Array` and `LinearAlgebra.Adjoint` are subtypes of `AbstractArray`, as can be seen in the following example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5a04b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = [1. 2.; 3. 4.]; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69d3ed07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isa(m, Array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cfa17fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isa(m, AbstractArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc7a1ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearAlgebra.Adjoint{Float64, Matrix{Float64}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeof(m') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7249074a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isa(m', Array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19a7cf1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isa(m', AbstractArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43600581",
   "metadata": {},
   "source": [
    "Since both types, `Array` and `LinearAlgebra.Adjoint` are subtypes of `AbstractArray`, to allow an argument to be of one type or the other, this will be the type to be used in the function arguments. Therefore, the above functions will be as follows: \n",
    "\n",
    "   ```julia\n",
    "        add(x::Real, y:: Real) = x+x;\n",
    "        mse(outputs::AbstractArray{<:Real,1},targets::AbstractArray{<:Real,1})=mean((targets.-outputs).^2)\n",
    "        avgGreaterThan0(values::AbstractArray{<:Real,1}) = mean(values[values.>0]); \n",
    "        avgGreaterThan0(values:: AbstractArray{<:Real,1})=(positives=values.>0; mean(values[poitives]);) \n",
    "        \n",
    "        function avgGreaterThan0(values::AbstractArray{<:Real,1})    \n",
    "            positives=values.>0;    \n",
    "            return mean(Values[positives]); \n",
    "        end;  \n",
    "   ```\n",
    "When an argument is a vector or array of boolean values, as is the case of the desired outputs in a classification problem, the type to use will be `AbstractArray{Bool,N}`, where N is the dimensionality of the array.  As mentioned in the previous tutorial, this type is a supertype of both `Array{Bool,N}` and `BitArray{N}`. \n",
    "\n",
    "An interesting feature of Julia functions is that they can return more than one value. This is done by using the type `Tuple{...}`, which designates tuples where each element has a certain type, for example `Tuple{Float32,Float32}`, `Tuple{Array{Float32,2}, Int64}` or `Tuple{Float32, Tuple{Int64, Int4}}`. In general, if you want to return more than one element, you return a tuple with the elements you want to return, for example:\n",
    "\n",
    "   ```julia\n",
    "        function avgGreaterThan0(values::AbstractArray{<:Real,1})    \n",
    "            positives=valores.>0;    \n",
    "            return (positives, mean(values[positives])) \n",
    "        end; \n",
    "\n",
    "        (positives, average) = avgGreaterThan0([1.2 , -1.3 , 5.5 , -3.8 , -2.1])\n",
    "   ```\n",
    "\n",
    "When creating tuples where all elements have the same type, a simplified way to define the type is to use `NTuple`, specifying the number of elements and the type. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "73e6a43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuple{Float64,Float64}==NTuple{2,Float64}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8072b588",
   "metadata": {},
   "source": [
    "As was aforementioned, the names of those functions that modify the values of one of their arguments are usually terminated with `!` by convention."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
